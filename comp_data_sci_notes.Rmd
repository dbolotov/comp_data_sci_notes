---
title: "Competitive Data Science Notes"
output:
  html_document:
    theme: cerulean
    highlight: tango
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require("knitr")
```

# Introduction

This is a set of notes for the Competitive Data Science course on Coursera.

# Week 1

## Families of ML Algorithms

### Linear Models
- separate objects with a plane
- examples: Logistic regression, SVM
- linear models with different loss functions
- good for sparse, high-dimensional data
- often, points cannot be separated by such a simple approach

### Tree-based: Decision Tree, Random Forest, Gradient Boosted Decision Trees
- recursively split spaces by lines parallel to an axis
	  - this reduces the number of possible lines significantly
- another way to look at it: a decision tree separates the data into boxes, and approximates the data in the boxes by constants.
- very powerful general method for tabular data
- hard to capture linear dependencies, since this requires a lot of splits
	  - ex: 2 classes that can be separated by a diagonal line will require many vert/horizontal splits.

### kNN-based methods
- select class labels by majority vote of closest neighbors
- features based on kNN are very informative
- scikit-learn has implementation with algorithmic tricks to speed up calculation

### Neural Networks
- produce smooth non-linear decision boundary

### No Free Lunch Theorem
- "There is no method which outperforms all others for all tasks" or "For every method we can construct a task for which this particular method will not be the best".
- Every method relies on some assumptions for it to work. If the assumptions fail, the method will perform poorly.

## Feature preprocessing and generation with respect to models

### Numeric features
- Some models depend on feature scale; some don't. 
- Tree-based models do not depend on feature scale. 
- NN, kNN, and linear models do depend on feature scale.
	  - For linear models, regularization impact is proportional to feature scale.
	  - For kNN, larger features are treated as more important.

#### Preprocessing
- preprocessing: scaling
	  -  scaling to [0,1] interval (sklearn.preprocessing.MinMaxScaler)
		    - X = (X - X.min()) / (X.max() - X.min())
	  - scaling to have mean 0, std 1 (sklearn.preprocessing.StandardScaler)
	  - scaling should be initially applied to all numeric features equally.
	  - scaling params can be optimized (tuned) to boost features which seem to be more important.
- preprocessing: outliers
	  - to protect models from outliers, can clip feature values between lower and upper bounds (e.g., 1% and 99%). This is widely used in finanical data, and is called Winsorization.
- preprocessing: rank transformation (scipy.stats.rankdata)
  	- set spaces between properly sorted values to be roughly equal.
  	- Can be a better option than MinMaxScaler because the rank transform will move outliers closer to other objects.
  	- Can be used for kNN, NN, and linear models if there is no time to handle outliers manually.
  	- To apply rank transform to TEST data, need to store the mapping; alternatively, concatenate train and test sets before applying rank transform.
- Preprocessing - other transforms that help linear models and especially NN
  	- log transform (np.log(1+x))
  	- raising to the power < 1 (np.sqrt(x + 2/3))
  	- these transforms drag large values closer to the mean
- Additional ideas:
  	- Train models on concatenated data frames produced by different preprocessings
  	- Mix models trained on differently preprocessed data.

#### Feature generation
- Sometimes, features can be engineered by prior knowledge.
- GBDT models have difficulty with approximating multiplications and divisions. Adding new features that are linear combinations of other features can help make the model more robust with less trees.
- Fractional part: if have a feature with numbers after a decimal (e.g., price), add a new feature that is just the fractional part (e.g., .23 is the fractional part of 1.23).

### Categorical and ordinal features

#### Ordinal features
- Ordinal features are categorical features that are ordered in some meaningful way. However, unlike with numeric features, we don't know if the difference between categories 1 and 2 is equal to the difference between categories 2 and 3.
- Examples of ordinal features: ticket class (1,2,3), driver's license (A, B, C, D), Education (kindergarten, school, undergraduate, etc).

#### Label encoding
- The simplest way to encode a categorical feature is to map its values to different numbers. This is known as *label encoding*. This works with trees because a tree method can split features and extract most of the useful information in categories on its own. Non-tree-based methods cannot use label-encoded features effectively.
- label encoding can be accomplished in the following ways:
  	- `sklearn.preprocessing.LabelEncoder` applies encoding in alphabetical order: [S,C,Q] -> [2,1,3].
  	- `Pandas.factorize` applies encoding in order of appearance: [S,C,Q] -> [1,2,3]
  		  - This is useful if the rows are sorted in some meaningful way.
  	- frequency encoding: encode a feature via mapping values to their frequencies: [S,C,Q] -> [0.5,0.3,0.2]
  		  - This preserves some information about distribution of the values.
  		  - If frequency of category is correlated with target value, a linear model will use this dependency.
  		  - A tree model may have a lower number of splits due to the correlation.
  		```{}
  			encoding = titanic.groupby('Embarked').size()
  			encoding = encoding/len(titanic)
  			titanic['enc'] = titanic.Embarked.map(encoding)
  		```
  		  - In the case of ties for variables that occur with the same frequency, a rank operation can be applied: [S,C,Q] -> [0.5,0.3,0.2]
  		```{}
  			from scipy.stats import rankdata
  		```

#### One-hot encoding
- This is useful for linear models.
- One-hot encoded features are already scaled.
- For a data set with a few numeric features and many one-hot encoded features, it may be difficult for a tree model to use the numeric features efficiently. More precisely, tree models will slow down, not always improving on previous results.
- If a categorical feature has too many unique values, one-hot encoding will add too many columns with zero values. Use sparse matrices to store this data efficiently.
- Use `pandas.get_dummies` or `sklearn.preprocessing.OneHotEncoder`.

#### Feature generation for categorical features
- One of the most useful examples of feature generation is interaction between categorical features. This is usually useful for linear models and kNN.
- To implement an interaction between two features, first concatenate the feature values into a string (e.g., "1male", "2female"), and then apply one-hot encoding.

### Datetime and coordinates

### Handling missing values

## Feature extraction from text and images

### Bag of words

### Word2vec, CNN

# Appendix 1 - Links

## Week 1

### Recap of main ML algorithms

#### Links for solving the quiz
- [Explanation of Random Forest](http://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)
- [Explanation/Demonstration of Gradient Boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)
- [Example of kNN](https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/)

#### Additional materials and links
- Overview of methods
    - [Scikit-Learn (or sklearn) library](http://scikit-learn.org/)
    - [Overview of k-NN (sklearn's documentation)](http://scikit-learn.org/stable/modules/neighbors.html)
    - [Overview of Linear Models (sklearn's documentation)](http://scikit-learn.org/stable/modules/linear_model.html)
    - [Overview of Decision Trees (sklearn's documentation)](http://scikit-learn.org/stable/modules/tree.html)
    - [Overview of algorithms and parameters in H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)

- Additional tools
    - [Vowpal Wabbit repository](https://github.com/JohnLangford/vowpal_wabbit)
    - [XGBoost repository](https://github.com/dmlc/xgboost)
    - [LightGBM repository](https://github.com/Microsoft/LightGBM)
    - [Interactive demo of simple feed-forward Neural Net](http://playground.tensorflow.org/)
    - Frameworks for Neural Nets: [Keras](https://keras.io/) , [PyTorch](http://pytorch.org/) , [TensorFlow](https://www.tensorflow.org/) , [MXNet](http://mxnet.io/) , [Lasagne](http://lasagne.readthedocs.io/)
    - [Example from sklearn with different decision surfaces](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)
    - [Arbitrary order factorization machines](https://github.com/geffy/tffm)

### Software/hardware requirements
#### Additional material and links
- StandCloud Computing:
    - [AWS](https://aws.amazon.com/), [Google Cloud](https://cloud.google.com/), [Microsoft Azure](https://azure.microsoft.com/)
- AWS spot option:
    - [Overview of Spot mechanism](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html)
    - [Spot Setup Guide](http://www.datasciencebowl.com/aws_guide/)
- Stack and packages:
  - 


### Feature preprocessing and generation with respect to models
#### Additional material and links

### Feature extraction from text and images
#### Additional material and links



    - []()
    - []()
    - []()
    