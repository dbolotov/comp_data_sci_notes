---
title: "Competitive Data Science Notes"
output:
  html_document:
    theme: cerulean
    highlight: tango
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require("knitr")
```

# Introduction

This is a set of notes for the Competitive Data Science course on Coursera.

# Week 1

## Recap of main machine learning algorithms

### Linear models
- Examples: Logistic regression, SVM.
- Separate objects with a plane.
- Good for sparse, high-dimensional data.
- Often, points cannot be separated by such a simple approach.

### Tree-based: Decision Tree, Random Forest, Gradient Boosted Decision Trees
- Recursively split data by lines parallel to an axis. This significantly reduces the number of possible lines.
- Another way to look at it: a decision tree separates the data into boxes, and approximates the data in the boxes by constants.
- Very powerful general method for tabular data
- Hard to capture linear dependencies, since this requires a lot of splits. For example, 2 classes that can be separated by a diagonal line will require many vertical/horizontal splits.

### kNN-based methods
- Select class labels by majority vote of closest neighbors.
- Features based on kNN are very informative.
- `scikit-learn` has implementation with algorithmic tricks to speed up calculation.

### Neural Networks
- Produce smooth non-linear decision boundary.

### No Free Lunch Theorem
- "There is no method which outperforms all others for all tasks" or "For every method we can construct a task for which this particular method will not be the best".
- Every method relies on some assumptions for it to work. If the assumptions fail, the method will perform poorly.


### Links for solving the quiz
- [Explanation of Random Forest](http://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)
- [Explanation/Demonstration of Gradient Boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)
- [Example of kNN](https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/)


### Additional materials and links
- Overview of methods
    - [Scikit-Learn (or sklearn) library](http://scikit-learn.org/)
    - [Overview of k-NN (sklearn's documentation)](http://scikit-learn.org/stable/modules/neighbors.html)
    - [Overview of Linear Models (sklearn's documentation)](http://scikit-learn.org/stable/modules/linear_model.html)
    - [Overview of Decision Trees (sklearn's documentation)](http://scikit-learn.org/stable/modules/tree.html)
    - [Overview of algorithms and parameters in H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)
- Additional tools
    - [Vowpal Wabbit repository](https://github.com/JohnLangford/vowpal_wabbit)
    - [XGBoost repository](https://github.com/dmlc/xgboost)
    - [LightGBM repository](https://github.com/Microsoft/LightGBM)
    - [Interactive demo of simple feed-forward Neural Net](http://playground.tensorflow.org/)
    - Frameworks for Neural Nets: [Keras](https://keras.io/) , [PyTorch](http://pytorch.org/) , [TensorFlow](https://www.tensorflow.org/) , [MXNet](http://mxnet.io/) , [Lasagne](http://lasagne.readthedocs.io/)
    - [Example from sklearn with different decision surfaces](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)
    - [Arbitrary order factorization machines](https://github.com/geffy/tffm)

## Software/hardware requirements
### Additional material and links
- StandCloud Computing:
    - [AWS](https://aws.amazon.com/), [Google Cloud](https://cloud.google.com/), [Microsoft Azure](https://azure.microsoft.com/)
- AWS spot option:
    - [Overview of Spot mechanism](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html)
    - [Spot Setup Guide](http://www.datasciencebowl.com/aws_guide/)
- Stack and packages:
    - [Basic SciPy stack (ipython, numpy, pandas, matplotlib)](https://www.scipy.org/)
    - [Jupyter Notebook](http://jupyter.org/)
    - [Stand-alone python tSNE package](https://github.com/danielfrg/tsne)
    - Libraries to work with sparse CTR-like data: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)
    - Another tree-based method: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))
    - Python distribution with all-included packages: [Anaconda](https://www.continuum.io/what-is-anaconda)
    - [Blog "datas-frame" (contains posts about effective Pandas usage)](https://tomaugspurger.github.io/)

## Feature preprocessing and generation with respect to models

### Numeric features
- Tree-based models do not depend on feature scale. 
- NN, kNN, and linear models do depend on feature scale.
	  - For linear models, regularization impact is proportional to feature scale.
	  - For kNN, larger features are treated as more important.

#### Preprocessing
- Scaling:
	  - Scaling to [0,1] interval (`sklearn.preprocessing.MinMaxScaler`)
		    - X = (X - X.min()) / (X.max() - X.min())
	  - Scaling to have mean 0, std 1 (`sklearn.preprocessing.StandardScaler`)
	  - Scaling should be initially applied to all numeric features equally.
	  - Scaling params can be optimized (tuned) to boost features which seem to be more important.
- Outliers
	  - To protect models from outliers, can clip feature values between lower and upper bounds (e.g., 1% and 99%). This is widely used in finanical data, and is called Winsorization.
- Rank transformation (`scipy.stats.rankdata`):
  	- set spaces between properly sorted values to be roughly equal.
  	- Can be a better option than `MinMaxScaler` because the rank transform will move outliers closer to other objects.
  	- Can be used for kNN, NN, and linear models if there is no time to handle outliers manually.
  	- To apply rank transform to TEST data, need to store the mapping; alternatively, concatenate train and test sets before applying rank transform.
- Other transforms that help linear models and especially NN:
  	- Log transform (`np.log(1+x)`)
  	- Raising to the power < 1 (`np.sqrt(x + 2/3)`)
  	- These transforms drag large values closer to the mean.
- Additional ideas:
  	- Train models on concatenated data frames produced by different preprocessings.
  	- Mix models trained on differently preprocessed data.

#### Feature generation
- Sometimes, features can be engineered by prior knowledge.
- GBDT models have difficulty with approximating multiplications and divisions. Adding new features that are linear combinations of other features can help make the model more robust with less trees.
- Fractional part: generate a new feature that is the fractional part of a feature with numbers after a decimal (e.g., .23 is the fractional part of 1.23).

### Categorical and ordinal features

#### Ordinal features
- Ordinal features are categorical features that are ordered in some meaningful way. However, unlike with numeric features, we don't know if the difference between categories 1 and 2 is equal to the difference between categories 2 and 3.
- Examples of ordinal features: ticket class (1,2,3), driver's license (A, B, C, D), Education (kindergarten, school, undergraduate, etc).

#### Label encoding
- The simplest way to encode a categorical feature is to map its values to different numbers. This is known as *label encoding*. This works with trees because a tree method can split features and extract most of the useful information in categories on its own. Non-tree-based methods cannot use label-encoded features effectively.
- Label encoding can be accomplished in the following ways:
  	- `sklearn.preprocessing.LabelEncoder` applies encoding in alphabetical order: [S,C,Q] -> [2,1,3].
  	- `Pandas.factorize` applies encoding in order of appearance: [S,C,Q] -> [1,2,3]
  		  - This is useful if the rows are sorted in some meaningful way.
  	- Frequency encoding: encode a feature via mapping values to their frequencies: [S,C,Q] -> [0.5,0.3,0.2]
  		  - This preserves some information about distribution of the values.
  		  - If frequency of category is correlated with target value, a linear model will use this dependency.
  		  - A tree model may have a lower number of splits due to the correlation.
  		```{}
  			encoding = titanic.groupby('Embarked').size()
  			encoding = encoding/len(titanic)
  			titanic['enc'] = titanic.Embarked.map(encoding)
  		```
  		  - In the case of ties for variables that occur with the same frequency, a rank operation can be applied: [S,C,Q] -> [0.5,0.3,0.2]
  		```{}
  			from scipy.stats import rankdata
  		```

#### One-hot encoding
- This is useful for linear models.
- One-hot encoded features are already scaled by definition.
- For a data set with a few numeric features and many one-hot encoded features, it may be difficult for a tree model to use the numeric features efficiently. More precisely, tree models will slow down, not always improving on previous results.
- If a categorical feature has too many unique values, one-hot encoding will add too many columns with zero values. Use sparse matrices to store this data efficiently.
- Use `pandas.get_dummies` or `sklearn.preprocessing.OneHotEncoder`.

#### Feature generation for categorical features
- One of the most useful examples of feature generation is interaction between categorical features. This is usually useful for linear models and kNN.
- To implement an interaction between two features, first concatenate the feature values into a string (e.g., "1male", "2female"), and then apply one-hot encoding.

### Datetime and coordinates
- Both these features differ significantly from numeric and categorical features.
- Since we can interpret the meaning of datetime and coordinates, we can come up with specific ideas about feature generation.

#### Datetime
- Most new features generated from a datetime fall into two categories: time moments in a period, and time passed since a particular event.
- Periodicity: day number in week, month, season, year, second, minute, hour. This is useful to capture repetitive patterns. Can also add non-common periodicity, e.g. when a patient receives medication every 3 days.
- Time since:
    - Row-independent; e.g. since 00:00:00 UTC, 1 January 1970
    - Row-dependent; e.g. number of days left until next holiday, or time passed since last holiday.
- Difference between dates: `datetime_feature_1 - datetime_feature_2`. E.g., for churn prediction, days since last purchase date or days since registration.
- These new generated features will then need to be treated accordingly; e.g. label or one-hot encoding for categorical.

#### Coordinates
- If have geographical coordinates, can add new features like distance to the nearest shop, hospitals, schools, etc.
- Divide a map into squares with a grid, and in each square find the most expensive flat. Then a new feature is the distance to this flat for each object in the square.
- Organize the data into clusters. Use cluster centers as important points, and generate new features as distances to the clusters.
- Find special areas, like an area with very old buildings, and add distance to this area as a feature.
- Calculate aggregated statistics for the area surrounding an object: number of flats (interpreted as the area's popularity), mean real estate price (indicates how expensive the area is).
- Trick for decision trees: can add slightly rotated coordinates as new features. This helps a model make more precise selections on a map. E.g., a street divides an area into high-cost and low-cost district. If the street is diagonal, this will require many splits; with rotated coordinates, this separation can be accomplished in one split. It can be hard to know which rotations to make, so we may want to add all rotations to 45 or 22.5 degrees.

### Handling missing values
- Missing values can look like NaN, empty strings, or outliers like -999.
- Sometimes, missing values can contain useful information.

#### Finding replaced missing values
- Missing values are sometimes already replaced in the data. One way to find such values is to plot a histogram. E.g., a historgram might show an smooth distribution for most numbers, but a large spike for a specific number. The number might be -1 or the mean value of the feature, which means missing values were replaced. 

#### Missing value imputation
- Three ways of dealing with Missing value imputation:
  - Replace by some value outside the normal range (e.g., -999, -1).
      - Pro: allows to take the missing value into a separate category.
      - Con: linear and NN model performance can suffer.
  - Replace with mean or median.
    - Pro: can help NN and linear models.
    - Con: for tree models, can be harder to select object which had missing value in the first place.
  - Reconstruct value
      - In time series, the rows of a dataset are not independent, and interpolation interpolation works well.
      - For data that is not time series, there will usually be no proper logic to reconstruct MVs.
- Feature generation
    - Add a new "isnull" indicator feature, indicating whether a specific feature is null or NaN. This solves the problem with trees and NN while computing the mean or median. However, this method doubles the number of columns in the dataset.
    - Be careful with replacing missing values before feature generation.
        - When encoding a categorical feature together with a numeric feature, ignore missing values when calculating mean values for the numeric features.
        - In general, avoid filling NaNs before feature generation since this can decrease the usefulness of the features.
- XGBoost can handle missing values that are NaN, and this can sometimes change the score drastically.
- Outliers can be treated as missing values when it makes sense.
- Categories which are present in test data but not in train data should be treated as missing values. Encode such a feature with frequencies of occurrence in the concatenated train and test datasets.


### Additional material and links
- Feature preprocessing
    - [Preprocessing in Sklearn](http://scikit-learn.org/stable/modules/preprocessing.html)
    - [Andrew NG about gradient descent and feature scaling](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling)
    - [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)

- Feature generation
    - [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
    - [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)



## Feature extraction from text and images

- Often in competitions, we have data like text or images. If only this kind of data is available, we can apply approach specific to this type of data. For example, we can use search engines to find text similar to that used in the Allen AI Challenge. For images, we can use CNN.
- But if we have text or images as *additional* data, we must craft specific features that can be added as complementary to our main data frame.
  - The Titanic dataset has **name** column, which is more or less like text. To use it, we must first derive useful features from it.
  - As a more serious example, we can predict if two online advertisements are duplicates - copies of each other with slight differences (Avito Duplicate Ads Detection competition). We can use images of the ads as complementary data.

### Feature extraction from text - bag of words
- This is the simplest approach.

#### TF-IDF
- Ceate a new column for each unique word in the data. Then, count the number of occurences of each word, and place this value in the appropriate column. Apply this operation to each row of the dataset. This can be done using  `sklearn.feature_extraction.text.CountVectorizer`.
- We can also post-process the calculated metrics using predefined methods. This may be necessary because some models (kNN, linear, NN) depend on feature scale. So, the main goal of post-processing here is to make samples more comparable, and boost more important features while decreasing the scale of useless features.
- One way to make the samples more comparable is to normalize the sum of values in each row. This counts not *occurences*, but *frequencies* of words. Thus, texts of different sizes will be more comparable. This is known as **Term frequency** transformation:
    ```{}
    tf = 1 / x.sum(axis=1)[:,None]
    x = x * tf
    ```
- To boost more important features, we post-process our matrix by normalizing data column-wise. A good idea is to normalize each feature by the inverse fraction of documents which contain the exact word corresponding to this feature. In this case, features corresponding to frequent words will be scaled down compared to features corresponding to rare words. We can further improve this idea by taking a log of these normalization coefficients. This will decrease the significance of widespread words in the dataset and perform the required feature scaling. This is the purpose of **Inverse Document Frequency** transformation.
    ```{}
    idf = np.log(x.shape[0] / x(>0).sum(0))
    x = x * idf
    ```
- TF and IDF are frequently used together, `sklearn.feature_extraction.text.TfidfVectorizer` in sklearn.
- There are other variants of TF-IDF which may work better for certain datasets.

#### N-grams
- Add not only column corresponding to a word, but also columns corresponding to N consequent words.
- This concept can also be applied to a sequence of chars. In cases with low N, we will have a column with each possible combination of N chars. For example, the number of bigrams for 28 unique symbols is equal to 28*28.
- Sometimes it can be cheaper to have every possible char n-gram as a feature instead of having a feature for each unique word in the data.
- Using char n-grams also helps model handle unseen words; for example, rare forms of already used words.
- In sklearn, `sklearn.feature_extraction.text.CountVectorizer` has an appropriate parameter for using n-grams (`Ngram_range`). To change from word n-grams to char n-grams, use the the parameter named `analyzer`.

#### Text preprocessing
- Usually, we may want to preprocess text before applying bag-of-words; preprocessing can help bag-of-words drastically.
- Lowercase: this reduces the number of redundant columns. `CountVectorizer` does this by default.
- Lemmatization and stemming
    - Stemming is a heuristic process that chops off the endings of words and thus unites derivationally related words. 
        - democracy, democratic, democratization -> democr
        - Saw -> s
    - Lemmatization uses knowledge of vocabulary and morphological analysis of words.
        - democracy, democratic, democratization -> democracy
        - Saw -> see or saw (depending on context)
- Stopwords
    - These are words that are insignificant (articles or prepositions), or are so common that they do not help to solve our task.
    - Most languages have pre-defined lists of stopwords which can be found on the internet or loaded from `NLTK` in python. `CountVectorizer` in sklearn also has a parameter `max_df` related to stopwords.

#### Pipeline for applying BOW
1. Preprocessing: lowercase, stemming, lemmatization, stopwords
2. N-grams can help to use local context
3. Postprocessing: TFiDF


### Feature extraction from text - word2vec
- Just as with the BOW approach, we want to get a vector representation of words and text. However, this approach is more concise.
- Word2vec converts each word to some vector in some sophisticated space which usually has 100's of dimensions. To learn the word embedding, word2vec uses nearby words. Different words that are often used in the same context will be very close in this vector representation, which will benefit our models.
- We can also apply basic operations like addition and subtraction on these vectors and expect the result of such operations to be interpretable. E.g., king + woman + man = queen.
- There are several implementations of this embedding approach besides Word2vec:
  - Words: Glove (global vector for word representation), FastText, etc.
  - Sentences: Complications may occur if we need to derive vectors for sentences. Here we may take different approaches. For example, we can calculate the mean or sum of the word vectors. Or, we can use Doc2vec.
- Training word2vec can take a long time, and pretrained models are available (e.g., trained on Wikipedia). Training does not require target values; it only requires the text to extract context for each word.
    - Note: all pre-processing techniques discussed earlier can be applied to text before training word2vec models.

#### BOW and w2v comparison
- BOW
    - Vectors are very large.
    - The meaning of each value of vector is known.
- Word2vec
    - Vectors are relatively small.
    - Values in the vector can be interpreted only in some cases.
    - Words with similar meaning often have similar embeddings. This is crucial in competitions.
- Usually, BOW and w2v give different results, and can be used together in one solution.

### Feature extraction from images

- Similarly to w2v for words, CNNs give a compressed representation for images.

#### Descriptors
- Besides getting the final output of a network, outputs from the inner layers (*descriptors*) can also be used. 
    - Descriptors from later layers are better to solve tasks similar to solve tasks similar to the one the network was trained on.
    - Descriptors from early layers have more task-independent information.
    - For example, if a CNN was trained on ImageNet, its later layers can be used in some car model classification task. But to use this network in some medicine-specific task, it is better to use an earlier layer, or even retrain the network from scratch.
    
#### Finetuning
- Sometimes, we can slightly tune a network to recieve more suitable representations using target values associate with our images. In general, the process of pretrained model tuning is called *finetuning*.
    - Finetuning, especially for small datasets, is usually better than training a standalone model on descriptors (it allows to tune all network parameters, and thus extract more effective image representations) or training a network from scratch (model can use domain knowledge already encoded in network parameters, which can lead to faster results and a faster retraining procedure).
  
#### Augmentation
- Augmentation increases the number of images to train a better network.
- Although augmentation is not as good as adding brand new images to a training set, this is still very useful.
- Augmentation reduces overfitting and allows to train more robust models.
- Examples of augmentation: crop, rotate, add noise.

### Additional material and links
- Feature extraction from text
    - Bag of words
        - [Feature extraction from text with Sklearn](http://scikit-learn.org/stable/modules/feature_extraction.html)
        - [More examples of using Sklearn](https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/)
    - Word2vec
        - [Tutorial to Word2vec](https://www.tensorflow.org/tutorials/word2vec)
        - [Tutorial to word2vec usage](https://rare-technologies.com/word2vec-tutorial/)
        - [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)
        - [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)
- NLP Libraries
    - [NLTK](http://www.nltk.org/)
    - [TextBlob](https://github.com/sloria/TextBlob)
- Feature extraction from images
    - Pretrained models
        - [Using pretrained models in Keras](https://keras.io/applications/)
        - [Image classification with a pre-trained deep neural network](https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11)
- Finetuning
    - [How to Retrain Inception's Final Layer for New Categories in Tensorflow](https://www.tensorflow.org/tutorials/image_retraining)
    - [Fine-tuning Deep Learning Models in Keras](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)


# Week 2

## Exploratory Data Analysis (EDA)

### EDA: what and why?
- EDA allows to:
    - Better understand the data.
    - Build intuition about the data.
    - Generate hypotheses about new features.
    - Find insights.
- One of the main EDA tools is visualization. When we visualize the data, we immediately see patterns. 

### Building intuition about the data

#### Getting domain knowledge
- It is preferable to understand what the aim is, what data we have, and how people usually tackle the specific kind of problem to build a baseline.
- The first step would be searching the internet on the topic and making sure we understand the data.
  - Example: when predicting advertising cost, look into Google Adwords.
#### Checking if the data is intuitive
- E.g. if there is a column with age data, check for strange outliers.
- E.g. if number of clicks is higher than the number of impressions, something is wrong with that data row.
- Such mistakes can be used to generate new features. E.g. for the advertising data, if the number of impressions is 0 but clicks is non-zero, add a new indicator column `is_incorrect` that is 1 when clicks > impressions.

#### Understanding how the data was generated
- What was the algorithm for sampling objects from the database? Maybe the competition host sampled the data at random, or they oversampled a particular class.
- Were the train and test data generated with the same algorithm? If the train and test sets are different, we cannot use part of the train set for validation, since it will not be representative of the test set.
- It is crucial to understand the data generation process to set up a proper validation scheme.

### Exploring anonymized data

- Sometimes, competition organizers do not want some information to be revealed, so they make an effort to export the data in a way that one could not get value out of it. Yet, all the features are preserved, and the ML model would be able to do its job.
    - For example, if a company wants a model to classify its documents, but doesn't want to reveal the document content, it can replace all the word occurences with hash values of those words.
- Things to try while exploring this data:
    - Explore individual features:
        - Guess the meaning of the columns.
        - Guess the type of the column (categorical, numeric, text, date). 
    - Explore feature relations:
        - Find relations between pairs.
        - Find feature groups.
- Example on competition data:
  - Build a quick random forest model and check feature importances.
  - Check feature mean and standard deviation. If a feature was standard scaled (mean close to 1 and std close to 0), it may be possible to unscale.
  - Check counts of unique feature values to see if any values are frequently repeated.
- Helpul functions:
    - `df.dtypes`: pandas function that guesses column types.
    - `df.info`
    - `x.value_counts()`
    - `x.isnull()`

### Visualizations

- EDA is an art, but there are several tools.

#### Tools for exploring individual features
- Histograms
    - Try to vary the number of bins, because histograms can be deceiving.
    - Use transformations (like log) before plotting historam. This can reveal that the data has a specific distribution.
    - If a particular value (like the mean) occurs very frequently, it may indicate that missing values were replaced. Can then use this information in various ways: convert back to NaN for XGBoost, add a new feature indicating NaN or not, replace with a different value, impute, etc.
    - Use `plt.hist(x)`.
- Plot index versus value
    - Scatter plot row index versus feature value.
    - Horizontal lines in this plot indicate a lot of repeated values ina  particular feature.
    - Vertical lines indicate that the data may not be properly shuffled.
    - Use `plt.plot(x, '.')`.
- Plot index versus value with color coding for class labels.
    - This shows whether a feature is correlated with class separation, and whether the data is shuffled.
    - Use `plt.scatter(range(len(x)), x, c=y)`.
- Check statistics
    - Check mean, std, and percentiles.
    - Use pandas `df.describe()`.
- Other tools
    - value counts: `x.value_counts()`.
    - null: `x.isnull()`. Null patterns can be plotted as row index vs feature index, colored by description (NA, empty string, -1, very large number, -99999 (and less), 999, 99).

#### Tools for exploring feature relations
- Sometimes it's beneficial to look at feature pairs.
- The best tool here is scatterplot: `plt.scatter(x1,x2)`. 
    - Plot one feature against another, and color by class label for classification or target value for regression. Can also visualize a regression target value by point size.
    - Check if the distributions of the train and test sets are the same. Plot two features from the train set and color by class, then plot the same features from the test set and color all test observations gray. For example, if the gray points are located in a region where there are no train points, this means that the train and test sets are in different distributions. In this case, make the same kind of plot for other feature pairs to rule out that this specific feature pair is not overfitted and that there is no bug in the code.
    - If the data only has a small number of features, it is possible to plot all feature pairs at once using `pd.scatter_matrix(df)`.
    - It is a good idea to look at scatter plots and histograms at the same time, since scatter plots give weak information about densities and histograms do not show feature interactions.
- We can also compute some kind of distance between the feature columns and store them in a matrix of size num_of_features^2.
    - For example, we can compute correlation between the columns using `df.corr()`, and plot it with `plt.matshow(...)`.
    - We can also compute other numbers:
        - How many times is one feature larger than the other?
        - How many distinct combinations do features have in the dataset?
    - If the matrix looks like a mess, it is possible to run a clustering (e.g. kmeans) on the matrix and reorder the features. This may visualize feature groups.

#### Tools for exploring feature groups
- A clustered feature matrix may have feature groups, and it is usually a good idea to generate new features based on the groups. E.g., some statistics calculated over the group may work well.
- Calculate the statistic (e.g., mean value) of each feature, and then plot it against column index: `df.mean().plot(style='.')`.
    - The plot may look random if the columns are shuffled, but may reveal a pattern when the columns are sorted based on the statistic: `df.mean().sort_values().plot(style='.')`.
    - If there is a pattern, use the feature groups to generate new features.
  
### Data cleaning and other things to check

It is important to understand that competition data can be only a part of the data that the organizers have. The organizers could provide a fraction of the observations they have, or a fraction of the features.

#### Constant features
- A feature can have only one constant value in both training and test sets, or a constant in the training set, and different values in the test set.
- This could be due to the sampling procedure (E.g., the competition only has data from one particular year, while the full dataset spans several years).
- Such features are not useful for the model, and should be removed.
- Use `traintest.nunique(axis=1) == 1` to check.
- If there are values that are only present in the test set, this situation needs to be handled properly. We need to decide if these values matter much or not. This can be checked with a separate validation set, comparing the prediction quality using observations with features with same values, and observations with new values. Then, we decide if we should remove this feature, or create a separate model that works with these new feature values.

#### Duplicated features
- Sometimes features are identical.
- Use `traintest.T.drop_duplicates(0)` to drop duplicate columns.
- If categorical features have duplicates but their levels have different names, drop the features using label encoding:
    ```{}
    for f in categorical_feats:
        traintest[f] = traintest[f].factorize()
    traintest.T.drop_duplicates()
    ```

#### Duplicated rows
- If there are duplicate rows, check if they have the same label.
- Duplicate rows may be the result of a mistake. Try to understand why.
- Check if train and test sets have the same rows. We can manually set labels for the test rows that are present in the train set.

#### Dataset shuffling
- Check if the dataset is shuffled. If it is not, there is a high chance that there is a data leakage.
- Plot a feature or target versus row index. Optionally, smooth the values using a running average. Also plot the mean of the feature vs the index on the same plot (this should be a horizontal line). If the data was shuffled properly, there should be a random spread around the mean value.
    - In the example shown, the end of the train set has a lower rolling mean than the rest of the data. This may not necessarily generate a new feature, but is useful to understand.
  
#### Cool visualizations
- Try to visualize every possible thing in the dataset, because visualizations will lead to magic features.

### Additional material and links
- Visualization tools
    - [Seaborn](https://seaborn.pydata.org/)
    - [Plotly](https://plot.ly/python/)
    - [Bokeh](https://github.com/bokeh/bokeh)
    - [ggplot](http://ggplot.yhathq.com/)
    - [Graph visualization with NetworkX](https://networkx.github.io/)
- Others
    - [Biclustering algorithms for sorting corrplots](http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html)


  