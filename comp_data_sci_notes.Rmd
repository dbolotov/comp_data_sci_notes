---
title: "Competitive Data Science Notes"
output:
  html_document:
    theme: cerulean
    highlight: tango
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require("knitr")
```

```{r echo=FALSE}
txc <- function(txt, col) {
  # color input text inline
  paste("<font color='",col,"'>",txt,"</font>",sep="")
}
```

# Introduction

This is a set of notes for the Competitive Data Science course on Coursera.

***

# Week 1

## Recap of main machine learning algorithms

### Linear models
- Examples: Logistic regression, SVM.
- Separate objects with a plane.
- Good for sparse, high-dimensional data.
- Often, points cannot be separated by such a simple approach.

### Tree-based: Decision Tree, Random Forest, Gradient Boosted Decision Trees
- Recursively split data by lines parallel to an axis. This significantly reduces the number of possible lines.
- Another way to look at it: a decision tree separates the data into boxes, and approximates the data in the boxes by constants.
- Very powerful general method for tabular data
- Hard to capture linear dependencies, since this requires a lot of splits. For example, 2 classes that can be separated by a diagonal line will require many vertical/horizontal splits.

### kNN-based methods
- Select class labels by majority vote of closest neighbors.
- Features based on kNN are very informative.
- `scikit-learn` has implementation with algorithmic tricks to speed up calculation.

### Neural Networks
- Produce smooth non-linear decision boundary.

### No Free Lunch Theorem
- "There is no method which outperforms all others for all tasks" or "For every method we can construct a task for which this particular method will not be the best".
- Every method relies on some assumptions for it to work. If the assumptions fail, the method will perform poorly.


### Links for solving the quiz
- [Explanation of Random Forest](http://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)
- [Explanation/Demonstration of Gradient Boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)
- [Example of kNN](https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/)


### Additional materials and links
- Overview of methods
    - [Scikit-Learn (or sklearn) library](http://scikit-learn.org/)
    - [Overview of k-NN (sklearn's documentation)](http://scikit-learn.org/stable/modules/neighbors.html)
    - [Overview of Linear Models (sklearn's documentation)](http://scikit-learn.org/stable/modules/linear_model.html)
    - [Overview of Decision Trees (sklearn's documentation)](http://scikit-learn.org/stable/modules/tree.html)
    - [Overview of algorithms and parameters in H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)
- Additional tools
    - [Vowpal Wabbit repository](https://github.com/JohnLangford/vowpal_wabbit)
    - [XGBoost repository](https://github.com/dmlc/xgboost)
    - [LightGBM repository](https://github.com/Microsoft/LightGBM)
    - [Interactive demo of simple feed-forward Neural Net](http://playground.tensorflow.org/)
    - Frameworks for Neural Nets: [Keras](https://keras.io/) , [PyTorch](http://pytorch.org/) , [TensorFlow](https://www.tensorflow.org/) , [MXNet](http://mxnet.io/) , [Lasagne](http://lasagne.readthedocs.io/)
    - [Example from sklearn with different decision surfaces](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)
    - [Arbitrary order factorization machines](https://github.com/geffy/tffm)

## Software/hardware requirements
### Additional material and links
- StandCloud Computing:
    - [AWS](https://aws.amazon.com/), [Google Cloud](https://cloud.google.com/), [Microsoft Azure](https://azure.microsoft.com/)
- AWS spot option:
    - [Overview of Spot mechanism](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html)
    - [Spot Setup Guide](http://www.datasciencebowl.com/aws_guide/)
- Stack and packages:
    - [Basic SciPy stack (ipython, numpy, pandas, matplotlib)](https://www.scipy.org/)
    - [Jupyter Notebook](http://jupyter.org/)
    - [Stand-alone python tSNE package](https://github.com/danielfrg/tsne)
    - Libraries to work with sparse CTR-like data: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)
    - Another tree-based method: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))
    - Python distribution with all-included packages: [Anaconda](https://www.continuum.io/what-is-anaconda)
    - [Blog "datas-frame" (contains posts about effective Pandas usage)](https://tomaugspurger.github.io/)

## Feature preprocessing and generation with respect to models

### Numeric features
- Tree-based models do not depend on feature scale. 
- NN, kNN, and linear models do depend on feature scale.
	  - For linear models, regularization impact is proportional to feature scale.
	  - For kNN, larger features are treated as more important.

#### Preprocessing
- Scaling:
	  - Scaling to [0,1] interval (`sklearn.preprocessing.MinMaxScaler`)
		    - X = (X - X.min()) / (X.max() - X.min())
	  - Scaling to have mean 0, std 1 (`sklearn.preprocessing.StandardScaler`)
	  - Scaling should be initially applied to all numeric features equally.
	  - Scaling params can be optimized (tuned) to boost features which seem to be more important.
- Outliers
	  - To protect models from outliers, can clip feature values between lower and upper bounds (e.g., 1% and 99%). This is widely used in finanical data, and is called Winsorization.
- Rank transformation (`scipy.stats.rankdata`):
  	- set spaces between properly sorted values to be roughly equal.
  	- Can be a better option than `MinMaxScaler` because the rank transform will move outliers closer to other objects.
  	- Can be used for kNN, NN, and linear models if there is no time to handle outliers manually.
  	- To apply rank transform to TEST data, need to store the mapping; alternatively, concatenate train and test sets before applying rank transform.
- Other transforms that help linear models and especially NN:
  	- Log transform (`np.log(1+x)`)
  	- Raising to the power < 1 (`np.sqrt(x + 2/3)`)
  	- These transforms drag large values closer to the mean.
- Additional ideas:
  	- Train models on concatenated data frames produced by different preprocessings.
  	- Mix models trained on differently preprocessed data.

#### Feature generation
- Sometimes, features can be engineered by prior knowledge.
- GBDT models have difficulty with approximating multiplications and divisions. Adding new features that are linear combinations of other features can help make the model more robust with less trees.
- Fractional part: generate a new feature that is the fractional part of a feature with numbers after a decimal (e.g., .23 is the fractional part of 1.23).

### Categorical and ordinal features

#### Ordinal features
- Ordinal features are categorical features that are ordered in some meaningful way. However, unlike with numeric features, we don't know if the difference between categories 1 and 2 is equal to the difference between categories 2 and 3.
- Examples of ordinal features: ticket class (1,2,3), driver's license (A, B, C, D), Education (kindergarten, school, undergraduate, etc).

#### Label encoding
- The simplest way to encode a categorical feature is to map its values to different numbers. This is known as *label encoding*. This works with trees because a tree method can split features and extract most of the useful information in categories on its own. Non-tree-based methods cannot use label-encoded features effectively.
- Label encoding can be accomplished in the following ways:
  	- `sklearn.preprocessing.LabelEncoder` applies encoding in alphabetical order: [S,C,Q] -> [2,1,3].
  	- `Pandas.factorize` applies encoding in order of appearance: [S,C,Q] -> [1,2,3]
  		  - This is useful if the rows are sorted in some meaningful way.
  	- Frequency encoding: encode a feature via mapping values to their frequencies: [S,C,Q] -> [0.5,0.3,0.2]
  		  - This preserves some information about distribution of the values.
  		  - If frequency of category is correlated with target value, a linear model will use this dependency.
  		  - A tree model may have a lower number of splits due to the correlation.
  		```{}
  			encoding = titanic.groupby('Embarked').size()
  			encoding = encoding/len(titanic)
  			titanic['enc'] = titanic.Embarked.map(encoding)
  		```
  		  - In the case of ties for variables that occur with the same frequency, a rank operation can be applied: [S,C,Q] -> [0.5,0.3,0.2]
  		```{}
  			from scipy.stats import rankdata
  		```

#### One-hot encoding
- This is useful for linear models.
- One-hot encoded features are already scaled by definition.
- For a data set with a few numeric features and many one-hot encoded features, it may be difficult for a tree model to use the numeric features efficiently. More precisely, tree models will slow down, not always improving on previous results.
- If a categorical feature has too many unique values, one-hot encoding will add too many columns with zero values. Use sparse matrices to store this data efficiently.
- Use `pandas.get_dummies` or `sklearn.preprocessing.OneHotEncoder`.

#### Feature generation for categorical features
- One of the most useful examples of feature generation is interaction between categorical features. This is usually useful for linear models and kNN.
- To implement an interaction between two features, first concatenate the feature values into a string (e.g., "1male", "2female"), and then apply one-hot encoding.

### Datetime and coordinates
- Both these features differ significantly from numeric and categorical features.
- Since we can interpret the meaning of datetime and coordinates, we can come up with specific ideas about feature generation.

#### Datetime
- Most new features generated from a datetime fall into two categories: time moments in a period, and time passed since a particular event.
- Periodicity: day number in week, month, season, year, second, minute, hour. This is useful to capture repetitive patterns. Can also add non-common periodicity, e.g. when a patient receives medication every 3 days.
- Time since:
    - Row-independent; e.g. since 00:00:00 UTC, 1 January 1970
    - Row-dependent; e.g. number of days left until next holiday, or time passed since last holiday.
- Difference between dates: `datetime_feature_1 - datetime_feature_2`. E.g., for churn prediction, days since last purchase date or days since registration.
- These new generated features will then need to be treated accordingly; e.g. label or one-hot encoding for categorical.

#### Coordinates
- If have geographical coordinates, can add new features like distance to the nearest shop, hospitals, schools, etc.
- Divide a map into squares with a grid, and in each square find the most expensive flat. Then a new feature is the distance to this flat for each object in the square.
- Organize the data into clusters. Use cluster centers as important points, and generate new features as distances to the clusters.
- Find special areas, like an area with very old buildings, and add distance to this area as a feature.
- Calculate aggregated statistics for the area surrounding an object: number of flats (interpreted as the area's popularity), mean real estate price (indicates how expensive the area is).
- Trick for decision trees: can add slightly rotated coordinates as new features. This helps a model make more precise selections on a map. E.g., a street divides an area into high-cost and low-cost district. If the street is diagonal, this will require many splits; with rotated coordinates, this separation can be accomplished in one split. It can be hard to know which rotations to make, so we may want to add all rotations to 45 or 22.5 degrees.

### Handling missing values
- Missing values can look like NaN, empty strings, or outliers like -999.
- Sometimes, missing values can contain useful information.

#### Finding replaced missing values
- Missing values are sometimes already replaced in the data. One way to find such values is to plot a histogram. E.g., a historgram might show an smooth distribution for most numbers, but a large spike for a specific number. The number might be -1 or the mean value of the feature, which means missing values were replaced. 

#### Missing value imputation
- Three ways of dealing with Missing value imputation:
    - Replace by some value outside the normal range (e.g., -999, -1).
        - Pro: allows to take the missing value into a separate category.
        - Con: linear and NN model performance can suffer.
    - Replace with mean or median.
        - Pro: can help NN and linear models.
        - Con: for tree models, can be harder to select object which had missing value in the first place.
    - Reconstruct value
        - In time series, the rows of a dataset are not independent, and interpolation interpolation works well.
        - For data that is not time series, there will usually be no proper logic to reconstruct MVs.
- Feature generation
    - Add a new "isnull" indicator feature, indicating whether a specific feature is null or NaN. This solves the problem with trees and NN while computing the mean or median. However, this method doubles the number of columns in the dataset.
    - Be careful with replacing missing values before feature generation.
        - When encoding a categorical feature together with a numeric feature, ignore missing values when calculating mean values for the numeric features.
        - In general, avoid filling NaNs before feature generation since this can decrease the usefulness of the features.
- XGBoost can handle missing values that are NaN, and this can sometimes change the score drastically.
- Outliers can be treated as missing values when it makes sense.
- Categories which are present in test data but not in train data should be treated as missing values. Encode such a feature with frequencies of occurrence in the concatenated train and test datasets.


### Additional material and links
- Feature preprocessing
    - [Preprocessing in Sklearn](http://scikit-learn.org/stable/modules/preprocessing.html)
    - [Andrew NG about gradient descent and feature scaling](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling)
    - [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)

- Feature generation
    - [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
    - [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)



## Feature extraction from text and images

- Often in competitions, we have data like text or images. If only this kind of data is available, we can apply approach specific to this type of data. For example, we can use search engines to find text similar to that used in the Allen AI Challenge. For images, we can use CNN.
- But if we have text or images as *additional* data, we must craft specific features that can be added as complementary to our main data frame.
  - The Titanic dataset has **name** column, which is more or less like text. To use it, we must first derive useful features from it.
  - As a more serious example, we can predict if two online advertisements are duplicates - copies of each other with slight differences (Avito Duplicate Ads Detection competition). We can use images of the ads as complementary data.

### Feature extraction from text - bag of words
- This is the simplest approach.

#### TF-IDF
- Ceate a new column for each unique word in the data. Then, count the number of occurences of each word, and place this value in the appropriate column. Apply this operation to each row of the dataset. This can be done using  `sklearn.feature_extraction.text.CountVectorizer`.
- We can also post-process the calculated metrics using predefined methods. This may be necessary because some models (kNN, linear, NN) depend on feature scale. So, the main goal of post-processing here is to make samples more comparable, and boost more important features while decreasing the scale of useless features.
- One way to make the samples more comparable is to normalize the sum of values in each row. This counts not *occurences*, but *frequencies* of words. Thus, texts of different sizes will be more comparable. This is known as **Term frequency** transformation:
    ```{}
    tf = 1 / x.sum(axis=1)[:,None]
    x = x * tf
    ```
- To boost more important features, we post-process our matrix by normalizing data column-wise. A good idea is to normalize each feature by the inverse fraction of documents which contain the exact word corresponding to this feature. In this case, features corresponding to frequent words will be scaled down compared to features corresponding to rare words. We can further improve this idea by taking a log of these normalization coefficients. This will decrease the significance of widespread words in the dataset and perform the required feature scaling. This is the purpose of **Inverse Document Frequency** transformation.
    ```{}
    idf = np.log(x.shape[0] / x(>0).sum(0))
    x = x * idf
    ```
- TF and IDF are frequently used together, `sklearn.feature_extraction.text.TfidfVectorizer` in sklearn.
- There are other variants of TF-IDF which may work better for certain datasets.

#### N-grams
- Add not only column corresponding to a word, but also columns corresponding to N consequent words.
- This concept can also be applied to a sequence of chars. In cases with low N, we will have a column with each possible combination of N chars. For example, the number of bigrams for 28 unique symbols is equal to 28*28.
- Sometimes it can be cheaper to have every possible char n-gram as a feature instead of having a feature for each unique word in the data.
- Using char n-grams also helps model handle unseen words; for example, rare forms of already used words.
- In sklearn, `sklearn.feature_extraction.text.CountVectorizer` has an appropriate parameter for using n-grams (`Ngram_range`). To change from word n-grams to char n-grams, use the the parameter named `analyzer`.

#### Text preprocessing
- Usually, we may want to preprocess text before applying bag-of-words; preprocessing can help bag-of-words drastically.
- Lowercase: this reduces the number of redundant columns. `CountVectorizer` does this by default.
- Lemmatization and stemming
    - Stemming is a heuristic process that chops off the endings of words and thus unites derivationally related words. 
        - democracy, democratic, democratization -> democr
        - Saw -> s
    - Lemmatization uses knowledge of vocabulary and morphological analysis of words.
        - democracy, democratic, democratization -> democracy
        - Saw -> see or saw (depending on context)
- Stopwords
    - These are words that are insignificant (articles or prepositions), or are so common that they do not help to solve our task.
    - Most languages have pre-defined lists of stopwords which can be found on the internet or loaded from `NLTK` in python. `CountVectorizer` in sklearn also has a parameter `max_df` related to stopwords.

#### Pipeline for applying BOW
1. Preprocessing: lowercase, stemming, lemmatization, stopwords
2. N-grams can help to use local context
3. Postprocessing: TFiDF


### Feature extraction from text - word2vec
- Just as with the BOW approach, we want to get a vector representation of words and text. However, this approach is more concise.
- Word2vec converts each word to some vector in some sophisticated space which usually has 100's of dimensions. To learn the word embedding, word2vec uses nearby words. Different words that are often used in the same context will be very close in this vector representation, which will benefit our models.
- We can also apply basic operations like addition and subtraction on these vectors and expect the result of such operations to be interpretable. E.g., king + woman + man = queen.
- There are several implementations of this embedding approach besides Word2vec:
  - Words: Glove (global vector for word representation), FastText, etc.
  - Sentences: Complications may occur if we need to derive vectors for sentences. Here we may take different approaches. For example, we can calculate the mean or sum of the word vectors. Or, we can use Doc2vec.
- Training word2vec can take a long time, and pretrained models are available (e.g., trained on Wikipedia). Training does not require target values; it only requires the text to extract context for each word.
    - Note: all pre-processing techniques discussed earlier can be applied to text before training word2vec models.

#### BOW and w2v comparison
- BOW
    - Vectors are very large.
    - The meaning of each value of vector is known.
- Word2vec
    - Vectors are relatively small.
    - Values in the vector can be interpreted only in some cases.
    - Words with similar meaning often have similar embeddings. This is crucial in competitions.
- Usually, BOW and w2v give different results, and can be used together in one solution.

### Feature extraction from images

- Similarly to w2v for words, CNNs give a compressed representation for images.

#### Descriptors
- Besides getting the final output of a network, outputs from the inner layers (*descriptors*) can also be used. 
    - Descriptors from later layers are better to solve tasks similar to solve tasks similar to the one the network was trained on.
    - Descriptors from early layers have more task-independent information.
    - For example, if a CNN was trained on ImageNet, its later layers can be used in some car model classification task. But to use this network in some medicine-specific task, it is better to use an earlier layer, or even retrain the network from scratch.
    
#### Finetuning
- Sometimes, we can slightly tune a network to recieve more suitable representations using target values associate with our images. In general, the process of pretrained model tuning is called *finetuning*.
    - Finetuning, especially for small datasets, is usually better than training a standalone model on descriptors (it allows to tune all network parameters, and thus extract more effective image representations) or training a network from scratch (model can use domain knowledge already encoded in network parameters, which can lead to faster results and a faster retraining procedure).
  
#### Augmentation
- Augmentation increases the number of images to train a better network.
- Although augmentation is not as good as adding brand new images to a training set, this is still very useful.
- Augmentation reduces overfitting and allows to train more robust models.
- Examples of augmentation: crop, rotate, add noise.

### Additional material and links
- Feature extraction from text
    - Bag of words
        - [Feature extraction from text with Sklearn](http://scikit-learn.org/stable/modules/feature_extraction.html)
        - [More examples of using Sklearn](https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/)
    - Word2vec
        - [Tutorial to Word2vec](https://www.tensorflow.org/tutorials/word2vec)
        - [Tutorial to word2vec usage](https://rare-technologies.com/word2vec-tutorial/)
        - [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)
        - [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)
- NLP Libraries
    - [NLTK](http://www.nltk.org/)
    - [TextBlob](https://github.com/sloria/TextBlob)
- Feature extraction from images
    - Pretrained models
        - [Using pretrained models in Keras](https://keras.io/applications/)
        - [Image classification with a pre-trained deep neural network](https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11)
- Finetuning
    - [How to Retrain Inception's Final Layer for New Categories in Tensorflow](https://www.tensorflow.org/tutorials/image_retraining)
    - [Fine-tuning Deep Learning Models in Keras](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)

* * *

# Week 2

## Exploratory Data Analysis (EDA)

### EDA: what and why?
- EDA allows to:
    - Better understand the data.
    - Build intuition about the data.
    - Generate hypotheses about new features.
    - Find insights.
- One of the main EDA tools is visualization. When we visualize the data, we immediately see patterns. 

### Building intuition about the data

#### Getting domain knowledge
- It is preferable to understand what the aim is, what data we have, and how people usually tackle the specific kind of problem to build a baseline.
- The first step would be searching the internet on the topic and making sure we understand the data.
    - Example: when predicting advertising cost, look into Google Adwords.

#### Checking if the data is intuitive
- E.g. if there is a column with age data, check for strange outliers.
- E.g. if number of clicks is higher than the number of impressions, something is wrong with that data row.
- Such mistakes can be used to generate new features. E.g. for the advertising data, if the number of impressions is 0 but clicks is non-zero, add a new indicator column `is_incorrect` that is 1 when clicks > impressions.

#### Understanding how the data was generated
- What was the algorithm for sampling objects from the database? Maybe the competition host sampled the data at random, or they oversampled a particular class.
- Were the train and test data generated with the same algorithm? If the train and test sets are different, we cannot use part of the train set for validation, since it will not be representative of the test set.
- It is crucial to understand the data generation process to set up a proper validation scheme.

### Exploring anonymized data

- Sometimes, competition organizers do not want some information to be revealed, so they make an effort to export the data in a way that one could not get value out of it. Yet, all the features are preserved, and the ML model would be able to do its job.
    - For example, if a company wants a model to classify its documents, but doesn't want to reveal the document content, it can replace all the word occurences with hash values of those words.
- Things to try while exploring this data:
    - Explore individual features:
        - Guess the meaning of the columns.
        - Guess the type of the column (categorical, numeric, text, date). 
    - Explore feature relations:
        - Find relations between pairs.
        - Find feature groups.
- Example on competition data:
  - Build a quick random forest model and check feature importances.
  - Check feature mean and standard deviation. If a feature was standard scaled (mean close to 1 and std close to 0), it may be possible to unscale.
  - Check counts of unique feature values to see if any values are frequently repeated.
- Helpul functions:
    - `df.dtypes`: pandas function that guesses column types.
    - `df.info`
    - `x.value_counts()`
    - `x.isnull()`

### Visualizations

- EDA is an art, but there are several tools.

#### Tools for exploring individual features
- Histograms
    - Try to vary the number of bins, because histograms can be deceiving.
    - Use transformations (like log) before plotting historam. This can reveal that the data has a specific distribution.
    - If a particular value (like the mean) occurs very frequently, it may indicate that missing values were replaced. Can then use this information in various ways: convert back to NaN for XGBoost, add a new feature indicating NaN or not, replace with a different value, impute, etc.
    - Use `plt.hist(x)`.
- Plot index versus value
    - Scatter plot row index versus feature value.
    - Horizontal lines in this plot indicate a lot of repeated values ina  particular feature.
    - Vertical lines indicate that the data may not be properly shuffled.
    - Use `plt.plot(x, '.')`.
- Plot index versus value with color coding for class labels.
    - This shows whether a feature is correlated with class separation, and whether the data is shuffled.
    - Use `plt.scatter(range(len(x)), x, c=y)`.
- Check statistics
    - Check mean, std, and percentiles.
    - Use pandas `df.describe()`.
- Other tools
    - value counts: `x.value_counts()`.
    - null: `x.isnull()`. Null patterns can be plotted as row index vs feature index, colored by description (NA, empty string, -1, very large number, -99999 (and less), 999, 99).

#### Tools for exploring feature relations
- Sometimes it's beneficial to look at feature pairs.
- The best tool here is scatterplot: `plt.scatter(x1,x2)`. 
    - Plot one feature against another, and color by class label for classification or target value for regression. Can also visualize a regression target value by point size.
    - Check if the distributions of the train and test sets are the same. Plot two features from the train set and color by class, then plot the same features from the test set and color all test observations gray. For example, if the gray points are located in a region where there are no train points, this means that the train and test sets are in different distributions. In this case, make the same kind of plot for other feature pairs to rule out that this specific feature pair is not overfitted and that there is no bug in the code.
    - If the data only has a small number of features, it is possible to plot all feature pairs at once using `pd.scatter_matrix(df)`.
    - It is a good idea to look at scatter plots and histograms at the same time, since scatter plots give weak information about densities and histograms do not show feature interactions.
- We can also compute some kind of distance between the feature columns and store them in a matrix of size num_of_features^2.
    - For example, we can compute correlation between the columns using `df.corr()`, and plot it with `plt.matshow(...)`.
    - We can also compute other numbers:
        - How many times is one feature larger than the other?
        - How many distinct combinations do features have in the dataset?
    - If the matrix looks like a mess, it is possible to run a clustering (e.g. kmeans) on the matrix and reorder the features. This may visualize feature groups.

#### Tools for exploring feature groups
- A clustered feature matrix may have feature groups, and it is usually a good idea to generate new features based on the groups. E.g., some statistics calculated over the group may work well.
- Calculate the statistic (e.g., mean value) of each feature, and then plot it against column index: `df.mean().plot(style='.')`.
    - The plot may look random if the columns are shuffled, but may reveal a pattern when the columns are sorted based on the statistic: `df.mean().sort_values().plot(style='.')`.
    - If there is a pattern, use the feature groups to generate new features.
  
### Data cleaning and other things to check

It is important to understand that competition data can be only a part of the data that the organizers have. The organizers could provide a fraction of the observations they have, or a fraction of the features.

#### Constant features
- A feature can have only one constant value in both training and test sets, or a constant in the training set, and different values in the test set.
- This could be due to the sampling procedure (E.g., the competition only has data from one particular year, while the full dataset spans several years).
- Such features are not useful for the model, and should be removed.
- Use `traintest.nunique(axis=1) == 1` to check.
- If there are values that are only present in the test set, this situation needs to be handled properly. We need to decide if these values matter much or not. This can be checked with a separate validation set, comparing the prediction quality using observations with features with same values, and observations with new values. Then, we decide if we should remove this feature, or create a separate model that works with these new feature values.

#### Duplicated features
- Sometimes features are identical.
- Use `traintest.T.drop_duplicates(0)` to drop duplicate columns.
- If categorical features have duplicates but their levels have different names, drop the features using label encoding:
    ```{}
    for f in categorical_feats:
        traintest[f] = traintest[f].factorize()
    traintest.T.drop_duplicates()
    ```

#### Duplicated rows
- If there are duplicate rows, check if they have the same label.
- Duplicate rows may be the result of a mistake. Try to understand why.
- Check if train and test sets have the same rows. We can manually set labels for the test rows that are present in the train set.

#### Dataset shuffling
- Check if the dataset is shuffled. If it is not, there is a high chance that there is a data leakage.
- Plot a feature or target versus row index. Optionally, smooth the values using a running average. Also plot the mean of the feature vs the index on the same plot (this should be a horizontal line). If the data was shuffled properly, there should be a random spread around the mean value.
    - In the example shown, the end of the train set has a lower rolling mean than the rest of the data. This may not necessarily generate a new feature, but is useful to understand.
  
#### Cool visualizations
- Try to visualize every possible thing in the dataset, because visualizations will lead to magic features.

### Additional material and links
- Visualization tools
    - [Seaborn](https://seaborn.pydata.org/)
    - [Plotly](https://plot.ly/python/)
    - [Bokeh](https://github.com/bokeh/bokeh)
    - [ggplot](http://ggplot.yhathq.com/)
    - [Graph visualization with NetworkX](https://networkx.github.io/)
- Others
    - [Biclustering algorithms for sorting corrplots](http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html)


## Validation
### Validation and overfitting
- It is not a rare case in competitions that scores jump down on the leaderboard after private results are revealed. There are two main reasons for this:
    - Competitors could ignore the validation and select the submission which scored best against the public leaderboard.
    - Sometimes competitions have no consistent public/private data split. Or, they have too little data in either public or private learderboard.
- While we as participants cant influence the competition's organization, we can certainly make sure that we select our most appropriate submission to be evaluated on the private leaderboard.
- This section provides a systematic way to set up validation in a competition and tackle most common validation problems.

#### Validation
- With validation, we want to check if the model gives expected results on unseen data.
- Motivating example: predicting whether a patient will have a particular disease in the near future.
    - We need to be sure that the model we train will be applicable in the future, and what quality this model will have.
    - Usually we divide the data we do have into two parts: train and validation. We fit the model on the train part and check its quality on the validation part.
    - We should be ready for the possibility that unseen data from the future will differ from the train data.
- In competitions, we have a similar situation. The organizers give training data with all target values, and test data without target values. As before, we should split the data into train and validation parts.
- Furthermore, to ensure the competition spirit, the organizers split the test data into public and private parts. We see the scores for the public test set during the competition. The private test scores are released only after the competition is over. This ensures that we do not cheat using the test set (overfit).
    - For example, after data is already divided into train and val parts, repeated checking against the val set will produce better scores for some models simply by chance. If we continue to select best models, modify them, and again select the best, we will see constant improvements in the val score. But it doesn't mean that the score on the test data. By repeating this over and over, we could overfit the validation set, or in terms of the competition, we could cheat the public leaderboard. Unrealistically good scores on the public learderboard will later jump down on private leaderboard.
- We want our model to capture patterns in the data, but only those patterns that generalize well between both train and test data.

#### Underfitting and overfitting
- To choose the best model, we should avoid both underfitting and overfitting.
- **Uderfitting**: If the model is too simple, it can't capture the underlying relationships, and will get poor results.
- **Overfitting**: The complexity of the model can be increased to improve results. Prediction quality on the training data will go up, but if the model is fits the training data too much, it will start describing noise and patterns that don't generalize to the test data.
- Note: The meaning of overfitting *in general* is slightly different than overfitting in ML competitions.
    - In general, we say that a model is overfitted if the quality is better on train than on test set.
    - In competitions, we say that a model is overfitted only in the case that the quality on the test set is worse than expected.
    - A different approach to the same concept: plot of error (loss) vs model complexity (plot not shown here).
        - Underfitting shows up as high error on both train and val sets.
        - Overfitting shows up as low error on train and high error on val set.
        - Somewhere in the middle is the model with best complexity - it has the lowest val error, and thus is expected to have the lowest error on the unseen test data.
        - Note: in the plot, the training error is always lower than the validation error. This implies overfitting in the general sense, but does not imply overfitting in the context of competitions.


### Validation strategies

- **Note**: The samples between train and validation must not overlap. If they do, we just can't trust our validation. This is sometimes the case with repeated samples in the data. The predictions for these samples will be better, and more optimistic quality estimation overall. This will prevent us from selecting the best parameters for the model.
    - Overfitting is generally bad, but if we have duplicated samples in both train and test simulaneously, and we overfit, the validation score will deceive us into a belief that we are moving in the right direction.
- The following are three often-used validation strategies, and the main difference between them is the number of splits being done.

#### 1. Holdout

- Simple split that divides the data into two parts.
- Fit the model on train set, and evaluate its quality on the val set. Using scores from validation, select the best model. Before making a submission, retrain the model on all data that has labels.
- This method is a good choice for a large amount of data.
- In python:
    - `ngroups = 1`
    - `sklearn.model_selection.ShuffleSplit`

#### 2. K-fold

- Repeated holdout. Split the data into k parts and iterate through them, using every part as a validation set only once. Then average scores over these k folds.
- There is a difference between k-fold and the above holdout strategy repeated k times. While it is possible to average the scores of k different holdout splits, some samples may never be in validation, while others may be placed there multiple times. The core idea of k-fold is that we want to use every sample for validation only once.
- This method is a good choice for a medium amount of data and we can get either a sufficiently big difference in quality, or different optimal parameters between folds.
- Example - estimating the complexity of k-fold compared to holdout. For k=5, and a holdout split of 80% train & 20% test, if prediction takes an insignificant amount of time, k-fold is similar to holdout repeated 5 times. Thus, k-fold will be 5 times slower.
- In python:
    - `ngroups = k`
    - `sklearn.model_selection.Kfold`

#### 3. Leave-one-out

- This is a special case of k-fold when k equals the number of samples in the data.
- Iterate through every sample in the data, each time using k-1 objects as the train data, and the one left-over object as the test data.
- This method can be helpful for when the data is small and the model is fast enough to retrain.
- In python:
    - `ngroups = len(train)`
    - `sklearn.model_selection.LeaveOneOut`

#### Stratification

- We usually use k-fold or holdout on shuffled data. By shuffling data, we are trying to reproduce random train/validation splits. But sometimes, especially if the data is small, or target classes are highly imbalanced, a random split can fail.
- Stratification is a way to ensure that we get a similar target distribution over different folds, thus making validation more stable. It is useful for small datasets, unbalanced data sets (e.g. target average very close to 0 or 1 for binary classification), and multiclass classification (large amount of classes).
    - For datasets not in the above list, stratification will be quite similar to a shuffle (random) split.
    
### Validation strategies - document

This section contains information about main validation strategies (schemes): holdout, K-Fold, LOO.

The main rule you should know: never use data you train on to measure the quality of your model. The trick is to split all your data into training and validation parts.

Below you will find several ways to validate a model.

a) **Holdout scheme**:
    1. Split train data into two parts: partA and partB.
    2. Fit the model on partA, predict for partB.
    3. Use predictions for partB for estimating model quality. Find such hyper-parameters, that quality on partB is maximized.

b) **K-Fold scheme**:
    1. Split train data into K folds.
    2. Iterate though each fold: retrain the model on all folds except current fold, predict for the current fold.
    3. Use the predictions to calculate quality on each fold. Find such hyper-parameters, that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement.

c) **LOO (Leave-One-Out) scheme**:
    1. Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset).
    2. In the end you will get LOO predictions for every sample in the trainset and can calculate loss. 

Notice, that these are *validation* schemes are supposed to be used to estimate quality of the model. When you found the right hyper-parameters and want to get test predictions don't forget to retrain your model using all training data.

### Data splitting strategies
- This section covers more concrete examples of data splitting.

#### Different approaches to validation (time series example)
- Task: predict the number of customers for a shop for each day during the next month.
- Two strategies are possible here: split using random choice, or make a time-based split (every row before a day as train, and every day after as test).
  - If the train/validation split is made differently than the train/test split, the model created will be useless.
- The **main rule of reliable validation**: set up validation to mimic the train/test split.
- Suppose that we have a pool of models trained on different features and we select the best model for each type of validation. Will these models differ, and how significantly?
    - It is certain that a model optimized using random splits (and so favoring points before and next target values) will perform poorly, since *next* observations for are not available for the test data (NaN fed to model, and model probably has not had much experience with NaNs).
    - The model for the second type of validation was trained to predict many points ahead, and will not use adjacent target values.
    - An important conclusion here is that the most useful features for one model are useless for another.
- The generated features are not the only problem here. The actual train/test split is time-based. If we carefully generate features that draw attention to time-based patterns, will we get reliable validation with a random split? Or put another way, if we create features which are useful for a time-based split and are useless for a random split, is it correct to use random split to select the model?
    - Consider the case when the target follows a linear trend (diagonal line plot). In general, the model predictions will be close to target mean value (the mean y value across all samples), calculated using the train data. For random-based val set, if the validation points are closer to the mean value compared to test set, we get a better score on validation than on test. For time-based val set, the val points are nearly as far as the test points for the target mean value, and the validation score will be similar to the test score. This shows that for incorrect validation, not only the features but even the target can lead to unrealistic estimation of the score.
- In summary, different splitting strategies can differ significantly in the generated features, in the way that the model will rely on those features, and in some kind of target leak. To be able to great features, we absolutely must identify the train-test split made by the competition organizers, and reproduce it.
  
#### Splitting data into train and validation
- Most splits fall into 3 categories:
    - Random, row-wise
    - Time-wise
    - By id

##### Random split by rows
- This usually means that rows are fairly independent of each other.
- There may be some dependency between rows. E.g., for a task to predict whether a person will pay off a loan, the data might have members of the same family, or people who work for the same company. If a family member is present in the train, and another member of the same family is present in test, a special feature can be devised just for this case.

##### Time-based split
- Treat everything before a specific data as train, and everything after as test.
- In this approach, it is useful to make special features based on the target. E.g., for predicting the number of customers in the coming week, add features like the number of customers on the same day in the previous week, or the number of customers for the past month.
- Moving window validation is a special case of the time-based split. This entails creating several train/val splits by moving the validation week up by one week forward across the training set, increasing the train data by that one week: start with 3 weeks train and use 4th week for val, then use 4 weeks for train and 5th week for val, etc.

##### Id-based split
- For example, a task to recommend music for completely new users. This means that the train and test sets have completely different sets of users. We can probably make the conclusion that features based on user history (e.g., number of songs listened in last week) will not help for new users.
- Specific competition examples:
    - Caterpillar: data in this competition was split on id, but the id was hidden from competitors comp.
    - Nature concervancy; Intel & MobileODT Cervical Cancer Screening
        - Train and test data did not have overlap on id, but id was hidden.
        - Competitors came up with their own id schemes by clustering pictures.
            - In the case where pictures were taken one after another, the images were quite similar.

##### Combined methods
- Splitting methods can sometimes be combined.
- Example 1: for a task of predicting sales in shops, we can choose a split date for each shop independently instead of using one data for every shop in the data.
- Example 2: for a dataset of search queries from multiple users and search engines, split the data by a combination of user id and search engine id.
- Example 3: Home Depot product search relevance competition
    - Task: estimate search relevancy
    - Data consists of search terms and search results for those terms. Test set contained completely new terms. So, one shouldn't use a random split (favors more complicated models and overfitting) or a search-term-based split (leads to underfitting) for validation.
    - In order to select optimal models, it was crucial to select the ratio of new search terms from train/test split.

### Problems occurring during validation
- Problems in validation can be divided into two groups:
    - Problems during local validation: caused by inconsistency of the data (e.g. getting different parameters for different folds). Sovle by a more thorough validation.
    - Problems during submission: scores on validation and on the leaderboard don't match. This is usually because we can't mimic the exact train/test split on our validation.

#### Validation stage problems
- Generally, the main problem is a significant difference in scores for different train/val splits.
- Example: predicting sales in a shop in February. 
    - Given target values for the whole previous year, and take the last month (January) as validation. 
    - However, January has more holidays than February, causing people to buy more, thus leading to higher target values. MSE of predictions will be higher for Jan than Feb.
    - This does not mean that the model will perform worse for February, at least in terms of overfitting.
    - This example illustrates that this kind of behavior can sometimes be expected. But what if there is no clear reason why scores differ for different folds?
    
##### Causes of different scores and optimal parameters
- Too little data: not enough observations to generalize the high amount of patterns and trends available in the data. In this case, a model will utilize only some general patterns, and for each train/val split, these patterns will partially differ.
- The data is too diverse and inconsistent: for example, two similar samples with different target values can confuse a model.
    - If one such sample is in train, while the other is in val, we can get a pretty high error for the second sample.
    - If both samples are in validation, we will get smaller errors for them.
    - For the Jan/Feb sales example from earlier, we know the natural reason for the difference in scores. Diversity can be reduced by validating on February from the previous year.

##### Extensive validation
- To solve the above two problems, we should do extensive validation:
    - Increase k in k-fold (usually 5 is enough).
    - Make several k-fold splits with different random seeds, and average the results.
    - Use one set of splits to tune parameters, and another set of splits to check model quality.

#### Submission stage problems
- There are two cases of these issues:
    - LB score is consistently higher/lower than validation score
    - LB score is not correlated with validation score at all
##### Causes of submission stage problems
- We may already have quite different scores in k-fold. If we view the LB as another validation fold, then getting a different score on LB is not surpising in this case. We can also calculate the mean and std of the validation scores, and estimate if the LB score is expected.
- Other causes:
    - Too little data on public LB. In this case, trust your validation.
    - Train and test data are from different distribution.
        - Example: predicting heights from user photos. Train data contains only women, and test data contains only men. In this case predictions on train data are going to be close to the average height of women, and the model will have bad results on the test data.
        - Sometimes, this problem can be solved by adjusting the solution during training. But sometimes, this problem can be solved *only* by adjsting the solution to the LB (through leaderboard probing). In this case, the strategy is to find the optimal constant prediction for train, and for test data. Then, shift the predictions by that difference. 
            - For the height example, average height of women can be calculated from the train data.
            - If the metric of competition is MSE, calculate average male height by sending two constant submissions, writing down a simple formula, and finding out that the average value for test is 70 inches.
    - It is more common to find that the distributions are not completely different. For example, the train data can consist not *only* of women, but *mostly* of women, and test consists *mostly* of men.
        - To deal with this situation, remember to mimic the train/test split. Force the validation set to have the same distribution as the test. This is true for getting both scores and parameters correct.
- Example: Click-through rate prediction
    - Train data (history of displayed ads) does not contain ads which were not shown. Test data consists of *every possible* ad. This is an example of different distributions between train and test, since train has a huge bias towards shown ads.
    - To set up correct validation, complete the val set with rows of not-shown ads.
- Example: predict whether a user will listen to a song predicted by the system.
    - Test set contains only recommended songs. Train contains both recommended songs and songs users selected themselves.
    - Adjust validation by filtering out songs selected by users.

#### Leaderboard shuffle
- LB shuffle is when participants' positions on public and private leaderboard drastically differ. Shuffle can be due to randomness, little amount of data, and different public/private distributions.

##### Randomness
- This is the case when all participants have very simlar scores (good or poor).
- Example 1: Competitor scores are all very close, but many competitors overfit to public LB.
- Example 2: Unpredictable financial data.

##### Too little data overall, especially in private LB
- The train set consisted of < 200 rows, and test consisted of < 400 rows. Shuffle was expected.

##### Different public/private distributions
- This is usually the case with time series data.
- As competitors adjust their scores to the public LB, they overfit.
- In this case, it is better to trust the validation.

### Additional material and links
- [Validation in Sklearn](http://scikit-learn.org/stable/modules/cross_validation.html)
- [Advice on validation in a competition](http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/)

## Data leakages

### Basic data leaks
- We define leakage in a very general sense as unexpected information in the data that allows us to make unrealistically good predictions. You may think of it as directly or indirectly adding ground truth into the data.
- Data leaks are completely unusable in the real world. They provide too much signal and make the competition lose its main point and turn it into leak-hunting.
- This section discusses the main types of data leaks that appear during machine learning, competition-specific LB probing, and concrete walkthroughs.

### Leaks in time series
- Check the train, public and private splits. If any one of them is not split on time, this is a data leak. In this case, unrealistic features (e.g., price next week) will be the most important.
- Even if split by time, data still contains information about the future (in the test set)
    - E.g., user history in CTR tasks, fundamental indicators in stock market prediction.
- There are two ways to eliminate data leakages:
    - Make rows from future inaccessable
    - Test set with no features, only ID.

### Unexpected information
#### Metadata
- These types of leaks are much harder to find.
- Often, more than just train and test files are available in a competition (e.g., image or text archives).
- In this case, we can access metadata: file creation data, image resolution, etc. This information may be connected to the target variable.
- E.g., for a cat vs dog image classification competition, what if pictures of cats were always taken before dogs, or with a different camera? A good practice for the organizer would be to erase the metadata, resize the pictures, and change creation date.
#### Information on IDs
- It makes no sense to include IDs in a model, and it is assumed that the IDs are automatically generated.
- In reality, an ID could be a hash of something not intended for disclosure, and may contain traces of information connected to the target variable.
    - In the Caterpillar competition, adding ID as a feature slightly improved the result.

#### Row order
- In the trivial case, data may be shuffled by target variable.
- Sometimes adding row number or relative number suddenly improves the score.
- Example: In the TalkingData competition, rows next to each other usually had the same label.

### Leaderboard probing
- This is a competition technique tightly connected with data leaks.
- There are two types of LB probing. The first is extracting all ground truth from the public LB. This gives a little more training data, and is relatively easy to do: in every submission, change only a small set of rows to unambiguously calculate ground truth for those rows from LB score.
- This section mainly focuses on another type of LB probing. The public/private split is supposed to protect private part of test from information extraction, but is still vulnerable. Sometimes, it is possible to make submissions in such a way that will give out information about private data.
- Categories tightly connected with 'id' are vulnerable to LB probing (e.g. company of user in RedHat competition; Year, Month, Week in WestNile competition).
    - Example: a chunk of data with the same data for every row; rows with same IDs have the same target. Organizers split this chunk into public and private parts, but we still know that this particular chank has the same target for every row. After setting all the predictions close to zero in our submission for that particular chunk of data, we can expect 2 outcomes:
        - If score improves, that means that ground truth in public is 0. Then the ground truth in private is 0 as well, since all rows in the chunk have the same target value.
        - If score is worse, it means that the ground truth in both public and private is 1.
    - This is an annoying, technical data leak. Even if it is released close to the competition deadline, there are not enough submissions to fully exploit it.
    - Also, "consistent category" does not necessarily mean *exactly* the same target. It could be consistent in different ways. For example, the target label, could simply have the same distribution for private and public parts of the data.
        - In Quora question pairs competition (binary classification evaluated by log loss), target variable had different distributions in train and test, but allegedly the same in public and private parts of the data.
- Peculiar examples of data leakage
    - Truly Native: predict whether the content of an HTML file is sponsored or not.
        - There was a leak in the archive dates (sponsored and non-sponsored files were created in different periods of time). But do we really get rid of data leakage after erasing archive dates?
        - No, because text in HTML files may be connected to dates in a lot of ways, from explicit time-stamps to news content. The real problem was not meta-data leak but rather data collection. Even without meta information, ML algorithms will focus on actually useless features - features that only act as proxies for the date.
    - Expedia hotel recommendations: predict the hotel group a user is going to book.
        - One feature was the distance between the user's city to a hotel. This is a huge data leak. Using this feature, it was possible to reverse-engineer true coordinates and simply map ground truth from train to test.
    - Flavours of physics: predict something that has never been observed, using simulated data. Special statistical tests were implemented to punish models that exploit simulation flaws. But the tests could be bypassed to get a perfect score on the LB.
    - Quora question pairs: predict whether pairs of items are duplicates.
        - Like in all pairwise competitions, participants are not asked to evaluate *all* possible pairs - there is always some non-random subsampling. This subsampling is the cause of data leakage.
        - Usually, organisers mostly hard-to-distinguish pairs. This leads to imbalance in item frequency: more frequent items have a higher possibility of being duplicates.
        - We can create a connectivity matrix NxN, where N is the total number of items. If item i and item j appeared in a pair, put 1 in positions ij and ji. Treat the rows of this matrix as vector representations of every item. This means that we can compute similarities between those vectors. This trick works because two items have similar sets of neighbors, they have a high possibility of being duplicates.

### Additional material and links
- [Perfect score script by Oleg Trott](https://www.kaggle.com/olegtrott/the-perfect-score-script) -- used to probe leaderboard
- [Page about data leakages on Kaggle](https://www.kaggle.com/wiki/Leakage)


* * *


# Week 3

## Metrics optimization

### Motivation

#### Metrics
- Metrics are an essential part of any competition, and are used to evaluate submissions.
- There are many ways to measure the quality of an algorithm, and each organizer decides the most appropriate one for their particular problem.
    - E.g., an online shop is trying to measure the effectiveness of their website. Effectiveness needs to be formalized; need to define a metric to measure effectiveness. It can be the number of times a website was visited, or the number of times something was ordered using this website. The company decides which metric is the most important, and then tries to optimize it.

#### Motivation and take-away points
- It is really important to optimize the exact metric given in the competition, and not any other metric, since the chosen metric determines the optimal decision boundary.
- The biggest problem is that some metrics cannot be optimized efficiently; i.e. there is no simple enough way to find, say, the optimal hyperplane. In this case, sometimes we would train the model to optimize something different than the competition metric, but would need to apply various heuristics to improve the competition metric score.
- There is another case where we need to be smart about the metrics: it is when the train and test sets are different. In the lesson on leaks, we discussed leaderboard probing - we can, for example, check if the mean target value on public part of test set is the same as on train. If not, we would need to adapt our predictions to suit the test set better. This is a specific metric optimization technique we apply, because train and test sets are different.
- There can be more severe cases where an improved metric on the val set does not result in improvement on the test set. In this case, stop and think if there's a different way to approach the problem.
    - In particular, time series can be very difficult to forecast, even if the validation was done correctly. This can be because the distribution in the future is very different, or there is not enough training data. 
        - Example from a specific competition (using a trick to boost the score after modeling):
$Loss(\hat{y_i};y_i) = |y_i - \hat{y_i}|$ if trend predicted correctly, $(y_i-\hat{y_i})^2$ if trend predicted incorrectly
            - If the *trend* is guessed correctly, then the absolute difference between prediction and target is considered as the error. If model predicts next value to be higher than previous, but in reality it is lower, then the trend is predicted incorrectly, and the error is set to squared absolute difference.
            - This model cares more about predicting the correct trend than predicting the correct value.
            - In this competition, there were several time series to forecast, the horizon was long, and model predictions were unreliable.
            - It was not possible to optimize the metric mentioned above, and so it was better to set all the predictions to $y_{last} + 10^{-6}$ or $y_{last} - 10^{-6}$ - same value for all the points we are to predict for each time series. The sign depends on the estimation of what is more likely: the values on the horizon being lower than the last known value, or to be higher.

### Regression metrics review I
- Notation:
    - $N$ - number of objects
    - $y \in R^N$ - target values
    - $\hat{y} \in R$ - predictions
    - $\hat{y_i} \in R$ - prediction for i-th object 
    - $y_i \in R$ - target for i-th object

#### MSE: Mean Square Error
$$MSE = \frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2$$
- This is the most common metric for regression problems. Used when there is no specific preference to the problem solution, or no other metric is known.
- MSE measures the average squared error of the predictions.
- Visualization: for a dataset (of 5 points), how will the error change if we fix all predictions but one to be perfect, and vary the value of one prediction?
    - The MSE plot will look like a parabola, with the lowest value being when the prediction equals the target.
- **Baseline model**
    - The simplest baseline model does not use any features, and just predicts a constant value $\alpha$ that minimizes the MSE of the dataset.
    - The value $\alpha$ can be found by setting the derivative of MSE w.r.t. $\alpha$ to 0, and solve for $\alpha$.
    - The best $\alpha$ is the mean value of the target column.
- There are two other metrics related to MSE, discussed below.

##### RMSE: Root mean square error
- $RMSE = \sqrt{MSE}$
- The square root is introduced to make the scale of the errors to be the same as the scale of the target.
- Similarities between MSE and RMSE:
    - Every minimizer of MSE is also a minimizer of RMSE, and vice-versa.
    - More generally, $MSE(a) > MSE(b) \iff RMSE(a) > RMSE(b)$.
    - This works because the square root function is non-decreasing.
    - This means that if the target metric is RMSE, we can still compare models using MSE, since MSE orders models in the same way as RMSE. We can also optimize with MSE instead of RMSE. In fact, MSE is easier to work with, so everybody uses MSE instead.
- Differences between MSE and RMSE:
    - There is a difference for gradient-based models: $\frac{dRMSE}{d\hat{y_i}} = \frac{1}{2\sqrt{MSE}}\frac{dMSE}{d\hat{y_i}}$. The value $\frac{1}{2\sqrt{MSE}}$ does not depend on i. This means that traveling along MSE gradient is similar to traveling along RMSE gradient, but with a different learning rate which depends on MSE itself.
    - So MSE and RMSE are not immediately interchangeable for gradient-based methods. You will probably need to adjust some parameters, like the learning rate.

##### R-squared metric
$$R^2 = 1 - \frac{\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2}{\frac{1}{N}\sum_{i=1}^N(y_i-\bar{y_i})^2}= 1 - \frac{MSE}{\frac{1}{N}\sum_{i=1}^N(y_i-\bar{y_i})^2},$$
$$\bar{y} = \frac{1}{N}\sum_{i=1}^Ny_i$$

- To judge model quality by MSE and RMSE, take into account the properties of the dataset and the target.
- When MSE of predictions is 0, $R^2$ is 1. When the model MSE equals to MSE of the constant model, $R^2$ is zero.
- To optimize $R^2$, we can optimize MSE, since the constants in the $R^2$ equation do not matter for optimization.

#### MAE: Mean Absolute Error
- $MAE = \frac{1}{N}\sum_{i=1}^N|y_i-\hat{y_i}|$
- The error is an average of the absolute difference between the target and the predictions.
- This metric penalizes large errors *less* than MSE does, and so is not as sensitive to outliers as MSE.
- MAE is widely used in finance, where a \$10 error is usually exactly 2 times worse than a \$5 error. On the other hand, MSE thinks that a \$10 error  is 4 times worse than a \$5 error. MSE is easier to justify and explain.
- MAE is *not* the same as MSE from an optimization perspective.
- **Baseline model**
    - The best constant $\alpha$ for MAE is the median of the target values. 
- MAE gradients:
    - The gradient for MAE w.r.t. predictions is a step function: it equals -1 when $\hat{y_i} < y_i$, and 1 when $\hat{y_i} > y_i$.
    - The gradient is *not defined* when the prediction is perfect (MAE is not differentiable), but this happens rarely.
        - Practically, check for this and return 0 when prediction is exactly equal to target, and return the true gradient otherwise.
    - The second derivative is not defined at 0, and is 0 elsewhere.

#### MAE vs MSE
- MAE is more robust than MSE (less sensitive to outliers), but this does not mean that it's always better to use MAE. 
- The question is - are there any *real* outliers in the dataset, or are there just unexpectely high values that should be treated like any other value? Outliers are usually mistakes, measurement errors, etc, but similarly-looking observations can be natural.
- Therefore, if the extreme values are *not* outliers, do not use a metric which will ignore them.
- To summarize: If sure that data has outliers, use MAE. If these are unexpected values that are still important, use MSE.


### Regression Metrics Review II

- Example problem: predicting the number of laptops sold by two shops.
    - Shop 1: predicted 9, sold 10, MSE = MAE = 1
    - Shop 2: predicted 999, sold 1000, MSE = MAE = 1
    - The error for shop 1 is much more critical than in the 2nd case, but MSE and MAE are both 1 for both shops. According to these metrics, the two errors are indistinguishable. This is because MAE and MSE both work with absolute errors. However, relative error can be more important, since an off-by-1 error for shop 1 is equivalent to an off-by-100 error for shop 2.
- On the error plots for MSE and MAE, we see that all the curves have the same shape for every target value (the curves are just shifted versions of each other). This is an indicator that the metric works with absolute errors.

#### From MSE and MAE to MSPE and MAPE
- Relative error preference can be expressed with MSPE (mean square percentage error) and MAPE (mean absolute percentage error):
    - $MSPE = \frac{100\%}{N}\sum_{i=1}^N(\frac{y_i-\hat{y_i}}{y_i})^2$
    - $MAPE = \frac{100\%}{N}\sum_{i=1}^N|\frac{y_i-\hat{y_i}}{y_i}|$
- For each observation, the absolute error is devided by the target value, giving relative error.
- MSPE and MAPE can be thought of as weighted versions of MSE and MAE:
    - For MAPE, the weight of each sample is inversely proportional to its target.
    - For MSPE, the weight of each sample is inversely  proportional to target squared.
    - Note that the weights do not sum up to 1.

#### MSPE baseline
- For MSE, the optimal $\alpha$ is the target mean.
- For MSPE, the optimal $\alpha$ is the weighted target mean.
    - $MSPE = \frac{100\%}{N}\sum_{i=1}^N(\frac{y_i-\alpha}{y_i})^2$

#### MAPE baseline
- For MAE, the optimal $\alpha$ is the target median.
- For MAPE, the optimal $\alpha$ is the weighted target median.
    - $MAPE = \frac{100\%}{N}\sum_{i=1}^N|\frac{y_i-\alpha}{y_i}|$

#### RMSLE: Root Mean Squared Logarithmic Error
- RMSLE is RMSE calculated in log space.
    - A constant is added because log(0) is undefined. The constant can be 1 or something else.
- $RMSLE = \sqrt{\frac{1}{N}\sum_{i=1}^N(log(y_i+1)-log(\hat{y_i}+1))^2} = RMSE(log(y_i+1),log(\hat{y_i}+1)) = \sqrt{MSE(log(y_i+1),log(\hat{y_i}+1))}$
- Just like MAPE and MSPE, this metric cares more about relative errors than absolute ones.
- However, the error curves for RMSLE are asymmetric. For this metric, it is always better to predict more than the same amount less than target.
- The version of this metric without square root can also be used, but the rooted version is more common.

#### RMSLE baseline
- $RMSLE = \sqrt{\frac{1}{N}\sum_{i=1}^N(log(y_i+1)-log(\alpha+1))^2} = RMSE(log(y_i+1),log(\alpha+1)) = \sqrt{MSE(log(y_i+1),log(\alpha+1))}$
- The best constant in log space is a mean target value. We need to exponentiate it to get the answer.

#### Summary
- MSE is biased towards large values, while MAE is much less biased.
- MSPE and MAPE are biased towards smaller targets, as they assign higher weight to objects with smaller targets.
- RMSLE is frequently considered a better metric than MAPE since it is less biased towards small targets, yet works with relative errors.

### Classification Metrics
- Notation:
    - $N$ - number of objects
    - $L$ - number of classes
    - $y$ - ground truth
    - $\hat{y}$ - predictions
    - $[a=b]$ - indicator function
    - *Soft labels (soft predictions)*: classifier's scores
    - *Hard labels (hard predictions)*:
        - $arg max_if_i(x)$
        - $[f(x) > b]$, b - threshold (for binary classification)

#### Accuracy score
- Accuracy = $\frac{1}{N}\sum_{i=1}^N[\hat{y}=y_i]$
- Accuracy is the fraction of correctly classified objects, ranging from 0 to 1.
- Need hard labels to calculate accuracy.
- Baseline (best constant to predict):
    - Predict the most frequent class.
    - This can lead to high accuracy when predicting this constant on a dataset with imbalanced classes.
- While this score is intuitive, it is hard to optimize.
- Accuracy score also doesn't care how confident the classifier is in the predictions, or what the soft predictions are.
- This is why it is usually preferred other metrics that are easier to optimize and that work with soft predictions.

#### Logarithmic loss (logloss)
- Logloss tries to make the classifier output true posterior probabilities for the observation to be of a certain class.
- Logloss is written in different forms for binary and multiclass tasks.
- *Binary*: LogLoss = $-\frac{1}{N}\sum_{i=1}^Ny_ilog(\hat{y_i}) + (1-y_i)log(1-\hat{y_i}), y_i \in R, \hat{y_i} \in R$
    - $\hat{y_i}$ is assumed to be a number in the range [0,1], and is the probability of an object to belong to class 1. So, $(1-\hat{y_i})$ is the probability of the object to belong to class 0.
- *Multiclass*: LogLoss = $-\frac{1}{N}\sum_{i=1}^N\sum_{i=1}^Ly_{il}log(\hat{y_{il}}), y_i \in R^L, \hat{y_i} \in R^L$
    - $\hat{y_i}$ is a vector of size $L$, and its sum is exactly 1. The elements are the probabilities to belong to each of the classes.
- *In practice*: LogLoss = $-\frac{1}{N}\sum_{i=1}^N\sum_{i=1}^Ly_{il}log(min(max(\hat{y_{il}},10^{-15}),1-10^{-15}))$
    - To avoid NaNs in practice, predictions are first clipped to be not from 0 to 1, but from some small positive number to 1 minus some small positive number.
- Compared with absolute error, logloss strongly penalizes completely wrong answers, and prefers to make a lot of small mistakes rather than make few severe mistakes.
- Baseline:
    - LogLoss = $-\frac{1}{N}\sum_{i=1}^Ny_ilog(\alpha) + (1-y_i)log(1-\alpha)$
    - Set $\alpha_i$ to the frequency of i-th class.
    - For a dataset of 10 cats and 90 dogs, $\alpha$ = [0.1, 0.9].

#### Area under curve (AUC ROC) - binary classification only
- This is a good default metric.
- This metric tries all possible thresholds and aggregates their scores.
- AUC ROC does not care about absolute value of the predictions, but depends on the order of the objects.
- There are several ways that AUC can be explained:
    - Area under curve
    - Pairs ordering

##### Area under curve
- The ROC curve is constructed on a 2-d plot with false positives (FP) as the x-axis, and true positives (TP) as the y-axis. Start at (0,0), and move up/right while moving the threshold from 0 to 1.
- The area under the ROC curve is normalized by the total area of the square to give AUC.
- The AUC for a perfect classifier is 1. A threshold does not need to be specified, and there is no dependence on absolute values.
- In practice, AUC is plotted on a graph with a diagonal line plotted as well. The diagonal line is what AUC looks like if predictions are made at random, and can be thought of as a baseline.
    
##### Pairs ordering
- Consider all pairs of objects, such that one object is from class 0, and the other is from class 1.
- AUC is the probability that score for 1 will be higher than score for 0.
- In other words, AUC is the fraction of correctly ordered pairs: AUC = (# correctly ordered pairs)/(total number of pairs) = 1 - (# incorrectly ordered pairs)/(total number of pairs).


- **Baseline model**:
    - AUC doesn't depend on the value of the predictions, so all constants will lead to the same score.
    - This score will be around 0.5.

#### Cohen's kappa
- Recall that if we just predict the label of the most frequent class, we can already get a high accuracy score, and this can be misleading.
- We can introduce a new metric such that for accuracy of 1 it will give 1, and for baseline accuracy it will give 0:
    - my_score = 1 - (1-accuracy)/(1-baseline)
    - Ex: for a dataset of 10 cats and 90 dogs, the baseline is 0.9 (most frequent class). For accuracy 1, my_score = 1. For accuracy 0.9, my_score = 0.
    - Baselines are different for every dataset.
    - This is similar to what R-squared does with MSE (normalizing).
- For Cohen's Kappa, the baseline is calculated is the average of the accuracies for randomly shuffled hard predictions.
    - Ex: for a dataset of 10 cats and 90 dogs, predict 20 cats and 80 dogs at random: accuracy = $0.2*0.1 + 0.8*0.9 = 0.74$.
- Cohen's Kappa = 1 - (1 - accuracy)/(1 - $p_e$), where $p_e$ is the average accuracy for permuted predictions.
- $p_e$ can be computed analytically: $p_e = \frac{1}{N^2}\sum_kn_{k1}n_{k2}$.
    - Multiply the empirical frequencies of predictions and ground truth labels for each class, and then sum them up.
- The score can also be rewritten in terms of errors: Cohen's Kappa = 1 - error / (baseline error)

##### Weighted error and weighted kappa
- The weighted error is calculated using an error weight matrix and confusion matrix.
- An error weight matrix contains the weight for each mistake.
    - Ex: for a dataset of 10 cats, 90 dogs, and 20 tigers, form a 3x3 matrix, and set the weight to be 10 for when predict "cat" or "dog", but ground truth is "tiger".
- A confusion matrix shows how our classifier distributes predictions over the objects.
- Weighted error = $\frac{1}{const}\sum_{i,j}C_{i,j}W_{i,j}$
    - This is a sum of the element-wise matrix multiplication.
    - A constant is used to normalize the result between 0 and 1, but it will cancel out later.
- Then weighted kappa is calculated as: weighted kappa = 1 - (weighted error) / (weighted baseline error).

##### Quadratic and linear weighted kappa
- In many cases, the weight matrices are defined in a very simple way.
    - For a classification problem with ordered labels, weights can be linear: $w_{ij} = |i-j|$, or quadratic: $w_{ij} = (i-j)^2$.
- The quadratic weighted kappa has been used in several competitions on Kaggle. It is usually explained as inter-rater agreement coefficient (how much the model predictions agree with ground truth raters). This is quite intuitive for medicine applications - how much the model agrees with professional doctors.

### General approaches for metrics optimization

#### Loss vs metric
- The **target metric** is a function what *we* want to optimize (evaluate the quality of our model).
- **Optimization loss** is what the *model* optimizes.
    - No one really knows how to optimize metrics efficiently. Instead, we come up with proxy loss functions that are easy to optimize for a given model.
    - For example, logloss is widely used as an optimization loss, while the accuracy score is how the solution is eventually evaluated.
- This can be viewed as expectation vs reality.
- Sometimes the model can optimize the target metric directly. E.g., most libraries can optimize RMSE out-of-the-box.
- Sometimes, we want to optimize metrics that are really hard or impossible to optimize directly. In this case, we set the model to optimize a loss that is different to the metric, but after the model is trained, we use hacks and heuristics to negate the discrepancy and adjust the model to better fit the target metric.
- The words *loss*, *cost*, and *objective* are used as synonyms.

#### Approaches to target metric optimization
- Some metrics can be optimized directly. That is, we should just find a model that optimizes this metric and run it (set the model's loss function to this metric). MSE and Logloss are implemented as loss function in many libraries.
- For some metrics that cannot be optimized directly, we can preprocess the train set and optimize another metric. For example, while MSPE cannot be optimized directly with XGBoost, we can resample the train set and optimize MSE instead. This also applies to MAPE and RMSLE.
- Sometimes we can optimize an incorrect metric, but postprocess the predictions to fit a competition metric better. Examples: Accuracy, Kappa.
- Sometimes it is possible to write a custom loss function. E.g., this is possible for XGBoost.
- Sometimes it is possible to use another metric, and use early stopping.
    - Optimize metric M1, and monitor metric M2 on the validation set. Stop when M2 score is best.

### Regression metrics optimization
- Early stopping can be used for every metric, so will not be described further here.

#### MSE
- MSE is the most commonly used metric for regression tasks. Almost every modeling software implements MSE as a loss function.
- Libraries that support MSE:
    - Tree-based
        - `XGBoost`, `LightGBM`
        - `sklearn.RandomForestRegressor`
    - Linear models
        - `sklearn.<>Regression`
        - `sklearn.SGDRegressor`
        - Vowpal Wabbit (*quantile loss*)
    - Neural Nets
        - PyTorch, Keras, TensorFlow, etc.
- Synonyms: *L2 loss*.
    
#### MAE
- Since the 2nd derivative of MAE is 0, XGBoost cannot optimize it.
- Libraries that support MAE:
    - Tree-based
        - `LightGBM`
        - `sklearn.RandomForestRegressor` (running time high compared to MSE)
        - Some `sklearn` models have huber loss which is very similar to MAE, especially when the errors are large.
    - Linear models
        - Vowpal Wabbit (*quantile loss*)
            - MAE is a special case of quantile loss.
    - Neural Nets
        - PyTorch, Keras, TensorFlow, etc.
            - Can also be implemented by hand.
- Synonyms: *L1*, *Median regression*.
- There are many ways to make MAE smooth. The most famous is Huber loss - this is a mix between MAE and MSE.
    - MSE is computed when the error is small, so we can safely approach 0 error.
    - MAE is computed for large errors, giving robustness.

#### MSPE and MAPE
- Can implement custom loss for XGBoost or a neural net, or can use early stopping.

##### Using sample weights
- Many libraries accept sample weights.
- Sample weights:
    - MSPE: $w_i = \frac{1/y_i^2}{\sum_{i=1}^N1/y_i^2}$
    - MAPE: $w_i = \frac{1/y_i}{\sum_{i=1}^N1/y}$
- To optimize MSPE, set the loss in the library to MSE, and use MSPE sample weights. Similarly for MAPE.
- Not every library supports sample weights.

##### Using resampling
- This approach does not depend on sample weights support.
- Resample the train set: `df.sample(weights = sample_weights)`.
- And use *any* model that optimizes MSE (MAE).
- The size of the new dataset is up to the user (e.g., can sample twice the original number).
    - Usually need to resample many times, fitting a model each time, and averaging the predictions for a more stable score.
- The test set stays as-is.

##### Log scale
- If the errors are small, we can optimize the predictions in log scale.
- This approach was widely used on the Rossman competition on Kaggle.

#### RMSLE
- This is easy to optimize due to the connection with MSE loss.
- First, apply a transform to the target variables: $z_i = log(y_i+1)$.
- Then, fit a model with MSE loss.

### Classification metrics optimization I

#### Log loss
- Log loss for classification is like MSE for regression: it is implemented everywhere.
- $LogLoss = -\frac{1}{N}\sum_{i=1}^Ny_ilog(\hat{y}_i) + (1-y_i)log(1-\hat{y}_i)$
- Implemented in the following libraries:
    - Tree-based: XGboost, LightGBM
    - Linear models
        - `sklearn.LogisticRegression`
        - `sklearn.SGDClassifier`
        - Vowpal Wabbit
    - Neural nets by default optimize log loss for classification.
- Synonyms: *Logistic loss*.
- Random Forest predictions turn out to be quite bad in terms of log loss. However, we can calibrate the predictions to better fit log loss.

##### Probability calibration
- We've mentioned several times that log loss requires a model to output posterior probabilities, but what does this mean?
- This means that, e.g. if we take all objects with score ~ 0.8, then there will be exactly 4 times more positive objects (class 1) than negative objects (class 0).
- If a classifier doesn't directly optimize log loss, its predictions should be calibrated.
- Plot (not shown) has predictions (sorted by value) for the val set. The classifier's uncalibrated predictions are higher than target on the lower end, and lower than target on higher end. The calibrated predictions are much closer to the target on both ends.
- 3 types of probability calibration are mentioned here:
    - Platt scaling: fit Logistic Regression to the predictions (like in stacking)
    - Isotonic regression: fit Isotonic Regression to the predictions
    - Stacking: fit XGBoost or neural net to predictions
- The basic idea is this: We can fit any classifier - it does not need to optimize log loss, it just needs to be good in terms of, for example, AUC. Then, we can fit another model (model 2) on top that will take the first model's predictions and calibrate them properly. Model 2 uses log loss as its optimization loss.

#### Accuracy
- Accuracy = $\frac{1}{N}\sum_{i=1}^N[\hat{y}_i = y_i]$
- There is no easy recipe for direct optimization of accuracy.
- In general, the recipe is the following:
    - For binary classification: fit any metric and tune binarization threshold.
    - For multiclass: fit any metric and tune parameters comparing models by their accuracy score, not by the metric that the models were actually optimizing (early stopping and cross validation by accuracy).
- Plot (not shown) to get an intuition of why accuracy is hard to optimize: loss vs M, where M is signed distance to decision boundary (e.g., distance to hyperplane for linear model). Distance is considered to be positive if the class is predicted correctly, and negative if the object is located at the wrong side of the decision boundary.
    - Zero-one loss (step function with 1 for 0<M and 0 for M>0) corresponds to accuracy score. This loss has gradient 0 w.r.t. predictions. Most learning algorithms require a non-zero gradient to fit. Otherwise, it is not clear how to change predictions to decrease the loss.
    - Proxy losses have been invented that are upper bounds for the zero-one loss. If proxy loss is perfectly fit, then the accuracy will be perfect too. Such proxies are differentiable.
    - Logistic loss is used in logistic regression.
    - Hinge loss is used in SVM.
- Recall that to obtain hard labels for a test object, we take argmax of soft predictions, picking the class with max score. If the task is binary and predictions sum up to 1, then argmax is equivalent to a threshold function: output 1 if pred > 0.5, else output 0.
    - The threshold can be tuned with a grid search implemented with a for-loop.
    - This means we can fit any sufficiently powerful model, and it does not matter exactly which loss (hinge, logistic, etc) the model optimizes. All we want from the model's predictions is the existence of a good threshold that separates the classes.
    - If the classifier is ideally calibrated, then it is really returning posterior probabilities,
    and for such a classifier, the threshold of 0.5 is optimal. But such classifiers are rare, so threshold tuning is useful.


### Classification metrics optimization II

#### AUC
- Although the loss function of AUC has zero gradients almost everywhere (exactly as accuracy loss), there exists an algorithm to optimize AUC with gradient-based methods. Some models implement this algorithm, so we can use the algorithm by setting the right parameters.
- There is more than one way to implement the algorithm; one way is discussed here.

##### Pointwise loss
- Recall that originally, a classification task is usually solved at the object level. We want to assign 0 to red objects (true class 0), and 1 to green objects (true class 1). We do this independently for each object, and so our loss is pointwise. We compute it for each object individually, and sum or average the losses to get a total loss.
    - $min\sum_i^Nl_{point}(\hat{y}_i;y_i)$

##### Pairwise loss
- Recall that AUC is the probability of the pair of objects to be ordered in the right way. So ideally, we want predictions for the green objects to be greater than predictions for the red ones. So instead of using single objects, we are working with pairs of objects, and instead of pointwise loss, we are using pairwise loss.
    - Pairwise loss takes predictions and labels for a pair of objects and computes their loss. Ideally, the loss is 0 when ordering is correct, and greater than 0 when the ordering is incorrect. In practice, different loss functions can be used. $$min\sum_i^N\sum_i^Nl_{pair}(\hat{y}_i,\hat{y}_j;y_i,y_j)$$
- For example, we can use log loss: $$Loss = \frac{1}{N_0N_2}\sum_{j:y_j=1}^{N_1}\sum_{i:y_i=0}^{N_0}{log(prob(\hat{y}_j-\hat{y}_i))}$$
    - The target for this pairwise loss is 1. So there is one term in the logloss objective instead of two.
    - The $prob$ function is used to keep the difference between predictions for 0 to 1 range.
- Availability in packages
    - Tree-based: XGBoost, LightGBM
    - Linear models: ??
    - Neural nets: not out of the box, but is straightforward to implement.
- Most people still use logloss as an optimization loss without any more postprocessing. For example, an XGBoost model trained with logloss can give an AUC score comparable to model trained with pairwise loss.

#### Quadratic weighted Kappa
- There are two methods to optimize quadratic weighted kappa. One is very common and easy. The second is not that common, and requires implementing a custom loss function for XGBoost or neural nets.

##### Method 1
- Recall that we are solving an ordered classification problem, and our labels can be thought of as integer ratings, say from 1 to 5. 
- The task is classification, as we cannot output, e.g., 4.5 as an answer. 
- We can treat this task as a regression and then convert the predictions into integer ratings in post-processing. 
- Quadratic weights make Kappa similar to regression with MSE loss if we allow our predictions to take values between the labels (i.e., relax the predictions). But in fact, it is different to MSE. 
- If relaxed, Kappa would be the following: $$Kappa(y,\hat{y}) \approx 1 - \frac{MSE}{hard to deal with part}$$ 
- The hard-to-deal-with part is dependent on the predictions. Some users have the logic of since there's MSE in the nominator, we can optimize that and not care about the denominator. This is not the correct way to do it, but turns out to be useful in practice.
- MSE gives float values, which must be converted to integers. The straightforward way is to round the predictions. But we can think of rounding as applying thresholds: e.g. if the value is greater than 3.5 and less than 4.5 then output 3. The thresholds can be tuned with grid search.
- To summarize:
    - Fit MSE loss.
    - Find proper thresholds.

##### Method 2
- **On the Direct Maximization of Quadratic Weighted Kappa** describes a way to relax classification problems to regression but also deal with the denominator.

### Additional material and links

#### Classification
- [Evaluation Metrics for Classification Problems: Quick Examples + References](http://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references)
- [Decision Trees: “Gini” vs. “Entropy” criteria](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria)
- [Understanding ROC curves](http://www.navan.name/roc/)
    
#### Ranking
- [Learning to Rank using Gradient Descent](http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf) - original paper about pairwise method for AUC optimization
- [Overview of further developments of RankNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)
- [RankLib](https://sourceforge.net/p/lemur/wiki/RankLib/) (implemtations for the 2 papers from above)
- [Learning to Rank Overview](https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview)

#### Clustering
- [Evaluation metrics for clustering](http://nlp.uned.es/docs/amigo2007a.pdf)


## Mean encodings

### Concept of mean encoding
- This technique has a number of names, including likelihood encoding and target encoding.
- The general idea of this technique is to add new variables based on some feature together with the target.

#### Using target to generate features
- Example dataset with a feature that contains city names as strings, and the task is binary classification. 
- The most obvious way is label encoding: each string gets converted into a numeric value starting at 0.
- With mean encoding, we replace each feature string with the mean of the target for that city.
    - For example, there are five rows with "Moscow" value, and the target has 2 positive examples and 3 negative. So the string "Moscow" is replaced with 2/5 = 0.4.
- This is the basic idea, but there are many pitfalls one should overcome in an actual competition.

#### Why does mean encoding work?
- Imagine that the dataset is much bigger, and the feature contains hundreds of different cities.
- Plots (not shown) of feature histograms for class 0 and class 1:
    - For label encoding, we will always get a totally random picture, because there is no logical order.
    - For mean encoding, the classes look way more separable, and the plot looks kind of sorted. 
- It turns out that this sorting quality of mean encoding is quite helpful. 
- Remember that gradient boosting trees are the most popular and effective way to solve ML problems. One of the few downsides of GBT is an inability to handle high cardinality categorical variables. 
- With mean encoding, we can compensate for the fact that trees have limited depth. We can reach better loss with sorted trees. Plot (not shown) of loss vs tree depth, showing that label encoding generally has lower loss.
- In general, the more non-linear and complicated feature-target dependency, the more effective mean encoding is. 

### Indicators of usefulness of mean encoding
- The presence of categorical variables with a lot of levels is already a good indicator, but we need to go deeper.
- Example: XGBoost learning logs (roc auc score vs iteration number) from Spring Leaf competition. 
    - By increasing the tree depth (from 7 to 9 to 11), the score becomes better and better (nearly perfect). This is normal.
    - But we don't actually overfit, since the validation score also increases, and that's weird. 
    - This is a sign that the trees need a huge number of splits to extract information from some variable; we can check this from the model dump. It turns out that some features have a tremendous amount of split points, like 1200 to 1600. The model tries to treat all those categories differently, and they are also very important for predicting the target.
    - We can help the model by mean encoding.

### Ways to use target variable
- There are several ways to use the target variable. Consider that goods is the number of ones in a group, and bads is number of zeros.
    - Likelihood = goods / (goods + bads) = mean(target)
    - Weight of Evidence = ln(goods/bads)*100
    - Count = goods = sum(target)
    - Diff = goods - bads
- Springleaf example
    - The following snippet shows how to construct a mean encoding for an arbitrary column, and map it into new data frames (`train_new` and `val_new`):
    ```
    means = X_tr.groupby(col).target.mean()
    train_new[col+'_mean_target'] = train_new[col].map(means)
    val_new[col+'_mean_target'] = val_new[col].map(means)
    ```
    - Fit xgboost on this new data:
    ```
    dtrain = xgb.DMatrix(train_new, label=y_tr)
    dvalid = xgb.DMatrix(val_new, label=y_val)
    
    evallist = [(dtrain, 'train'), (dvalid, 'eval')]
    evals_result3 = {}
    model = xgb.train(xgb_par, dtrain, 3000, evals=evallist, verbose_eval=30,
            evals_result=evals_result3, early_stopping_rounds=50)
    ```
    - Something is definitely not right with the result of the above code. After several epochs, train AUC is nearly 1, while validation score saturates around 0.55, which is practically noise. This is a clear sign of terrible overfitting.
        - In this case we validated correctly, and used train data to only estimate mean encodings. If we would have estimated mean encoding before the train/val split, we would not notice such an overfitting.
        - With rare categories, it's pretty common to get results like in this example: target=0 in train, and target=1 in validation. Mean encoding turns into a perfect feature for such categories. That's why we immediately get very good scores on train, but fail hard on validation.
    - We obviously cannot use mean encodings like this in practice. We need to deal with overfitting first.


### Regularization
- This section covers 4 different methods of regularization.

#### CV loop inside training data
- This is an intuitive and robust method.
- For a given data point, we don't want to use the associated target.
- So we separate the data into k non-intersecting subsets (folds). To get mean encodings for a subset, use the other subsets.
- Iteratively walk through all the data subsets. Usually 4-5 folds gets decent results.
- There is still leakage from the target variable. This becomes apparent if we use LOO to separate the data.
- Example of leakage when encoding with LOO scheme.
      - Dataset has 6 rows, where the feature column has "Moscow" string. The target has 3 "0" and 2 "1" labels.
      - Example encoding: for the first row (target="0") the encoding is 0.5, because the rest of the data has 2 "1" and 2 "0".
      - The encoded feature now perfectly splits the data: rows with the feature >= 0.5 have target 0, and 1 otherwise.
      - The target variable is not explicitly used, but this encoding is biased.
      - This effect remains valid even for the k-fold scheme, just milder.
- In practice, with enough data and using 4-5 folds, mean encodings will work fine with this regularization strategy.
- Application in practice: 
  ```
  y_tr = df_tr['target'].values #target variable
  skf = StratifiedKFold(y_tr, 5, shuffle = True, random_state = 123)
  
  for tr_ind, val_ind in skf:
      X_tr, X_val = df_tr.iloc[tr_ind], df_tr.iloc[val_ind]
      for col in cols: #iterate through the columns we want to encode
          means = X_val[col].map(X_tr.groupby(col).target.mean())
          X_val[col+'_mean_target'] = means
      train_new.iloc[val_ind] = X_val
  
  prior = df_tr['target'].mean() #global mean
  train_new.fillna(prior, inplace = True) # fill NANs with global mean
  ```
    - `df_tr`: training data frame
    - `train_new`: data frame with encoded features
    - In outer `for` loop, iterate through the stratified folds to separate data into chunks.
    - `X_tr` is used to estimate the endcoding, and `X_val` is used to apply it.
    - In inner `for` loop, iterate through columns and map estimated encodings to to `X-val` data frame.
    - After the outer loop, fill new data frame with the results.
    - Some rare categories are present only in a single fold, so we don't have the data to estimate target mean. Fill these NANs with the global mean.

#### Smoothing
- Smoothing is based on the following idea: if a category is big (has a lot of data points), then we can trust the estimated encoding. But if the category is rare, it's the opposite.
- Regularization: (mean(target) \* nrows + globalmean \* alpha)/(nrows+alpha)
    - alpha is a hyperparam that controls the amount of regularization: no regularization when alpha=0, and globalmean dominates as alpha approaches infinity.
- In some sense, alpha is equal to the category size we can trust.
- It's also possible to use any other formula - anything that punishes encodings of rare categories can be considered smoothing.
- Smoothing only works when combined together with some other regularization method.

#### Adding noise
- Without regularization, mean encodings have better quality for train than test data.
- By adding noise, we simply degrade the quality of encoding on training data.
- This method is pretty unstable and it's hard to make it work. The main problem is the amount of noise we need to add. Too much noise turns the feature into garbage, while not enough noise means less regularization.
- This method is usually used together with LOO, and needs to be diligently fine-tuned. Thus it is not the best option if time is short.

#### Expanding mean
- Fix some sorting order of the data, and use only rows from 0 to N-1 to calculate encoding for row N.
- Implementation in `pandas`:
    ```
    cumsum = df_tr.groupby(col)['target'].cumsum() - df_tr['target']
    cumcnt = df_tr.groupby(col).cumcount()
    train_new[col+'_mean_target'] = cumsum/cumcnt
    ```
- This method introduces the least amount of leakage from the target variable, and requires no hyperparameter tuning.
- The only downside is that feature quality is not uniform, but this is not a big deal. We can average models fitted on encodings calculated from different data permutations.
- This method is used in the CatBoost library.

#### Summary
- CV loop or expanding mean are recommended for practical tasks, as they are the most robust and easy to tune.

### Extensions and generalizations
#### Regression
- Regression tasks are more flexible for feature encoding. 
- Unlike binary classification, where mean is the only meaningful statistic we can extract from the target variable, regression allows for many different statistics: median, percentiles, standard deviation, etc.
- We can even calculate distribution bins. For example, if target variable is distributed between 1 and 100, we can create 10 bin features. In feature 1, we count how many data points have target between 1 and 10, in feature 2 how many between 10 and 20, and so on.
- All these features need to be regularized.

#### Multiclass
- For every feature we want to encode, we will have N different encodings, where N is the number of classes.
- There is a non-obvious advantage. For example, tree models usually solve classification tasks in one-vs-all fashion, so every class has a different model. When we fit that model, it doesn't have information about structure of other classes, because they are merged into one entity. With mean encodings, we introduce additional information about the structure of other classes.

#### Many-to-many relations
- Domains with many-to-many relations are usually very complex, and require special approaches to create mean encodings. This section gives only a high level idea.
- Example task: binary classification of users based on the apps installed on user smart phones.
    - Each user may have multiple apps, and each app is used by multiple users.
    - To encode apps, first take a cross product of users and app entities. This results in a long reprentation with one row for each user-app pair (each row of the data frame contains one user id, one app id, and the target).
    - Using the long table, we can naturally calculate mean encodings for apps.
    - Since each user might have multiple apps, the short representation will have a vector of means instead of a list of apps. Represent this vector by a statistic: min, max, mean, std, etc.

#### Time series
- With time series, we obviously can't use future information.
- However, we can make some complicated features. 
    - In data with independent rows, we are forced to use all rows to calculate the statistic. It makes no sense to use a subset of rows.
    - This changes with time series. For a given category, we can calculate statistics from a previous day, previous week, etc.
- Example: predict on which categories a user spends money.
    - Good features would be: total amount of money spent in previous day by each user, average amount of money spent by all users in given category.

#### Interactions and numerical features
- In practice, it is beneficial to mean-encode numeric features and some combinations of features.
- To encode a numeric feature, we only need to bin it, and then treat it as categorical.
- There are two considerations: how to bin a numeric feature, and how to select useful combinations of features.
- We can find this out from model structure by analyzing the trees. For example, first fit an XGBoost model on the raw features, without any encodings. If a numeric feature has a lot of split points, it means that there is some complicated dependency with target, and it's worth trying to mean-encode it. Furthermore, these exact split points may be used to bin the feature.

- The process of extracting interactions is similar for 2-way, 3-way, etc. interactions.
- Two features interact in a tree if they are in two neighboring nodes. We can iterate through all the trees in the model and calculate how many times each interaction appeared. The most frequent interactions are probably worth to mean-encode.
    - For example, if interaction between feature1 and feature2 is found to be most frequent, we can concatenate those feature values in the data and mean-encode the resulting interaction.
- Example: Amazon employee access challenge competition.
    - The dataset contains only 9 categorical features.
    - If we blindly fit a LightGBM model on raw features, then the score will be in 0.87 AUC range no matter how well the parameters are tuned (this is around position 700 on LB).
    - Even if we mean-encode all the variables, we will not have any progress.
    - Using CatBoost model, which internally encodes feature interactions, the score will immediately be in 0.91 range (20th position on LB).
    - Note that CatBoost is not a silver bullet. We would still need to manually add more mean-encoded interactions.
- In general, if the data has categorical variables, it is always worth to work with interactions and mean encodings.

#### Correct validation reminder
- During all local experiments, first split the data in X_tr and X_val parts.
    - Estimate encodings on X_tr.
    - Map them to X_tr and X_val.
    - Regularize them on X_tr.
    - Validate model on X_tr/X_val split.
    - Don't even think about estimating encodings before splitting the data.
- At submission stage:
    - Estimate encodings on whole train data.
    - Map them to train and test.
    - Regularize on train.
    - Fit on train.

#### Summary
- Main advantages:
    - Compact transformation of categorical variables.
    - Powerful basis for feature engineering.
- Disadvantages:
    - Need careful validation, there are many ways to overfit.
    - Significant improvements only on specific datasets. Will not help in every competition.

# Week 4

## Hyperparameter tuning

### Hyperparameter tuning I
- 1. Select the most influential parameters
    - There are many parameters and we can't tune all of them. Understand which params affect the model the most.
- 2. Understand how exactly they influence the training.
- 3. Tune them.
    - a. Manually (change and examine)
    - b. Automatically (hyperopt, etc.)

#### Hyperparameter optimization software
- The following is a list of software for automatic hyperparameter tuning.
    - *Hyperopt*
    - Scikit-optimize
    - Spearmint
    - GPyOpt
    - RoBO
    - SMAC3
- Example usage:
    ```
    def xgb_score(param):
        # run XGBoost with parameters `param`
    
    def xgb_hyperopt():
        space = {
            'eta' : 0.01,
            ...
            ...
            ...
        }
    best = fmin(xgb_score, space, algo=tpe.suggest, max_evals=1000)
    ```
    - Define a function that will run our model with given set of params and return the validation score.
    - Specify a search space.
- The best strategy is to run overnight.
- Try to pick parameter ranges that make sense.

#### Parameter influence on model
- Different values of params result in three different fitting behaviors:
    - Underfitting: model is so constrained that it cannot even learn the train set.
    - Good fit and generalization - this is the desired behavior.
    - Overfitting: the model is so powerful that it overfits the train set and is not able to generalize at all.
- When tuning params, try to understand if the model is currently under or over fitting. Adjust params to get closer to desired behavior.
- Split all params to be tuned into two groups (color-coded):
    - 1. <span style="color:green">Params that constrain the model</span>. If we increase these params, the model changes its behavior from overfitting to underfitting. Larger param values mean heavier constraint. Decrease these params to allow model to fit easier.
    - 2. <span style="color:red">Params that lead to overfit</span>. Increasing these params lead to a better fit (overfit) on train set. Increase these params if model is underfitting; decrease if overfitting.





### Hyperparameter tuning II
#### Tree-based models
- XGBoost and LightGBM are the gold standard. These are awesome implementations of a very versatile gradient-boosted decision tree model.
- CatBoost appeared at the time that this course was being recorded, but looks promising.
- List of models and software:
    - GBDT
        - XGBoost (dmlc/xgboost)
        - LightGBM (Microsoft/LightGBM)
        - CatBoost (catboost/catboost)
    - RandomForest, ExtraTrees
        - scikit-learn
    - Others
        - Regularized Greedy Forest (baidu/fast_rgf): implementation is very slow and hard to use.
    
#### GBDT
- XGBoost and LightGBM both build decision trees one after another, gradually optimizing a given objective.
- There are several params that control the tree building process (listed in order of XGBoost, LightGBM).
- <span style="color:green">max_depth | max_depth/num_leaves</span>
    - Maximum depth of a tree. Deeper trees fit a dataset better. Increasing this param leads to faster fitting to the train set. 
    - Depending on the task, the optimal depth can vary a lot. If increasing the depth reduces validation error but does not lead to overfitting, this may be a sign there are important interactions to extract from the data. In this case, it's better to stop tuning and generate some features.
    - Recommend to start with `max_depth` = 7. As depth is incresed, learning will take a longer time.
    - For LightGBM, it is possible to control the number of leaves rather than max depth. This is nice because the resulting tree can be very deep but have a small number of leaves and not overfit.
- <span style="color:green">subsample | bagging_fraction</span>
    - Controls fraction of objects to use when fitting a tree (between 0 and 1).
    - In practice, using all objects at every iteration can lead to overfitting.
    - Using less objects will make model fit slower, but generalize better. So this parameter works kind of like regularization.
- <span style="color:green">colsample_bytree/colsample_bylevel | feature_fraction</span>
    - A tree may consider only a fraction of the features at every split. If the model is overfitting, try lowering these parameters.
- <span style="color:red">min_child_weight, lambda, alpha | min_data_in_leaf, lambda_l1, lambda_l2</span>
    - These are regularization parameters.
    - The most important is `min_child_weight`. Increasing this leads to a more conservative model. Setting it to 0 (minimum) leads to a less-constrained model. Depending on the task, this value can be 0, 5, 15, 300, etc. Try a wide range of values.
- <span style="color:green">eta, num_round | learning_rate, num_iterations</span>
    - `eta` is a learning rate.
    - `num_round` number of steps to perform (number of trees to build).
    - At each iteration, a new tree is built and added to the model, with learning rate `eta`.
    - In general, higher learning rate leads to faster fit to train set. More steps leat to better fit.
    - Caveats: if `eta` is too high, the model will not converge. If `eta` is too small, the model will learn nothing after a large number of rounds, but a smaller rate leads to better generalization.
    - We can freeze `eta` to be reasonably small (0.1 or 0.01), and then find the number of rounds till the model overfits. Usually we use early stopping: monitor the val loss and exit training when loss starts to go up. When the right number of rounds is found, do the following trick to make the resulting model better:
        - num_round = num_round * alpha
        - eta = eta / alpha
- seed | *_seed
    - This is the random seed argument.
    - It is recommended to fix seed before hand. In XGBoost, however, every change in param will lead to a different model.
    - This param can also be used to verify that different random seeds to not change the training results much.

#### RandomForest / ExtraTrees
- ExtraTrees is a more randomized version of RF, and has the same parameters.
- RF builds each tree to be independent of others. So, having many trees does not lead to overfitting for RF, unlike for GBDT.
- n_estimators (the higher the better)
    - Controls the number of trees.
    - At the start, it is a good idea to determine the number of trees that is sufficient.
        - Plot how validation error changes as n_estimators is increased.
        - Before submitting to LB, set n_estimators higher just to be sure.
- <span style="color:green">max_depth</span>
    - Controls depth of the trees. 
    - Can be set to `none` (unlimited depth). This can be useful when the features have repeated values and important interactions. A model with unlimited tree depth will also overfit quickly.
    - Recommend to start with depth of 7.
    - Usually, an optimal depth for RF is higher than for GBDT.
- <span style="color:green">max_features</span>
    - Similar to col_sample from XGBoost.
- <span style="color:red">min_samples_leaf</span>
    - Similar to min_child_weight from XGBoost.
- *criterion*
    - Can evaluate a split by this criterion. Can be either Gini or Entropy. Choose the best performing one.
- random_state: random seed param
- n_jobs: number of cores.

### Hyperparameter tuning III

#### Neural networks
- Which framework to use?
    - Keras is the most popular on Kaggle, and has the simplest interface.
    - TensorFlow - extensively used by companies in production
    - PyTorch - used in deep learning research community

##### Tuning hyparameters
- <span style="color:green">Number of neurons per layer</span>. Increasing the number of neurons per layer lets the NN learn more complex decision boundaries, and thus overfit faster.
- <span style="color:green">Number of hidden layers</span>. Increasing number of layers should result in the same, but due to optimization problems, the learning can even stop to converge.
    - Recommendation: start with 2-3 layers, 64 units per layer. Make sure the training and validation losses decrease. Then try to find the config that is able to overfit the training set (just as a sanity check). Then start tuning the network.
- Optimization method. Broadly speaking, we can pick either vanilla stochastic gradient descent with momentum or a modern adaptive method.
    - <span style="color:red">SGD + momentum</span>- converges slower, but the trained network generalizes better.
    - <span style="color:green">Adam/Adadelta/Adagrad/...</span> - these can fit faster, but also lead to overfitting in practice. These methods are useful, but in settings other than classification and regression.
- <span style="color:green">Batch size</span>. Large batch size leads (e.g., 500) leads to overfitting.
    - Recommendation: pick 32 or 64. If the network is still overfitting, decrease batch size. If underfitting, increase batch size.
    - Note that if the number of epochs is fixed, then NN with batch size reduced by factor of 2 gets updated twice as much. Maybe reduce the number of epochs.
    - The batch size should not be too small, or the gradient will be too noisy.
- Learning rate. When LR is too high, network will not converge. When LR is too low, the network will learn forever.
    - Recommendation: start with large LR (0.1), and lower it to find when network converges.
    - Rule of thumb connection between learning rate and batch size: if batch size is increased by factor $\alpha$, can increase LR by the same factor. But remember that larger batch size leads to more overfitting, so need good regularization.
- Regularization:
    - <span style="color:red">L2/L1 for weights</span>: used some time ago.
    - <span style="color:red">Dropout/Dropconnect</span>: popular currently. Can vary the dropout probability and place where this layer is inserted. Usually added closer toward end of the network. If added right after input, some information is lost right away, and perfomance degradation is observed.
    - <span style="color:red">Static dropconnect</span>:
        - Between input and a hidden layer (of 128 units), insert another hidden layer of 4096 units. To regularize, randomly drop 99% of connections between input and this new hidden layer. In dropconnect, we drop random connections at every learning iteration, while in static dropconnect, we fix connectivity pattern for the whole learning process.

#### Linear models
- Frameworks
    - Scikit-learn
        - SVC/SVR - wrappers for libLinear and libSVM
            - Even though a carefully tuned LightGBM would probably beat SVM even on a large sparse set, SVMs require almost no tuning.
            - Compile these yourself for multicore support.
            - This section only discusses linear SVM (not kernel SVM).
        - LogisticRegression/LinearRegression + regularizers
        - SGDClassifier/SGDRegressor
    - Vowpal Wabbit
        - Use for datasets that do not fit in memory. Data is read line-by-line from disk, and the entire dataset is not loaded into memory at once (online learning).
        - FTRL (Follow The Regularized Leader) - a method of online learning for linear models. Popular some time ago. (There are also other implementations in pure python.)

##### Tuning hyperparameters
- <span style="color:red">Regularization parameter (C, alpha, lambda, ...)</span>:
    - Recommendation: start with very small value and increase it.
    - Note: The parameter C in SVM is inversely proportional to regularization weight, so the dynamics are opposite. SVC starts to work slower as C increases.
        - For SVM, start with C = 10^-6, then increase by factor of 10.
    - Try both L1 and L2 regularization. L1 gives weight sparsity, which can be used for feature selection.

#### General advice
- Do not spend too much time tuning hyperparameters (especially when the competition has only started).
    - You cannot win a competition by tuning parameters. Appropriate features, hacks, leaks and insights will give much more than a model carefully tuned on all features.
    - Only do if you don't have any more ideas or you have spare computational resources.
- Be patient.
    - It can take thousands of rounds of GBDT or neural nets to fit.
- Average everything.
    - Over random seed: when submitting, learn 5 models starting from different seeds, and average the predictions.
    - Over small deviations from optimal parameters.
        - e.g. if optimal *max_depth* is 5, make models with 4,5, and 6.
        
### Additional material and links
- [Tuning the hyper-parameters of an estimator (sklearn)](http://scikit-learn.org/stable/modules/grid_search.html)
- [Optimizing hyperparameters with hyperopt](http://fastml.com/optimizing-hyperparams-with-hyperopt/)
- [Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)
        
## Tips and tricks

### Practical guide

#### Before you enter a competition
- Define your goals. What can you get out of the competition?
    - 1. To learn more about an interesting problem.
    - 2. To get acquainted with new software tools.
    - 3. To hunt for a medal.
        - Check how many submissions top participants have. If top 20 people have over 100 submissions, this can be a sign of leaderboard probing or difficulties in validation, including inconsistency of val and LB scores. On the other hand, if there are people with few submissions on the top, that usually means that there is a non-trivial approach to this competition, or leaks discovered only by a few people.
        - Pay attention to the size of the top teams.

#### After you enter a competition: Working with ideas
- Organize ideas in some structure
- Select the most important and promising ideas
- Try to understand the reasons why something does/doesn't work.


## Advanced features II

### Statistics and distance based features

### Matrix factorizations

### Feature interactions

### t-SNE



* * * 

# Course summary - most important points
- Set up the train/validation data split to mimic the train/test split of the competition.
- Metrics optimization: the target metric is how submissions are scored. Optimization loss is what the model optimizes, and it is not always the same as the target metric that we want to optimize. Predictions can be post-processed to make them better fit the target metric.