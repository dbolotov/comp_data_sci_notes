---
title: "Competitive Data Science Notes"
output:
  html_document:
    theme: cerulean
    highlight: tango
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require("knitr")
```

# Introduction

This is a set of notes for the Competitive Data Science course on Coursera.

***

# Week 1

## Recap of main machine learning algorithms

### Linear models
- Examples: Logistic regression, SVM.
- Separate objects with a plane.
- Good for sparse, high-dimensional data.
- Often, points cannot be separated by such a simple approach.

### Tree-based: Decision Tree, Random Forest, Gradient Boosted Decision Trees
- Recursively split data by lines parallel to an axis. This significantly reduces the number of possible lines.
- Another way to look at it: a decision tree separates the data into boxes, and approximates the data in the boxes by constants.
- Very powerful general method for tabular data
- Hard to capture linear dependencies, since this requires a lot of splits. For example, 2 classes that can be separated by a diagonal line will require many vertical/horizontal splits.

### kNN-based methods
- Select class labels by majority vote of closest neighbors.
- Features based on kNN are very informative.
- `scikit-learn` has implementation with algorithmic tricks to speed up calculation.

### Neural Networks
- Produce smooth non-linear decision boundary.

### No Free Lunch Theorem
- "There is no method which outperforms all others for all tasks" or "For every method we can construct a task for which this particular method will not be the best".
- Every method relies on some assumptions for it to work. If the assumptions fail, the method will perform poorly.


### Links for solving the quiz
- [Explanation of Random Forest](http://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)
- [Explanation/Demonstration of Gradient Boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)
- [Example of kNN](https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/)


### Additional materials and links
- Overview of methods
    - [Scikit-Learn (or sklearn) library](http://scikit-learn.org/)
    - [Overview of k-NN (sklearn's documentation)](http://scikit-learn.org/stable/modules/neighbors.html)
    - [Overview of Linear Models (sklearn's documentation)](http://scikit-learn.org/stable/modules/linear_model.html)
    - [Overview of Decision Trees (sklearn's documentation)](http://scikit-learn.org/stable/modules/tree.html)
    - [Overview of algorithms and parameters in H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)
- Additional tools
    - [Vowpal Wabbit repository](https://github.com/JohnLangford/vowpal_wabbit)
    - [XGBoost repository](https://github.com/dmlc/xgboost)
    - [LightGBM repository](https://github.com/Microsoft/LightGBM)
    - [Interactive demo of simple feed-forward Neural Net](http://playground.tensorflow.org/)
    - Frameworks for Neural Nets: [Keras](https://keras.io/) , [PyTorch](http://pytorch.org/) , [TensorFlow](https://www.tensorflow.org/) , [MXNet](http://mxnet.io/) , [Lasagne](http://lasagne.readthedocs.io/)
    - [Example from sklearn with different decision surfaces](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)
    - [Arbitrary order factorization machines](https://github.com/geffy/tffm)

## Software/hardware requirements
### Additional material and links
- StandCloud Computing:
    - [AWS](https://aws.amazon.com/), [Google Cloud](https://cloud.google.com/), [Microsoft Azure](https://azure.microsoft.com/)
- AWS spot option:
    - [Overview of Spot mechanism](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html)
    - [Spot Setup Guide](http://www.datasciencebowl.com/aws_guide/)
- Stack and packages:
    - [Basic SciPy stack (ipython, numpy, pandas, matplotlib)](https://www.scipy.org/)
    - [Jupyter Notebook](http://jupyter.org/)
    - [Stand-alone python tSNE package](https://github.com/danielfrg/tsne)
    - Libraries to work with sparse CTR-like data: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)
    - Another tree-based method: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))
    - Python distribution with all-included packages: [Anaconda](https://www.continuum.io/what-is-anaconda)
    - [Blog "datas-frame" (contains posts about effective Pandas usage)](https://tomaugspurger.github.io/)

## Feature preprocessing and generation with respect to models

### Numeric features
- Tree-based models do not depend on feature scale. 
- NN, kNN, and linear models do depend on feature scale.
	  - For linear models, regularization impact is proportional to feature scale.
	  - For kNN, larger features are treated as more important.

#### Preprocessing
- Scaling:
	  - Scaling to [0,1] interval (`sklearn.preprocessing.MinMaxScaler`)
		    - X = (X - X.min()) / (X.max() - X.min())
	  - Scaling to have mean 0, std 1 (`sklearn.preprocessing.StandardScaler`)
	  - Scaling should be initially applied to all numeric features equally.
	  - Scaling params can be optimized (tuned) to boost features which seem to be more important.
- Outliers
	  - To protect models from outliers, can clip feature values between lower and upper bounds (e.g., 1% and 99%). This is widely used in finanical data, and is called Winsorization.
- Rank transformation (`scipy.stats.rankdata`):
  	- set spaces between properly sorted values to be roughly equal.
  	- Can be a better option than `MinMaxScaler` because the rank transform will move outliers closer to other objects.
  	- Can be used for kNN, NN, and linear models if there is no time to handle outliers manually.
  	- To apply rank transform to TEST data, need to store the mapping; alternatively, concatenate train and test sets before applying rank transform.
- Other transforms that help linear models and especially NN:
  	- Log transform (`np.log(1+x)`)
  	- Raising to the power < 1 (`np.sqrt(x + 2/3)`)
  	- These transforms drag large values closer to the mean.
- Additional ideas:
  	- Train models on concatenated data frames produced by different preprocessings.
  	- Mix models trained on differently preprocessed data.

#### Feature generation
- Sometimes, features can be engineered by prior knowledge.
- GBDT models have difficulty with approximating multiplications and divisions. Adding new features that are linear combinations of other features can help make the model more robust with less trees.
- Fractional part: generate a new feature that is the fractional part of a feature with numbers after a decimal (e.g., .23 is the fractional part of 1.23).

### Categorical and ordinal features

#### Ordinal features
- Ordinal features are categorical features that are ordered in some meaningful way. However, unlike with numeric features, we don't know if the difference between categories 1 and 2 is equal to the difference between categories 2 and 3.
- Examples of ordinal features: ticket class (1,2,3), driver's license (A, B, C, D), Education (kindergarten, school, undergraduate, etc).

#### Label encoding
- The simplest way to encode a categorical feature is to map its values to different numbers. This is known as *label encoding*. This works with trees because a tree method can split features and extract most of the useful information in categories on its own. Non-tree-based methods cannot use label-encoded features effectively.
- Label encoding can be accomplished in the following ways:
  	- `sklearn.preprocessing.LabelEncoder` applies encoding in alphabetical order: [S,C,Q] -> [2,1,3].
  	- `Pandas.factorize` applies encoding in order of appearance: [S,C,Q] -> [1,2,3]
  		  - This is useful if the rows are sorted in some meaningful way.
  	- Frequency encoding: encode a feature via mapping values to their frequencies: [S,C,Q] -> [0.5,0.3,0.2]
  		  - This preserves some information about distribution of the values.
  		  - If frequency of category is correlated with target value, a linear model will use this dependency.
  		  - A tree model may have a lower number of splits due to the correlation.
  		```{}
  			encoding = titanic.groupby('Embarked').size()
  			encoding = encoding/len(titanic)
  			titanic['enc'] = titanic.Embarked.map(encoding)
  		```
  		  - In the case of ties for variables that occur with the same frequency, a rank operation can be applied: [S,C,Q] -> [0.5,0.3,0.2]
  		```{}
  			from scipy.stats import rankdata
  		```

#### One-hot encoding
- This is useful for linear models.
- One-hot encoded features are already scaled by definition.
- For a data set with a few numeric features and many one-hot encoded features, it may be difficult for a tree model to use the numeric features efficiently. More precisely, tree models will slow down, not always improving on previous results.
- If a categorical feature has too many unique values, one-hot encoding will add too many columns with zero values. Use sparse matrices to store this data efficiently.
- Use `pandas.get_dummies` or `sklearn.preprocessing.OneHotEncoder`.

#### Feature generation for categorical features
- One of the most useful examples of feature generation is interaction between categorical features. This is usually useful for linear models and kNN.
- To implement an interaction between two features, first concatenate the feature values into a string (e.g., "1male", "2female"), and then apply one-hot encoding.

### Datetime and coordinates
- Both these features differ significantly from numeric and categorical features.
- Since we can interpret the meaning of datetime and coordinates, we can come up with specific ideas about feature generation.

#### Datetime
- Most new features generated from a datetime fall into two categories: time moments in a period, and time passed since a particular event.
- Periodicity: day number in week, month, season, year, second, minute, hour. This is useful to capture repetitive patterns. Can also add non-common periodicity, e.g. when a patient receives medication every 3 days.
- Time since:
    - Row-independent; e.g. since 00:00:00 UTC, 1 January 1970
    - Row-dependent; e.g. number of days left until next holiday, or time passed since last holiday.
- Difference between dates: `datetime_feature_1 - datetime_feature_2`. E.g., for churn prediction, days since last purchase date or days since registration.
- These new generated features will then need to be treated accordingly; e.g. label or one-hot encoding for categorical.

#### Coordinates
- If have geographical coordinates, can add new features like distance to the nearest shop, hospitals, schools, etc.
- Divide a map into squares with a grid, and in each square find the most expensive flat. Then a new feature is the distance to this flat for each object in the square.
- Organize the data into clusters. Use cluster centers as important points, and generate new features as distances to the clusters.
- Find special areas, like an area with very old buildings, and add distance to this area as a feature.
- Calculate aggregated statistics for the area surrounding an object: number of flats (interpreted as the area's popularity), mean real estate price (indicates how expensive the area is).
- Trick for decision trees: can add slightly rotated coordinates as new features. This helps a model make more precise selections on a map. E.g., a street divides an area into high-cost and low-cost district. If the street is diagonal, this will require many splits; with rotated coordinates, this separation can be accomplished in one split. It can be hard to know which rotations to make, so we may want to add all rotations to 45 or 22.5 degrees.

### Handling missing values
- Missing values can look like NaN, empty strings, or outliers like -999.
- Sometimes, missing values can contain useful information.

#### Finding replaced missing values
- Missing values are sometimes already replaced in the data. One way to find such values is to plot a histogram. E.g., a historgram might show an smooth distribution for most numbers, but a large spike for a specific number. The number might be -1 or the mean value of the feature, which means missing values were replaced. 

#### Missing value imputation
- Three ways of dealing with Missing value imputation:
    - Replace by some value outside the normal range (e.g., -999, -1).
        - Pro: allows to take the missing value into a separate category.
        - Con: linear and NN model performance can suffer.
    - Replace with mean or median.
        - Pro: can help NN and linear models.
        - Con: for tree models, can be harder to select object which had missing value in the first place.
    - Reconstruct value
        - In time series, the rows of a dataset are not independent, and interpolation interpolation works well.
        - For data that is not time series, there will usually be no proper logic to reconstruct MVs.
- Feature generation
    - Add a new "isnull" indicator feature, indicating whether a specific feature is null or NaN. This solves the problem with trees and NN while computing the mean or median. However, this method doubles the number of columns in the dataset.
    - Be careful with replacing missing values before feature generation.
        - When encoding a categorical feature together with a numeric feature, ignore missing values when calculating mean values for the numeric features.
        - In general, avoid filling NaNs before feature generation since this can decrease the usefulness of the features.
- XGBoost can handle missing values that are NaN, and this can sometimes change the score drastically.
- Outliers can be treated as missing values when it makes sense.
- Categories which are present in test data but not in train data should be treated as missing values. Encode such a feature with frequencies of occurrence in the concatenated train and test datasets.


### Additional material and links
- Feature preprocessing
    - [Preprocessing in Sklearn](http://scikit-learn.org/stable/modules/preprocessing.html)
    - [Andrew NG about gradient descent and feature scaling](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling)
    - [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)

- Feature generation
    - [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
    - [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)



## Feature extraction from text and images

- Often in competitions, we have data like text or images. If only this kind of data is available, we can apply approach specific to this type of data. For example, we can use search engines to find text similar to that used in the Allen AI Challenge. For images, we can use CNN.
- But if we have text or images as *additional* data, we must craft specific features that can be added as complementary to our main data frame.
  - The Titanic dataset has **name** column, which is more or less like text. To use it, we must first derive useful features from it.
  - As a more serious example, we can predict if two online advertisements are duplicates - copies of each other with slight differences (Avito Duplicate Ads Detection competition). We can use images of the ads as complementary data.

### Feature extraction from text - bag of words
- This is the simplest approach.

#### TF-IDF
- Ceate a new column for each unique word in the data. Then, count the number of occurences of each word, and place this value in the appropriate column. Apply this operation to each row of the dataset. This can be done using  `sklearn.feature_extraction.text.CountVectorizer`.
- We can also post-process the calculated metrics using predefined methods. This may be necessary because some models (kNN, linear, NN) depend on feature scale. So, the main goal of post-processing here is to make samples more comparable, and boost more important features while decreasing the scale of useless features.
- One way to make the samples more comparable is to normalize the sum of values in each row. This counts not *occurences*, but *frequencies* of words. Thus, texts of different sizes will be more comparable. This is known as **Term frequency** transformation:
    ```{}
    tf = 1 / x.sum(axis=1)[:,None]
    x = x * tf
    ```
- To boost more important features, we post-process our matrix by normalizing data column-wise. A good idea is to normalize each feature by the inverse fraction of documents which contain the exact word corresponding to this feature. In this case, features corresponding to frequent words will be scaled down compared to features corresponding to rare words. We can further improve this idea by taking a log of these normalization coefficients. This will decrease the significance of widespread words in the dataset and perform the required feature scaling. This is the purpose of **Inverse Document Frequency** transformation.
    ```{}
    idf = np.log(x.shape[0] / x(>0).sum(0))
    x = x * idf
    ```
- TF and IDF are frequently used together, `sklearn.feature_extraction.text.TfidfVectorizer` in sklearn.
- There are other variants of TF-IDF which may work better for certain datasets.

#### N-grams
- Add not only column corresponding to a word, but also columns corresponding to N consequent words.
- This concept can also be applied to a sequence of chars. In cases with low N, we will have a column with each possible combination of N chars. For example, the number of bigrams for 28 unique symbols is equal to 28*28.
- Sometimes it can be cheaper to have every possible char n-gram as a feature instead of having a feature for each unique word in the data.
- Using char n-grams also helps model handle unseen words; for example, rare forms of already used words.
- In sklearn, `sklearn.feature_extraction.text.CountVectorizer` has an appropriate parameter for using n-grams (`Ngram_range`). To change from word n-grams to char n-grams, use the the parameter named `analyzer`.

#### Text preprocessing
- Usually, we may want to preprocess text before applying bag-of-words; preprocessing can help bag-of-words drastically.
- Lowercase: this reduces the number of redundant columns. `CountVectorizer` does this by default.
- Lemmatization and stemming
    - Stemming is a heuristic process that chops off the endings of words and thus unites derivationally related words. 
        - democracy, democratic, democratization -> democr
        - Saw -> s
    - Lemmatization uses knowledge of vocabulary and morphological analysis of words.
        - democracy, democratic, democratization -> democracy
        - Saw -> see or saw (depending on context)
- Stopwords
    - These are words that are insignificant (articles or prepositions), or are so common that they do not help to solve our task.
    - Most languages have pre-defined lists of stopwords which can be found on the internet or loaded from `NLTK` in python. `CountVectorizer` in sklearn also has a parameter `max_df` related to stopwords.

#### Pipeline for applying BOW
1. Preprocessing: lowercase, stemming, lemmatization, stopwords
2. N-grams can help to use local context
3. Postprocessing: TFiDF


### Feature extraction from text - word2vec
- Just as with the BOW approach, we want to get a vector representation of words and text. However, this approach is more concise.
- Word2vec converts each word to some vector in some sophisticated space which usually has 100's of dimensions. To learn the word embedding, word2vec uses nearby words. Different words that are often used in the same context will be very close in this vector representation, which will benefit our models.
- We can also apply basic operations like addition and subtraction on these vectors and expect the result of such operations to be interpretable. E.g., king + woman + man = queen.
- There are several implementations of this embedding approach besides Word2vec:
  - Words: Glove (global vector for word representation), FastText, etc.
  - Sentences: Complications may occur if we need to derive vectors for sentences. Here we may take different approaches. For example, we can calculate the mean or sum of the word vectors. Or, we can use Doc2vec.
- Training word2vec can take a long time, and pretrained models are available (e.g., trained on Wikipedia). Training does not require target values; it only requires the text to extract context for each word.
    - Note: all pre-processing techniques discussed earlier can be applied to text before training word2vec models.

#### BOW and w2v comparison
- BOW
    - Vectors are very large.
    - The meaning of each value of vector is known.
- Word2vec
    - Vectors are relatively small.
    - Values in the vector can be interpreted only in some cases.
    - Words with similar meaning often have similar embeddings. This is crucial in competitions.
- Usually, BOW and w2v give different results, and can be used together in one solution.

### Feature extraction from images

- Similarly to w2v for words, CNNs give a compressed representation for images.

#### Descriptors
- Besides getting the final output of a network, outputs from the inner layers (*descriptors*) can also be used. 
    - Descriptors from later layers are better to solve tasks similar to solve tasks similar to the one the network was trained on.
    - Descriptors from early layers have more task-independent information.
    - For example, if a CNN was trained on ImageNet, its later layers can be used in some car model classification task. But to use this network in some medicine-specific task, it is better to use an earlier layer, or even retrain the network from scratch.
    
#### Finetuning
- Sometimes, we can slightly tune a network to recieve more suitable representations using target values associate with our images. In general, the process of pretrained model tuning is called *finetuning*.
    - Finetuning, especially for small datasets, is usually better than training a standalone model on descriptors (it allows to tune all network parameters, and thus extract more effective image representations) or training a network from scratch (model can use domain knowledge already encoded in network parameters, which can lead to faster results and a faster retraining procedure).
  
#### Augmentation
- Augmentation increases the number of images to train a better network.
- Although augmentation is not as good as adding brand new images to a training set, this is still very useful.
- Augmentation reduces overfitting and allows to train more robust models.
- Examples of augmentation: crop, rotate, add noise.

### Additional material and links
- Feature extraction from text
    - Bag of words
        - [Feature extraction from text with Sklearn](http://scikit-learn.org/stable/modules/feature_extraction.html)
        - [More examples of using Sklearn](https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/)
    - Word2vec
        - [Tutorial to Word2vec](https://www.tensorflow.org/tutorials/word2vec)
        - [Tutorial to word2vec usage](https://rare-technologies.com/word2vec-tutorial/)
        - [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)
        - [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)
- NLP Libraries
    - [NLTK](http://www.nltk.org/)
    - [TextBlob](https://github.com/sloria/TextBlob)
- Feature extraction from images
    - Pretrained models
        - [Using pretrained models in Keras](https://keras.io/applications/)
        - [Image classification with a pre-trained deep neural network](https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11)
- Finetuning
    - [How to Retrain Inception's Final Layer for New Categories in Tensorflow](https://www.tensorflow.org/tutorials/image_retraining)
    - [Fine-tuning Deep Learning Models in Keras](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)

***

# Week 2

## Exploratory Data Analysis (EDA)

### EDA: what and why?
- EDA allows to:
    - Better understand the data.
    - Build intuition about the data.
    - Generate hypotheses about new features.
    - Find insights.
- One of the main EDA tools is visualization. When we visualize the data, we immediately see patterns. 

### Building intuition about the data

#### Getting domain knowledge
- It is preferable to understand what the aim is, what data we have, and how people usually tackle the specific kind of problem to build a baseline.
- The first step would be searching the internet on the topic and making sure we understand the data.
    - Example: when predicting advertising cost, look into Google Adwords.

#### Checking if the data is intuitive
- E.g. if there is a column with age data, check for strange outliers.
- E.g. if number of clicks is higher than the number of impressions, something is wrong with that data row.
- Such mistakes can be used to generate new features. E.g. for the advertising data, if the number of impressions is 0 but clicks is non-zero, add a new indicator column `is_incorrect` that is 1 when clicks > impressions.

#### Understanding how the data was generated
- What was the algorithm for sampling objects from the database? Maybe the competition host sampled the data at random, or they oversampled a particular class.
- Were the train and test data generated with the same algorithm? If the train and test sets are different, we cannot use part of the train set for validation, since it will not be representative of the test set.
- It is crucial to understand the data generation process to set up a proper validation scheme.

### Exploring anonymized data

- Sometimes, competition organizers do not want some information to be revealed, so they make an effort to export the data in a way that one could not get value out of it. Yet, all the features are preserved, and the ML model would be able to do its job.
    - For example, if a company wants a model to classify its documents, but doesn't want to reveal the document content, it can replace all the word occurences with hash values of those words.
- Things to try while exploring this data:
    - Explore individual features:
        - Guess the meaning of the columns.
        - Guess the type of the column (categorical, numeric, text, date). 
    - Explore feature relations:
        - Find relations between pairs.
        - Find feature groups.
- Example on competition data:
  - Build a quick random forest model and check feature importances.
  - Check feature mean and standard deviation. If a feature was standard scaled (mean close to 1 and std close to 0), it may be possible to unscale.
  - Check counts of unique feature values to see if any values are frequently repeated.
- Helpul functions:
    - `df.dtypes`: pandas function that guesses column types.
    - `df.info`
    - `x.value_counts()`
    - `x.isnull()`

### Visualizations

- EDA is an art, but there are several tools.

#### Tools for exploring individual features
- Histograms
    - Try to vary the number of bins, because histograms can be deceiving.
    - Use transformations (like log) before plotting historam. This can reveal that the data has a specific distribution.
    - If a particular value (like the mean) occurs very frequently, it may indicate that missing values were replaced. Can then use this information in various ways: convert back to NaN for XGBoost, add a new feature indicating NaN or not, replace with a different value, impute, etc.
    - Use `plt.hist(x)`.
- Plot index versus value
    - Scatter plot row index versus feature value.
    - Horizontal lines in this plot indicate a lot of repeated values ina  particular feature.
    - Vertical lines indicate that the data may not be properly shuffled.
    - Use `plt.plot(x, '.')`.
- Plot index versus value with color coding for class labels.
    - This shows whether a feature is correlated with class separation, and whether the data is shuffled.
    - Use `plt.scatter(range(len(x)), x, c=y)`.
- Check statistics
    - Check mean, std, and percentiles.
    - Use pandas `df.describe()`.
- Other tools
    - value counts: `x.value_counts()`.
    - null: `x.isnull()`. Null patterns can be plotted as row index vs feature index, colored by description (NA, empty string, -1, very large number, -99999 (and less), 999, 99).

#### Tools for exploring feature relations
- Sometimes it's beneficial to look at feature pairs.
- The best tool here is scatterplot: `plt.scatter(x1,x2)`. 
    - Plot one feature against another, and color by class label for classification or target value for regression. Can also visualize a regression target value by point size.
    - Check if the distributions of the train and test sets are the same. Plot two features from the train set and color by class, then plot the same features from the test set and color all test observations gray. For example, if the gray points are located in a region where there are no train points, this means that the train and test sets are in different distributions. In this case, make the same kind of plot for other feature pairs to rule out that this specific feature pair is not overfitted and that there is no bug in the code.
    - If the data only has a small number of features, it is possible to plot all feature pairs at once using `pd.scatter_matrix(df)`.
    - It is a good idea to look at scatter plots and histograms at the same time, since scatter plots give weak information about densities and histograms do not show feature interactions.
- We can also compute some kind of distance between the feature columns and store them in a matrix of size num_of_features^2.
    - For example, we can compute correlation between the columns using `df.corr()`, and plot it with `plt.matshow(...)`.
    - We can also compute other numbers:
        - How many times is one feature larger than the other?
        - How many distinct combinations do features have in the dataset?
    - If the matrix looks like a mess, it is possible to run a clustering (e.g. kmeans) on the matrix and reorder the features. This may visualize feature groups.

#### Tools for exploring feature groups
- A clustered feature matrix may have feature groups, and it is usually a good idea to generate new features based on the groups. E.g., some statistics calculated over the group may work well.
- Calculate the statistic (e.g., mean value) of each feature, and then plot it against column index: `df.mean().plot(style='.')`.
    - The plot may look random if the columns are shuffled, but may reveal a pattern when the columns are sorted based on the statistic: `df.mean().sort_values().plot(style='.')`.
    - If there is a pattern, use the feature groups to generate new features.
  
### Data cleaning and other things to check

It is important to understand that competition data can be only a part of the data that the organizers have. The organizers could provide a fraction of the observations they have, or a fraction of the features.

#### Constant features
- A feature can have only one constant value in both training and test sets, or a constant in the training set, and different values in the test set.
- This could be due to the sampling procedure (E.g., the competition only has data from one particular year, while the full dataset spans several years).
- Such features are not useful for the model, and should be removed.
- Use `traintest.nunique(axis=1) == 1` to check.
- If there are values that are only present in the test set, this situation needs to be handled properly. We need to decide if these values matter much or not. This can be checked with a separate validation set, comparing the prediction quality using observations with features with same values, and observations with new values. Then, we decide if we should remove this feature, or create a separate model that works with these new feature values.

#### Duplicated features
- Sometimes features are identical.
- Use `traintest.T.drop_duplicates(0)` to drop duplicate columns.
- If categorical features have duplicates but their levels have different names, drop the features using label encoding:
    ```{}
    for f in categorical_feats:
        traintest[f] = traintest[f].factorize()
    traintest.T.drop_duplicates()
    ```

#### Duplicated rows
- If there are duplicate rows, check if they have the same label.
- Duplicate rows may be the result of a mistake. Try to understand why.
- Check if train and test sets have the same rows. We can manually set labels for the test rows that are present in the train set.

#### Dataset shuffling
- Check if the dataset is shuffled. If it is not, there is a high chance that there is a data leakage.
- Plot a feature or target versus row index. Optionally, smooth the values using a running average. Also plot the mean of the feature vs the index on the same plot (this should be a horizontal line). If the data was shuffled properly, there should be a random spread around the mean value.
    - In the example shown, the end of the train set has a lower rolling mean than the rest of the data. This may not necessarily generate a new feature, but is useful to understand.
  
#### Cool visualizations
- Try to visualize every possible thing in the dataset, because visualizations will lead to magic features.

### Additional material and links
- Visualization tools
    - [Seaborn](https://seaborn.pydata.org/)
    - [Plotly](https://plot.ly/python/)
    - [Bokeh](https://github.com/bokeh/bokeh)
    - [ggplot](http://ggplot.yhathq.com/)
    - [Graph visualization with NetworkX](https://networkx.github.io/)
- Others
    - [Biclustering algorithms for sorting corrplots](http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html)


## Validation
### Validation and overfitting
- It is not a rare case in competitions that scores jump down on the leaderboard after private results are revealed. There are two main reasons for this:
    - Competitors could ignore the validation and select the submission which scored best against the public leaderboard.
    - Sometimes competitions have no consistent public/private data split. Or, they have too little data in either public or private learderboard.
- While we as participants cant influence the competition's organization, we can certainly make sure that we select our most appropriate submission to be evaluated on the private leaderboard.
- This section provides a systematic way to set up validation in a competition and tackle most common validation problems.

#### Validation
- With validation, we want to check if the model gives expected results on unseen data.
- Motivating example: predicting whether a patient will have a particular disease in the near future.
    - We need to be sure that the model we train will be applicable in the future, and what quality this model will have.
    - Usually we divide the data we do have into two parts: train and validation. We fit the model on the train part and check its quality on the validation part.
    - We should be ready for the possibility that unseen data from the future will differ from the train data.
- In competitions, we have a similar situation. The organizers give training data with all target values, and test data without target values. As before, we should split the data into train and validation parts.
- Furthermore, to ensure the competition spirit, the organizers split the test data into public and private parts. We see the scores for the public test set during the competition. The private test scores are released only after the competition is over. This ensures that we do not cheat using the test set (overfit).
    - For example, after data is already divided into train and val parts, repeated checking against the val set will produce better scores for some models simply by chance. If we continue to select best models, modify them, and again select the best, we will see constant improvements in the val score. But it doesn't mean that the score on the test data. By repeating this over and over, we could overfit the validation set, or in terms of the competition, we could cheat the public leaderboard. Unrealistically good scores on the public learderboard will later jump down on private leaderboard.
- We want our model to capture patterns in the data, but only those patterns that generalize well between both train and test data.

#### Underfitting and overfitting
- To choose the best model, we should avoid both underfitting and overfitting.
- **Uderfitting**: If the model is too simple, it can't capture the underlying relationships, and will get poor results.
- **Overfitting**: The complexity of the model can be increased to improve results. Prediction quality on the training data will go up, but if the model is fits the training data too much, it will start describing noise and patterns that don't generalize to the test data.
- Note: The meaning of overfitting *in general* is slightly different than overfitting in ML competitions.
    - In general, we say that a model is overfitted if the quality is better on train than on test set.
    - In competitions, we say that a model is overfitted only in the case that the quality on the test set is worse than expected.
    - A different approach to the same concept: plot of error (loss) vs model complexity (plot not shown here).
        - Underfitting shows up as high error on both train and val sets.
        - Overfitting shows up as low error on train and high error on val set.
        - Somewhere in the middle is the model with best complexity - it has the lowest val error, and thus is expected to have the lowest error on the unseen test data.
        - Note: in the plot, the training error is always lower than the validation error. This implies overfitting in the general sense, but does not imply overfitting in the context of competitions.


### Validation strategies

- **Note**: The samples between train and validation must not overlap. If they do, we just can't trust our validation. This is sometimes the case with repeated samples in the data. The predictions for these samples will be better, and more optimistic quality estimation overall. This will prevent us from selecting the best parameters for the model.
    - Overfitting is generally bad, but if we have duplicated samples in both train and test simulaneously, and we overfit, the validation score will deceive us into a belief that we are moving in the right direction.
- The following are three often-used validation strategies, and the main difference between them is the number of splits being done.

#### 1. Holdout

- Simple split that divides the data into two parts.
- Fit the model on train set, and evaluate its quality on the val set. Using scores from validation, select the best model. Before making a submission, retrain the model on all data that has labels.
- This method is a good choice for a large amount of data.
- In python:
    - `ngroups = 1`
    - `sklearn.model_selection.ShuffleSplit`

#### 2. K-fold

- Repeated holdout. Split the data into k parts and iterate through them, using every part as a validation set only once. Then average scores over these k folds.
- There is a difference between k-fold and the above holdout strategy repeated k times. While it is possible to average the scores of k different holdout splits, some samples may never be in validation, while others may be placed there multiple times. The core idea of k-fold is that we want to use every sample for validation only once.
- This method is a good choice for a medium amount of data and we can get either a sufficiently big difference in quality, or different optimal parameters between folds.
- Example - estimating the complexity of k-fold compared to holdout. For k=5, and a holdout split of 80% train & 20% test, if prediction takes an insignificant amount of time, k-fold is similar to holdout repeated 5 times. Thus, k-fold will be 5 times slower.
- In python:
    - `ngroups = k`
    - `sklearn.model_selection.Kfold`

#### 3. Leave-one-out

- This is a special case of k-fold when k equals the number of samples in the data.
- Iterate through every sample in the data, each time using k-1 objects as the train data, and the one left-over object as the test data.
- This method can be helpful for when the data is small and the model is fast enough to retrain.
- In python:
    - `ngroups = len(train)`
    - `sklearn.model_selection.LeaveOneOut`

#### Stratification

- We usually use k-fold or holdout on shuffled data. By shuffling data, we are trying to reproduce random train/validation splits. But sometimes, especially if the data is small, or target classes are highly imbalanced, a random split can fail.
- Stratification is a way to ensure that we get a similar target distribution over different folds, thus making validation more stable. It is useful for small datasets, unbalanced data sets (e.g. target average very close to 0 or 1 for binary classification), and multiclass classification (large amount of classes).
    - For datasets not in the above list, stratification will be quite similar to a shuffle (random) split.
    
### Validation strategies - document

This section contains information about main validation strategies (schemes): holdout, K-Fold, LOO.

The main rule you should know: never use data you train on to measure the quality of your model. The trick is to split all your data into training and validation parts.

Below you will find several ways to validate a model.

a) **Holdout scheme**:
    1. Split train data into two parts: partA and partB.
    2. Fit the model on partA, predict for partB.
    3. Use predictions for partB for estimating model quality. Find such hyper-parameters, that quality on partB is maximized.

b) **K-Fold scheme**:
    1. Split train data into K folds.
    2. Iterate though each fold: retrain the model on all folds except current fold, predict for the current fold.
    3. Use the predictions to calculate quality on each fold. Find such hyper-parameters, that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement.

c) **LOO (Leave-One-Out) scheme**:
    1. Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset).
    2. In the end you will get LOO predictions for every sample in the trainset and can calculate loss. 

Notice, that these are *validation* schemes are supposed to be used to estimate quality of the model. When you found the right hyper-parameters and want to get test predictions don't forget to retrain your model using all training data.

### Data splitting strategies
- This section covers more concrete examples of data splitting.

#### Different approaches to validation (time series example)
- Task: predict the number of customers for a shop for each day during the next month.
- Two strategies are possible here: split using random choice, or make a time-based split (every row before a day as train, and every day after as test).
  - If the train/validation split is made differently than the train/test split, the model created will be useless.
- The **main rule of reliable validation**: set up validation to mimic the train/test split.
- Suppose that we have a pool of models trained on different features and we select the best model for each type of validation. Will these models differ, and how significantly?
    - It is certain that a model optimized using random splits (and so favoring points before and next target values) will perform poorly, since *next* observations for are not available for the test data (NaN fed to model, and model probably has not had much experience with NaNs).
    - The model for the second type of validation was trained to predict many points ahead, and will not use adjacent target values.
    - An important conclusion here is that the most useful features for one model are useless for another.
- The generated features are not the only problem here. The actual train/test split is time-based. If we carefully generate features that draw attention to time-based patterns, will we get reliable validation with a random split? Or put another way, if we create features which are useful for a time-based split and are useless for a random split, is it correct to use random split to select the model?
    - Consider the case when the target follows a linear trend (diagonal line plot). In general, the model predictions will be close to target mean value (the mean y value across all samples), calculated using the train data. For random-based val set, if the validation points are closer to the mean value compared to test set, we get a better score on validation than on test. For time-based val set, the val points are nearly as far as the test points for the target mean value, and the validation score will be similar to the test score. This shows that for incorrect validation, not only the features but even the target can lead to unrealistic estimation of the score.
- In summary, different splitting strategies can differ significantly in the generated features, in the way that the model will rely on those features, and in some kind of target leak. To be able to great features, we absolutely must identify the train-test split made by the competition organizers, and reproduce it.
  
#### Splitting data into train and validation
- Most splits fall into 3 categories:
    - Random, row-wise
    - Time-wise
    - By id

##### Random split by rows
- This usually means that rows are fairly independent of each other.
- There may be some dependency between rows. E.g., for a task to predict whether a person will pay off a loan, the data might have members of the same family, or people who work for the same company. If a family member is present in the train, and another member of the same family is present in test, a special feature can be devised just for this case.

##### Time-based split
- Treat everything before a specific data as train, and everything after as test.
- In this approach, it is useful to make special features based on the target. E.g., for predicting the number of customers in the coming week, add features like the number of customers on the same day in the previous week, or the number of customers for the past month.
- Moving window validation is a special case of the time-based split. This entails creating several train/val splits by moving the validation week up by one week forward across the training set, increasing the train data by that one week: start with 3 weeks train and use 4th week for val, then use 4 weeks for train and 5th week for val, etc.

##### Id-based split
- For example, a task to recommend music for completely new users. This means that the train and test sets have completely different sets of users. We can probably make the conclusion that features based on user history (e.g., number of songs listened in last week) will not help for new users.
- Specific competition examples:
    - Caterpillar: data in this competition was split on id, but the id was hidden from competitors comp.
    - Nature concervancy; Intel & MobileODT Cervical Cancer Screening
        - Train and test data did not have overlap on id, but id was hidden.
        - Competitors came up with their own id schemes by clustering pictures.
            - In the case where pictures were taken one after another, the images were quite similar.

##### Combined methods
- Splitting methods can sometimes be combined.
- Example 1: for a task of predicting sales in shops, we can choose a split date for each shop independently instead of using one data for every shop in the data.
- Example 2: for a dataset of search queries from multiple users and search engines, split the data by a combination of user id and search engine id.
- Example 3: Home Depot product search relevance competition
    - Task: estimate search relevancy
    - Data consists of search terms and search results for those terms. Test set contained completely new terms. So, one shouldn't use a random split (favors more complicated models and overfitting) or a search-term-based split (leads to underfitting) for validation.
    - In order to select optimal models, it was crucial to select the ratio of new search terms from train/test split.

### Problems occurring during validation
- Problems in validation can be divided into two groups:
    - Problems during local validation: caused by inconsistency of the data (e.g. getting different parameters for different folds). Sovle by a more thorough validation.
    - Problems during submission: scores on validation and on the leaderboard don't match. This is usually because we can't mimic the exact train/test split on our validation.

#### Validation stage problems
- Generally, the main problem is a significant difference in scores for different train/val splits.
- Example: predicting sales in a shop in February. 
    - Given target values for the whole previous year, and take the last month (January) as validation. 
    - However, January has more holidays than February, causing people to buy more, thus leading to higher target values. MSE of predictions will be higher for Jan than Feb.
    - This does not mean that the model will perform worse for February, at least in terms of overfitting.
    - This example illustrates that this kind of behavior can sometimes be expected. But what if there is no clear reason why scores differ for different folds?
    
##### Causes of different scores and optimal parameters
- Too little data: not enough observations to generalize the high amount of patterns and trends available in the data. In this case, a model will utilize only some general patterns, and for each train/val split, these patterns will partially differ.
- The data is too diverse and inconsistent: for example, two similar samples with different target values can confuse a model.
    - If one such sample is in train, while the other is in val, we can get a pretty high error for the second sample.
    - If both samples are in validation, we will get smaller errors for them.
    - For the Jan/Feb sales example from earlier, we know the natural reason for the difference in scores. Diversity can be reduced by validating on February from the previous year.

##### Extensive validation
- To solve the above two problems, we should do extensive validation:
    - Increase k in k-fold (usually 5 is enough).
    - Make several k-fold splits with different random seeds, and average the results.
    - Use one set of splits to tune parameters, and another set of splits to check model quality.

#### Submission stage problems
- There are two cases of these issues:
    - LB score is consistently higher/lower than validation score
    - LB score is not correlated with validation score at all
##### Causes of submission stage problems
- We may already have quite different scores in k-fold. If we view the LB as another validation fold, then getting a different score on LB is not surpising in this case. We can also calculate the mean and std of the validation scores, and estimate if the LB score is expected.
- Other causes:
    - Too little data on public LB. In this case, trust your validation.
    - Train and test data are from different distribution.
        - Example: predicting heights from user photos. Train data contains only women, and test data contains only men. In this case predictions on train data are going to be close to the average height of women, and the model will have bad results on the test data.
        - Sometimes, this problem can be solved by adjusting the solution during training. But sometimes, this problem can be solved *only* by adjsting the solution to the LB (through leaderboard probing). In this case, the strategy is to find the optimal constant prediction for train, and for test data. Then, shift the predictions by that difference. 
            - For the height example, average height of women can be calculated from the train data.
            - If the metric of competition is MSE, calculate average male height by sending two constant submissions, writing down a simple formula, and finding out that the average value for test is 70 inches.
    - It is more common to find that the distributions are not completely different. For example, the train data can consist not *only* of women, but *mostly* of women, and test consists *mostly* of men.
        - To deal with this situation, remember to mimic the train/test split. Force the validation set to have the same distribution as the test. This is true for getting both scores and parameters correct.
- Example: Click-through rate prediction
    - Train data (history of displayed ads) does not contain ads which were not shown. Test data consists of *every possible* ad. This is an example of different distributions between train and test, since train has a huge bias towards shown ads.
    - To set up correct validation, complete the val set with rows of not-shown ads.
- Example: predict whether a user will listen to a song predicted by the system.
    - Test set contains only recommended songs. Train contains both recommended songs and songs users selected themselves.
    - Adjust validation by filtering out songs selected by users.

#### Leaderboard shuffle
- LB shuffle is when participants' positions on public and private leaderboard drastically differ. Shuffle can be due to randomness, little amount of data, and different public/private distributions.

##### Randomness
- This is the case when all participants have very simlar scores (good or poor).
- Example 1: Competitor scores are all very close, but many competitors overfit to public LB.
- Example 2: Unpredictable financial data.

##### Too little data overall, especially in private LB
- The train set consisted of < 200 rows, and test consisted of < 400 rows. Shuffle was expected.

##### Different public/private distributions
- This is usually the case with time series data.
- As competitors adjust their scores to the public LB, they overfit.
- In this case, it is better to trust the validation.

### Additional material and links
- [Validation in Sklearn](http://scikit-learn.org/stable/modules/cross_validation.html)
- [Advice on validation in a competition](http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/)

## Data leakages

### Basic data leaks
- We define leakage in a very general sense as unexpected information in the data that allows us to make unrealistically good predictions. You may think of it as directly or indirectly adding ground truth into the data.
- Data leaks are completely unusable in the real world. They provide too much signal and make the competition lose its main point and turn it into leak-hunting.
- This section discusses the main types of data leaks that appear during machine learning, competition-specific LB probing, and concrete walkthroughs.

### Leaks in time series
- Check the train, public and private splits. If any one of them is not split on time, this is a data leak. In this case, unrealistic features (e.g., price next week) will be the most important.
- Even if split by time, data still contains information about the future (in the test set)
    - E.g., user history in CTR tasks, fundamental indicators in stock market prediction.
- There are two ways to eliminate data leakages:
    - Make rows from future inaccessable
    - Test set with no features, only ID.

### Unexpected information
#### Metadata
- These types of leaks are much harder to find.
- Often, more than just train and test files are available in a competition (e.g., image or text archives).
- In this case, we can access metadata: file creation data, image resolution, etc. This information may be connected to the target variable.
- E.g., for a cat vs dog image classification competition, what if pictures of cats were always taken before dogs, or with a different camera? A good practice for the organizer would be to erase the metadata, resize the pictures, and change creation date.
#### Information on IDs
- It makes no sense to include IDs in a model, and it is assumed that the IDs are automatically generated.
- In reality, an ID could be a hash of something not intended for disclosure, and may contain traces of information connected to the target variable.
    - In the Caterpillar competition, adding ID as a feature slightly improved the result.

#### Row order
- In the trivial case, data may be shuffled by target variable.
- Sometimes adding row number or relative number suddenly improves the score.
- Example: In the TalkingData competition, rows next to each other usually had the same label.

### Leaderboard probing
- This is a competition technique tightly connected with data leaks.
- There are two types of LB probing. The first is extracting all ground truth from the public LB. This gives a little more training data, and is relatively easy to do: in every submission, change only a small set of rows to unambiguously calculate ground truth for those rows from LB score.
- This section mainly focuses on another type of LB probing. The public/private split is supposed to protect private part of test from information extraction, but is still vulnerable. Sometimes, it is possible to make submissions in such a way that will give out information about private data.
- Categories tightly connected with 'id' are vulnerable to LB probing (e.g. company of user in RedHat competition; Year, Month, Week in WestNile competition).
    - Example: a chunk of data with the same data for every row; rows with same IDs have the same target. Organizers split this chunk into public and private parts, but we still know that this particular chank has the same target for every row. After setting all the predictions close to zero in our submission for that particular chunk of data, we can expect 2 outcomes:
        - If score improves, that means that ground truth in public is 0. Then the ground truth in private is 0 as well, since all rows in the chunk have the same target value.
        - If score is worse, it means that the ground truth in both public and private is 1.
    - This is an annoying, technical data leak. Even if it is released close to the competition deadline, there are not enough submissions to fully exploit it.
    - Also, "consistent category" does not necessarily mean *exactly* the same target. It could be consistent in different ways. For example, the target label, could simply have the same distribution for private and public parts of the data.
        - In Quora question pairs competition (binary classification evaluated by log loss), target variable had different distributions in train and test, but allegedly the same in public and private parts of the data.
- Peculiar examples of data leakage
    - Truly Native: predict whether the content of an HTML file is sponsored or not.
        - There was a leak in the archive dates (sponsored and non-sponsored files were created in different periods of time). But do we really get rid of data leakage after erasing archive dates?
        - No, because text in HTML files may be connected to dates in a lot of ways, from explicit time-stamps to news content. The real problem was not meta-data leak but rather data collection. Even without meta information, ML algorithms will focus on actually useless features - features that only act as proxies for the date.
    - Expedia hotel recommendations: predict the hotel group a user is going to book.
        - One feature was the distance between the user's city to a hotel. This is a huge data leak. Using this feature, it was possible to reverse-engineer true coordinates and simply map ground truth from train to test.
    - Flavours of physics: predict something that has never been observed, using simulated data. Special statistical tests were implemented to punish models that exploit simulation flaws. But the tests could be bypassed to get a perfect score on the LB.
    - Quora question pairs: predict whether pairs of items are duplicates.
        - Like in all pairwise competitions, participants are not asked to evaluate *all* possible pairs - there is always some non-random subsampling. This subsampling is the cause of data leakage.
        - Usually, organisers mostly hard-to-distinguish pairs. This leads to imbalance in item frequency: more frequent items have a higher possibility of being duplicates.
        - We can create a connectivity matrix NxN, where N is the total number of items. If item i and item j appeared in a pair, put 1 in positions ij and ji. Treat the rows of this matrix as vector representations of every item. This means that we can compute similarities between those vectors. This trick works because two items have similar sets of neighbors, they have a high possibility of being duplicates.

### Additional material and links
- [Perfect score script by Oleg Trott](https://www.kaggle.com/olegtrott/the-perfect-score-script) -- used to probe leaderboard
- [Page about data leakages on Kaggle](https://www.kaggle.com/wiki/Leakage)



# Week 3

## Metrics optimization

### Motivation

#### Metrics
- Metrics are an essential part of any competition, and are used to evaluate submissions.
- There are many ways to measure the quality of an algorithm, and each organizer decides the most appropriate one for their particular problem.
    - E.g., an online shop is trying to measure the effectiveness of their website. Effectiveness needs to be formalized; need to define a metric to measure effectiveness. It can be the number of times a website was visited, or the number of times something was ordered using this website. The company decides which metric is the most important, and then tries to optimize it.

#### Motivation and take-away points
- It is really important to optimize the exact metric given in the competition, and not any other metric, since the chosen metric determines the optimal decision boundary.
- The biggest problem is that some metrics cannot be optimized efficiently; i.e. there is no simple enough way to find, say, the optimal hyperplane. In this case, sometimes we would train the model to optimize something different than the competition metric, but would need to apply various heuristics to improve the competition metric score.
- There is another case where we need to be smart about the metrics: it is when the train and test sets are different. In the lesson on leaks, we discussed leaderboard probing - we can, for example, check if the mean target value on public part of test set is the same as on train. If not, we would need to adapt our predictions to suit the test set better. This is a specific metric optimization technique we apply, because train and test sets are different.
- There can be more severe cases where an improved metric on the val set does not result in improvement on the test set. In this case, stop and think if there's a different way to approach the problem.
    - In particular, time series can be very difficult to forecast, even if the validation was done correctly. This can be because the distribution in the future is very different, or there is not enough training data. 
        - Example from a specific competition (using a trick to boost the score after modeling):
$Loss(\hat{y_i};y_i) = |y_i - \hat{y_i}|$ if trend predicted correctly, $(y_i-\hat{y_i})^2$ if trend predicted incorrectly
            - If the *trend* is guessed correctly, then the absolute difference between prediction and target is considered as the error. If model predicts next value to be higher than previous, but in reality it is lower, then the trend is predicted incorrectly, and the error is set to squared absolute difference.
            - This model cares more about predicting the correct trend than predicting the correct value.
            - In this competition, there were several time series to forecast, the horizon was long, and model predictions were unreliable.
            - It was not possible to optimize the metric mentioned above, and so it was better to set all the predictions to $y_{last} + 10^{-6}$ or $y_{last} - 10^{-6}$ - same value for all the points we are to predict for each time series. The sign depends on the estimation of what is more likely: the values on the horizon being lower than the last known value, or to be higher.

### Regression metrics review I
- Notation:
    - $N$ - number of objects
    - $y \in R^N$ - target values
    - $\hat{y} \in R$ - predictions
    - $\hat{y_i} \in R$ - prediction for i-th object 
    - $y_i \in R$ - target for i-th object

#### MSE: Mean Square Error
$$MSE = \frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2$$
- This is the most common metric for regression problems. Used when there is no specific preference to the problem solution, or no other metric is known.
- MSE measures the average squared error of the predictions.
- Visualization: for a dataset (of 5 points), how will the error change if we fix all predictions but one to be perfect, and vary the value of one prediction?
    - The MSE plot will look like a parabola, with the lowest value being when the prediction equals the target.
- Baseline model
    - The simplest baseline model does not use any features, and just predicts a constant value $\alpha$ that minimizes the MSE of the dataset.
    - The value $\alpha$ can be found by setting the derivative of MSE w.r.t. $\alpha$ to 0, and solve for $\alpha$.
    - The best $\alpha$ is the mean value of the target column.
- There are two other metrics related to MSE, discussed below.

##### RMSE: Root mean square error
- $RMSE = \sqrt{MSE}$
- The square root is introduced to make the scale of the errors to be the same as the scale of the target.
- Similarities between MSE and RMSE:
    - Every minimizer of MSE is also a minimizer of RMSE, and vice-versa.
    - More generally, $MSE(a) > MSE(b) \iff RMSE(a) > RMSE(b)$.
    - This works because the square root function is non-decreasing.
    - This means that if the target metric is RMSE, we can still compare models using MSE, since MSE orders models in the same way as RMSE. We can also optimize with MSE instead of RMSE. In fact, MSE is easier to work with, so everybody uses MSE instead.
- Differences between MSE and RMSE:
    - There is a difference for gradient-based models: $\frac{dRMSE}{d\hat{y_i}} = \frac{1}{2\sqrt{MSE}}\frac{dMSE}{d\hat{y_i}}$. The value $\frac{1}{2\sqrt{MSE}}$ does not depend on i. This means that traveling along MSE gradient is similar to traveling along RMSE gradient, but with a different learning rate which depends on MSE itself.
    - So MSE and RMSE are not immediately interchangeable for gradient-based methods. You will probably need to adjust some parameters, like the learning rate.

##### R-squared metric
$$R^2 = 1 - \frac{\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2}{\frac{1}{N}\sum_{i=1}^N(y_i-\bar{y_i})^2}= 1 - \frac{MSE}{\frac{1}{N}\sum_{i=1}^N(y_i-\bar{y_i})^2},$$
$$\bar{y} = \frac{1}{N}\sum_{i=1}^Ny_i$$

- To judge model quality by MSE and RMSE, take into account the properties of the dataset and the target.
- When MSE of predictions is 0, $R^2$ is 1. When the model MSE equals to MSE of the constant model, $R^2$ is zero.
- To optimize $R^2$, we can optimize MSE, since the constants in the $R^2$ equation do not matter for optimization.

#### MAE: Mean Absolute Error
- $MAE = \frac{1}{N}\sum_{i=1}^N|y_i-\hat{y_i}|$
- The error is an average of the absolute difference between the target and the predictions.
- This metric penalizes large errors *less* than MSE does, and so is not as sensitive to outliers as MSE.
- MAE is widely used in finance, where a \$10 error is usually exactly 2 times worse than a \$5 error. On the other hand, MSE thinks that a \$10 error  is 4 times worse than a \$5 error. MSE is easier to justify and explain.
- MAE is *not* the same as MSE from an optimization perspective.
- Baseline model
    - The best constant $\alpha$ for MAE is the median of the target values. 
- MAE gradients:
    - The gradient for MAE w.r.t. predictions is a step function: it equals -1 when $\hat{y_i} < y_i$, and 1 when $\hat{y_i} > y_i$.
    - The gradient is *not defined* when the prediction is perfect (MAE is not differentiable), but this happens rarely.
        - Practically, check for this and return 0 when prediction is exactly equal to target, and return the true gradient otherwise.
    - The second derivative is not defined at 0, and is 0 elsewhere.

#### MAE vs MSE
- MAE is more robust than MSE (less sensitive to outliers), but this does not mean that it's always better to use MAE. 
- The question is - are there any *real* outliers in the dataset, or are there just unexpectely high values that should be treated like any other value? Outliers are usually mistakes, measurement errors, etc, but similarly-looking observations can be natural.
- Therefore, if the extreme values are *not* outliers, do not use a metric which will ignore them.
- To summarize: If sure that data has outliers, use MAE. If these are unexpected values that are still important, use MSE.


### Regression Metrics Review II

- Example problem: predicting the number of laptops sold by two shops.
    - Shop 1: predicted 9, sold 10, MSE = MAE = 1
    - Shop 2: predicted 999, sold 1000, MSE = MAE = 1
    - The error for shop 1 is much more critical than in the 2nd case, but MSE and MAE are both 1 for both shops. According to these metrics, the two errors are indistinguishable. This is because MAE and MSE both work with absolute errors. However, relative error can be more important, since an off-by-1 error for shop 1 is equivalent to an off-by-100 error for shop 2.
- On the error plots for MSE and MAE, we see that all the curves have the same shape for every target value (the curves are just shifted versions of each other). This is an indicator that the metric works with absolute errors.

#### From MSE and MAE to MSPE and MAPE
- Relative error preference can be expressed with MSPE (mean square percentage error) and MAPE (mean absolute percentage error):
    - $MSPE = \frac{100\%}{N}\sum_{i=1}^N(\frac{y_i-\hat{y_i}}{y_i})^2$
    - $MAPE = \frac{100\%}{N}\sum_{i=1}^N|\frac{y_i-\hat{y_i}}{y_i}|$
- For each observation, the absolute error is devided by the target value, giving relative error.
- MSPE and MAPE can be thought of as weighted versions of MSE and MAE:
    - For MAPE, the weight of each sample is inversely proportional to its target.
    - For MSPE, the weight of each sample is inversely  proportional to target squared.
    - Note that the weights do not sum up to 1.

#### MSPE baseline
- For MSE, the optimal $\alpha$ is the target mean.
- For MSPE, the optimal $\alpha$ is the weighted target mean.
    - $MSPE = \frac{100\%}{N}\sum_{i=1}^N(\frac{y_i-\alpha}{y_i})^2$

#### MAPE baseline
- For MAE, the optimal $\alpha$ is the target median.
- For MAPE, the optimal $\alpha$ is the weighted target median.
    - $MAPE = \frac{100\%}{N}\sum_{i=1}^N|\frac{y_i-\alpha}{y_i}|$

#### RMSLE: Root Mean Squared Logarithmic Error
- RMSLE is RMSE calculated in log space.
    - A constant is added because log(0) is undefined. The constant can be 1 or something else.
- $RMSLE = \sqrt{\frac{1}{N}\sum_{i=1}^N(log(y_i+1)-log(\hat{y_i}+1))^2} = RMSE(log(y_i+1),log(\hat{y_i}+1)) = \sqrt{MSE(log(y_i+1),log(\hat{y_i}+1))}$
- Just like MAPE and MSPE, this metric cares more about relative errors than absolute ones.
- However, the error curves for RMSLE are asymmetric. For this metric, it is always better to predict more than the same amount less than target.
- The version of this metric without square root can also be used, but the rooted version is more common.

#### RMSLE baseline
- $RMSLE = \sqrt{\frac{1}{N}\sum_{i=1}^N(log(y_i+1)-log(\alpha+1))^2} = RMSE(log(y_i+1),log(\alpha+1)) = \sqrt{MSE(log(y_i+1),log(\alpha+1))}$
- The best constant in log space is a mean target value. We need to exponentiate it to get the answer.

#### Summary
- MSE is biased towards large values, while MAE is much less biased.
- MSPE and MAPE are biased towards smaller targets, as they assign higher weight to objects with smaller targets.
- RMSLE is frequently considered a better metric than MAPE since it is less biased towards small targets, yet works with relative errors.

### Classification Metrics
- Notation:
    - $N$ - number of objects
    - $L$ - number of classes
    - $y$ - ground truth
    - $\hat{y}$ - predictions
    - $[a=b]$ - indicator function
    - *Soft labels (soft predictions)*: classifier's scores
    - *Hard labels (hard predictions)*:
        - $arg max_if_i(x)$
        - $[f(x) > b]$, b - threshold (for binary classification)

#### Accuracy score
- Accuracy = $\frac{1}{N}\sum_{i=1}^N[\hat{y}=y_i]$
- Accuracy is the fraction of correctly classified objects, ranging from 0 to 1.
- Need hard labels to calculate accuracy.
- Baseline (best constant to predict):
    - Predict the most frequent class.
    - This can lead to high accuracy when predicting this constant on a dataset with imbalanced classes.
- While this score is intuitive, it is hard to optimize.
- Accuracy score also doesn't care how confident the classifier is in the predictions, or what the soft predictions are.
- This is why it is usually preferred other metrics that are easier to optimize and that work with soft predictions.

#### Logarithmic loss (logloss)
- Logloss tries to make the classifier output true posterior probabilities for the observation to be of a certain class.
- Logloss is written in different forms for binary and multiclass tasks.
- *Binary*: LogLoss = $-\frac{1}{N}\sum_{i=1}^Ny_ilog(\hat{y_i}) + (1-y_i)log(1-\hat{y_i}), y_i \in R, \hat{y_i} \in R$
    - $\hat{y_i}$ is assumed to be a number in the range [0,1], and is the probability of an object to belong to class 1. So, $(1-\hat{y_i})$ is the probability of the object to belong to class 0.
- *Multiclass*: LogLoss = $-\frac{1}{N}\sum_{i=1}^N\sum_{i=1}^Ly_{il}log(\hat{y_{il}}), y_i \in R^L, \hat{y_i} \in R^L$
    - $\hat{y_i}$ is a vector of size $L$, and its sum is exactly 1. The elements are the probabilities to belong to each of the classes.
- *In practice*: LogLoss = $-\frac{1}{N}\sum_{i=1}^N\sum_{i=1}^Ly_{il}log(min(max(\hat{y_{il}},10^{-15}),1-10^{-15}))$
    - To avoid NaNs in practice, predictions are first clipped to be not from 0 to 1, but from some small positive number to 1 minus some small positive number.
- Compared with absolute error, logloss strongly penalizes completely wrong answers, and prefers to make a lot of small mistakes rather than make few severe mistakes.
- Baseline:
    - LogLoss = $-\frac{1}{N}\sum_{i=1}^Ny_ilog(\alpha) + (1-y_i)log(1-\alpha)$
    - Set $\alpha_i$ to the frequency of i-th class.
    - For a dataset of 10 cats and 90 dogs, $\alpha$ = [0.1, 0.9].

#### Area under curve (AUC ROC) - binary classification only
- This is a good default metric.
- This metric tries all possible thresholds and aggregates their scores.
- AUC ROC does not care about absolute value of the predictions, but depends on the order of the objects.
- There are several ways that AUC can be explained:
    - Area under curve
    - Pairs ordering

##### Area under curve
- The ROC curve is constructed on a 2-d plot with false positives (FP) as the x-axis, and true positives (TP) as the y-axis. Start at (0,0), and move up/right while moving the threshold from 0 to 1.
- The area under the ROC curve is normalized by the total area of the square to give AUC.
- The AUC for a perfect classifier is 1. A threshold does not need to be specified, and there is no dependence on absolute values.
- In practice, AUC is plotted on a graph with a diagonal line plotted as well. The diagonal line is what AUC looks like if predictions are made at random, and can be thought of as a baseline.
    
##### Pairs ordering
- Consider all pairs of objects, such that one object is from class 0, and the other is from class 1.
- AUC is the probability that score for 1 will be higher than score for 0.
- In other words, AUC is the fraction of correctly ordered pairs: AUC = (# correctly ordered pairs)/(total number of pairs) = 1 - (# incorrectly ordered pairs)/(total number of pairs).


- Baseline:
    - AUC doesn't depend on the value of the predictions, so all constants will lead to the same score.
    - This score will be around 0.5.

#### Cohen's kappa
- Recall that if we just predict the label of the most frequent class, we can already get a high accuracy score, and this can be misleading.
- We can introduce a new metric such that for accuracy of 1 it will give 1, and for baseline accuracy it will give 0:
    - my_score = 1 - (1-accuracy)/(1-baseline)
    - Ex: for a dataset of 10 cats and 90 dogs, the baseline is 0.9 (most frequent class). For accuracy 1, my_score = 1. For accuracy 0.9, my_score = 0.
    - Baselines are different for every dataset.
    - This is similar to what R-squared does with MSE (normalizing).
- For Cohen's Kappa, the baseline is calculated is the average of the accuracies for randomly shuffled hard predictions.
    - Ex: for a dataset of 10 cats and 90 dogs, predict 20 cats and 80 dogs at random: accuracy = $0.2*0.1 + 0.8*0.9 = 0.74$.
- Cohen's Kappa = 1 - (1 - accuracy)/(1 - $p_e$), where $p_e$ is the average accuracy for permuted predictions.
- $p_e$ can be computed analytically: $p_e = \frac{1}{N^2}\sum_kn_{k1}n_{k2}$.
    - Multiply the empirical frequencies of predictions and ground truth labels for each class, and then sum them up.
- The score can also be rewritten in terms of errors: Cohen's Kappa = 1 - error / (baseline error)

##### Weighted error and weighted kappa
- The weighted error is calculated using an error weight matrix and confusion matrix.
- An error weight matrix contains the weight for each mistake.
    - Ex: for a dataset of 10 cats, 90 dogs, and 20 tigers, form a 3x3 matrix, and set the weight to be 10 for when predict "cat" or "dog", but ground truth is "tiger".
- A confusion matrix shows how our classifier distributes predictions over the objects.
- Weighted error = $\frac{1}{const}\sum_{i,j}C_{i,j}W_{i,j}$
    - This is a sum of the element-wise matrix multiplication.
    - A constant is used to normalize the result between 0 and 1, but it will cancel out later.
- Then weighted kappa is calculated as: weighted kappa = 1 - (weighted error) / (weighted baseline error).

##### Quadratic and linear weighted kappa
- In many cases, the weight matrices are defined in a very simple way.
    - For a classification problem with ordered labels, weights can be linear: $w_{ij} = |i-j|$, or quadratic: $w_{ij} = (i-j)^2$.
- The quadratic weighted kappa has been used in several competitions on Kaggle. It is usually explained as inter-rater agreement coefficient (how much the model predictions agree with ground truth raters). This is quite intuitive for medicine applications - how much the model agrees with professional doctors.

### General approaches for metrics optimization

#### Loss vs metric
- The **target metric** is a function what *we* want to optimize (evaluate the quality of our model).
- **Optimization loss** is what the *model* optimizes.
    - No one really knows how to optimize metrics efficiently. Instead, we come up with proxy loss functions that are easy to optimize for a given model.
    - For example, logloss is widely used as an optimization loss, while the accuracy score is how the solution is eventually evaluated.
- This can be viewed as expectation vs reality.
- Sometimes the model can optimize the target metric directly. E.g., most libraries can optimize RMSE out-of-the-box.
- Sometimes, we want to optimize metrics that are really hard or impossible to optimize directly. In this case, we set the model to optimize a loss that is different to the metric, but after the model is trained, we use hacks and heuristics to negate the discrepancy and adjust the model to better fit the target metric.
- The words *loss*, *cost*, and *objective* are used as synonyms.

#### Approaches to target metric optimization
- Some metrics can be optimized directly. That is, we should just find a model that optimizes this metric and run it (set the model's loss function to this metric). MSE and Logloss are implemented as loss function in many libraries.
- For some metrics that cannot be optimized directly, we can preprocess the train set and optimize another metric. For example, while MSPE cannot be optimized directly with XGBoost, we can resample the train set and optimize MSE instead. This also applies to MAPE and RMSLE.
- Sometimes we can optimize an incorrect metric, but postprocess the predictions to fit a competition metric better. Examples: Accuracy, Kappa.
- Sometimes it is possible to write a custom loss function. E.g., this is possible for XGBoost.
- Sometimes it is possible to use another metric, and use early stopping.
    - Optimize metric M1, and monitor metric M2 on the validation set. Stop when M2 score is best.

### Regression metrics optimization
- Early stopping can be used for every metric, so will not be described further here.

#### MSE
- MSE is the most commonly used metric for regression tasks. Almost every modeling software implements MSE as a loss function.
- Libraries that support MSE:
    - Tree-based
        - `XGBoost`, `LightGBM`
        - `sklearn.RandomForestRegressor`
    - Linear models
        - `sklearn.<>Regression`
        - `sklearn.SGDRegressor`
        - Vowpal Wabbit (*quantile loss*)
    - Neural Nets
        - PyTorch, Keras, TensorFlow, etc.
- Synonyms: *L2 loss*.
    
#### MAE
- Since the 2nd derivative of MAE is 0, XGBoost cannot optimize it.
- Libraries that support MAE:
    - Tree-based
        - `LightGBM`
        - `sklearn.RandomForestRegressor` (running time high compared to MSE)
        - Some `sklearn` models have huber loss which is very similar to MAE, especially when the errors are large.
    - Linear models
        - Vowpal Wabbit (*quantile loss*)
            - MAE is a special case of quantile loss.
    - Neural Nets
        - PyTorch, Keras, TensorFlow, etc.
            - Can also be implemented by hand.
- Synonyms: *L1*, *Median regression*.
- There are many ways to make MAE smooth. The most famous is Huber loss - this is a mix between MAE and MSE.
    - MSE is computed when the error is small, so we can safely approach 0 error.
    - MAE is computed for large errors, giving robustness.

#### MSPE and MAPE
- Can implement custom loss for XGBoost or a neural net, or can use early stopping.

##### Using sample weights
- Many libraries accept sample weights.
- Sample weights:
    - MSPE: $w_i = \frac{1/y_i^2}{\sum_{i=1}^N1/y_i^2}$
    - MAPE: $w_i = \frac{1/y_i}{\sum_{i=1}^N1/y}$
- To optimize MSPE, set the loss in the library to MSE, and use MSPE sample weights. Similarly for MAPE.
- Not every library supports sample weights.

##### Using resampling
- This approach does not depend on sample weights support.
- Resample the train set: `df.sample(weights = sample_weights)`.
- And use *any* model that optimizes MSE (MAE).
- The size of the new dataset is up to the user (e.g., can sample twice the original number).
    - Usually need to resample many times, fitting a model each time, and averaging the predictions for a more stable score.
- The test set stays as-is.

##### Log scale
- If the errors are small, we can optimize the predictions in log scale.
- This approach was widely used on the Rossman competition on Kaggle.

#### RMSLE
- This is easy to optimize due to the connection with MSE loss.
- First, apply a transform to the target variables: $z_i = log(y_i+1)$.
- Then, fit a model with MSE loss.

### Classification metrics optimization I

#### Log loss
- Log loss for classification is like MSE for regression: it is implemented everywhere.
- $LogLoss = -\frac{1}{N}\sum_{i=1}^Ny_ilog(\hat{y}_i) + (1-y_i)log(1-\hat{y}_i)$
- Implemented in the following libraries:
    - Tree-based: XGboost, LightGBM
    - Linear models
        - `sklearn.LogisticRegression`
        - `sklearn.SGDClassifier`
        - Vowpal Wabbit
    - Neural nets by default optimize log loss for classification.
- Synonyms: *Logistic loss*.
- Random Forest predictions turn out to be quite bad in terms of log loss. However, we can calibrate the predictions to better fit log loss.

##### Probability calibration
- We've mentioned several times that log loss requires a model to output posterior probabilities, but what does this mean?
- This means that, e.g. if we take all objects with score ~ 0.8, then there will be exactly 4 times more positive objects (class 1) than negative objects (class 0).
- If a classifier doesn't directly optimize log loss, its predictions should be calibrated.
- Plot (not shown) has predictions (sorted by value) for the val set. The classifier's uncalibrated predictions are higher than target on the lower end, and lower than target on higher end. The calibrated predictions are much closer to the target on both ends.
- 3 types of probability calibration are mentioned here:
    - Platt scaling: fit Logistic Regression to the predictions (like in stacking)
    - Isotonic regression: fit Isotonic Regression to the predictions
    - Stacking: fit XGBoost or neural net to predictions
- The basic idea is this: We can fit any classifier - it does not need to optimize log loss, it just needs to be good in terms of, for example, AUC. Then, we can fit another model (model 2) on top that will take the first model's predictions and calibrate them properly. Model 2 uses log loss as its optimization loss.

#### Accuracy
- Accuracy = $\frac{1}{N}\sum_{i=1}^N[\hat{y}_i = y_i]$
- There is no easy recipe for direct optimization of accuracy.
- In general, the recipe is the following:
    - For binary classification: fit any metric and tune binarization threshold.
    - For multiclass: fit any metric and tune parameters comparing models by their accuracy score, not by the metric that the models were actually optimizing (early stopping and cross validation by accuracy).
- Plot (not shown) to get an intuition of why accuracy is hard to optimize: loss vs M, where M is signed distance to decision boundary (e.g., distance to hyperplane for linear model). Distance is considered to be positive if the class is predicted correctly, and negative if the object is located at the wrong side of the decision boundary.
    - Zero-one loss (step function with 1 for 0<M and 0 for M>0) corresponds to accuracy score. This loss has gradient 0 w.r.t. predictions. Most learning algorithms require a non-zero gradient to fit. Otherwise, it is not clear how to change predictions to decrease the loss.
    - Proxy losses have been invented that are upper bounds for the zero-one loss. If proxy loss is perfectly fit, then the accuracy will be perfect too. Such proxies are differentiable.
    - Logistic loss is used in logistic regression.
    - Hinge loss is used in SVM.
- Recall that to obtain hard labels for a test object, we take argmax of soft predictions, picking the class with max score. If the task is binary and predictions sum up to 1, then argmax is equivalent to a threshold function: output 1 if pred > 0.5, else output 0.
    - The threshold can be tuned with a grid search implemented with a for-loop.
    - This means we can fit any sufficiently powerful model, and it does not matter exactly which loss (hinge, logistic, etc) the model optimizes. All we want from the model's predictions is the existence of a good threshold that separates the classes.
    - If the classifier is ideally calibrated, then it is really returning posterior probabilities,
    and for such a classifier, the threshold of 0.5 is optimal. But such classifiers are rare, so threshold tuning is useful.


### Classification metrics optimization II

#### AUC
- Although the loss function of AUC has zero gradients almost everywhere (exactly as accuracy loss), there exists an algorithm to optimize AUC with gradient-based methods. Some models implement this algorithm, so we can use the algorithm by setting the right parameters.
- There is more than one way to implement the algorithm; one way is discussed here.

##### Pointwise loss
- Recall that originally, a classification task is usually solved at the object level. We want to assign 0 to red objects (true class 0), and 1 to green objects (true class 1). We do this independently for each object, and so our loss is pointwise. We compute it for each object individually, and sum or average the losses to get a total loss.
    - $min\sum_i^Nl_{point}(\hat{y}_i;y_i)$

##### Pairwise loss
- Recall that AUC is the probability of the pair of objects to be ordered in the right way. So ideally, we want predictions for the green objects to be greater than predictions for the red ones. So instead of using single objects, we are working with pairs of objects, and instead of pointwise loss, we are using pairwise loss.
    - Pairwise loss takes predictions and labels for a pair of objects and computes their loss. Ideally, the loss is 0 when ordering is correct, and greater than 0 when the ordering is incorrect. In practice, different loss functions can be used. $$min\sum_i^N\sum_i^Nl_{pair}(\hat{y}_i,\hat{y}_j;y_i,y_j)$$
- For example, we can use log loss: $$Loss = \frac{1}{N_0N_2}\sum_{j:y_j=1}^{N_1}\sum_{i:y_i=0}^{N_0}{log(prob(\hat{y}_j-\hat{y}_i))}$$





### Additional material and links

## Mean encodings

### Concept of mean encoding

### Regularization

### Extensions and generalizations





* * * 

# Course summary - most important points
- Set up the train/validation data split to mimic the train/test split of the competition.
  