---
title: "Competitive Data Science Notes"
output:
  html_document:
    theme: cerulean
    highlight: tango
    fig_width: 6
    fig_height: 4
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require("knitr")
```

# Introduction

This is a set of notes for the Competitive Data Science course on Coursera.

# Week 1

## Recap of main machine learning algorithms

### Linear models
- Examples: Logistic regression, SVM.
- Separate objects with a plane.
- Good for sparse, high-dimensional data.
- Often, points cannot be separated by such a simple approach.

### Tree-based: Decision Tree, Random Forest, Gradient Boosted Decision Trees
- Recursively split data by lines parallel to an axis. This significantly reduces the number of possible lines.
- Another way to look at it: a decision tree separates the data into boxes, and approximates the data in the boxes by constants.
- Very powerful general method for tabular data
- Hard to capture linear dependencies, since this requires a lot of splits. For example, 2 classes that can be separated by a diagonal line will require many vertical/horizontal splits.

### kNN-based methods
- Select class labels by majority vote of closest neighbors.
- Features based on kNN are very informative.
- `scikit-learn` has implementation with algorithmic tricks to speed up calculation.

### Neural Networks
- Produce smooth non-linear decision boundary.

### No Free Lunch Theorem
- "There is no method which outperforms all others for all tasks" or "For every method we can construct a task for which this particular method will not be the best".
- Every method relies on some assumptions for it to work. If the assumptions fail, the method will perform poorly.


### Links for solving the quiz
- [Explanation of Random Forest](http://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)
- [Explanation/Demonstration of Gradient Boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)
- [Example of kNN](https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/)


### Additional materials and links
- Overview of methods
    - [Scikit-Learn (or sklearn) library](http://scikit-learn.org/)
    - [Overview of k-NN (sklearn's documentation)](http://scikit-learn.org/stable/modules/neighbors.html)
    - [Overview of Linear Models (sklearn's documentation)](http://scikit-learn.org/stable/modules/linear_model.html)
    - [Overview of Decision Trees (sklearn's documentation)](http://scikit-learn.org/stable/modules/tree.html)
    - [Overview of algorithms and parameters in H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)
- Additional tools
    - [Vowpal Wabbit repository](https://github.com/JohnLangford/vowpal_wabbit)
    - [XGBoost repository](https://github.com/dmlc/xgboost)
    - [LightGBM repository](https://github.com/Microsoft/LightGBM)
    - [Interactive demo of simple feed-forward Neural Net](http://playground.tensorflow.org/)
    - Frameworks for Neural Nets: [Keras](https://keras.io/) , [PyTorch](http://pytorch.org/) , [TensorFlow](https://www.tensorflow.org/) , [MXNet](http://mxnet.io/) , [Lasagne](http://lasagne.readthedocs.io/)
    - [Example from sklearn with different decision surfaces](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)
    - [Arbitrary order factorization machines](https://github.com/geffy/tffm)

## Software/hardware requirements
### Additional material and links
- StandCloud Computing:
    - [AWS](https://aws.amazon.com/), [Google Cloud](https://cloud.google.com/), [Microsoft Azure](https://azure.microsoft.com/)
- AWS spot option:
    - [Overview of Spot mechanism](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html)
    - [Spot Setup Guide](http://www.datasciencebowl.com/aws_guide/)
- Stack and packages:
    - [Basic SciPy stack (ipython, numpy, pandas, matplotlib)](https://www.scipy.org/)
    - [Jupyter Notebook](http://jupyter.org/)
    - [Stand-alone python tSNE package](https://github.com/danielfrg/tsne)
    - Libraries to work with sparse CTR-like data: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)
    - Another tree-based method: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))
    - Python distribution with all-included packages: [Anaconda](https://www.continuum.io/what-is-anaconda)
    - [Blog "datas-frame" (contains posts about effective Pandas usage)](https://tomaugspurger.github.io/)

## Feature preprocessing and generation with respect to models

### Numeric features
- Tree-based models do not depend on feature scale. 
- NN, kNN, and linear models do depend on feature scale.
	  - For linear models, regularization impact is proportional to feature scale.
	  - For kNN, larger features are treated as more important.

#### Preprocessing
- Scaling:
	  - Scaling to [0,1] interval (`sklearn.preprocessing.MinMaxScaler`)
		    - X = (X - X.min()) / (X.max() - X.min())
	  - Scaling to have mean 0, std 1 (`sklearn.preprocessing.StandardScaler`)
	  - Scaling should be initially applied to all numeric features equally.
	  - Scaling params can be optimized (tuned) to boost features which seem to be more important.
- Outliers
	  - To protect models from outliers, can clip feature values between lower and upper bounds (e.g., 1% and 99%). This is widely used in finanical data, and is called Winsorization.
- Rank transformation (`scipy.stats.rankdata`):
  	- set spaces between properly sorted values to be roughly equal.
  	- Can be a better option than `MinMaxScaler` because the rank transform will move outliers closer to other objects.
  	- Can be used for kNN, NN, and linear models if there is no time to handle outliers manually.
  	- To apply rank transform to TEST data, need to store the mapping; alternatively, concatenate train and test sets before applying rank transform.
- Other transforms that help linear models and especially NN:
  	- Log transform (`np.log(1+x)`)
  	- Raising to the power < 1 (`np.sqrt(x + 2/3)`)
  	- These transforms drag large values closer to the mean.
- Additional ideas:
  	- Train models on concatenated data frames produced by different preprocessings.
  	- Mix models trained on differently preprocessed data.

#### Feature generation
- Sometimes, features can be engineered by prior knowledge.
- GBDT models have difficulty with approximating multiplications and divisions. Adding new features that are linear combinations of other features can help make the model more robust with less trees.
- Fractional part: generate a new feature that is the fractional part of a feature with numbers after a decimal (e.g., .23 is the fractional part of 1.23).

### Categorical and ordinal features

#### Ordinal features
- Ordinal features are categorical features that are ordered in some meaningful way. However, unlike with numeric features, we don't know if the difference between categories 1 and 2 is equal to the difference between categories 2 and 3.
- Examples of ordinal features: ticket class (1,2,3), driver's license (A, B, C, D), Education (kindergarten, school, undergraduate, etc).

#### Label encoding
- The simplest way to encode a categorical feature is to map its values to different numbers. This is known as *label encoding*. This works with trees because a tree method can split features and extract most of the useful information in categories on its own. Non-tree-based methods cannot use label-encoded features effectively.
- Label encoding can be accomplished in the following ways:
  	- `sklearn.preprocessing.LabelEncoder` applies encoding in alphabetical order: [S,C,Q] -> [2,1,3].
  	- `Pandas.factorize` applies encoding in order of appearance: [S,C,Q] -> [1,2,3]
  		  - This is useful if the rows are sorted in some meaningful way.
  	- Frequency encoding: encode a feature via mapping values to their frequencies: [S,C,Q] -> [0.5,0.3,0.2]
  		  - This preserves some information about distribution of the values.
  		  - If frequency of category is correlated with target value, a linear model will use this dependency.
  		  - A tree model may have a lower number of splits due to the correlation.
  		```{}
  			encoding = titanic.groupby('Embarked').size()
  			encoding = encoding/len(titanic)
  			titanic['enc'] = titanic.Embarked.map(encoding)
  		```
  		  - In the case of ties for variables that occur with the same frequency, a rank operation can be applied: [S,C,Q] -> [0.5,0.3,0.2]
  		```{}
  			from scipy.stats import rankdata
  		```

#### One-hot encoding
- This is useful for linear models.
- One-hot encoded features are already scaled by definition.
- For a data set with a few numeric features and many one-hot encoded features, it may be difficult for a tree model to use the numeric features efficiently. More precisely, tree models will slow down, not always improving on previous results.
- If a categorical feature has too many unique values, one-hot encoding will add too many columns with zero values. Use sparse matrices to store this data efficiently.
- Use `pandas.get_dummies` or `sklearn.preprocessing.OneHotEncoder`.

#### Feature generation for categorical features
- One of the most useful examples of feature generation is interaction between categorical features. This is usually useful for linear models and kNN.
- To implement an interaction between two features, first concatenate the feature values into a string (e.g., "1male", "2female"), and then apply one-hot encoding.

### Datetime and coordinates
- Both these features differ significantly from numeric and categorical features.
- Since we can interpret the meaning of datetime and coordinates, we can come up with specific ideas about feature generation.

#### Datetime
- Most new features generated from a datetime fall into two categories: time moments in a period, and time passed since a particular event.
- Periodicity: day number in week, month, season, year, second, minute, hour. This is useful to capture repetitive patterns. Can also add non-common periodicity, e.g. when a patient receives medication every 3 days.
- Time since:
    - Row-independent; e.g. since 00:00:00 UTC, 1 January 1970
    - Row-dependent; e.g. number of days left until next holiday, or time passed since last holiday.
- Difference between dates: `datetime_feature_1 - datetime_feature_2`. E.g., for churn prediction, days since last purchase date or days since registration.
- These new generated features will then need to be treated accordingly; e.g. label or one-hot encoding for categorical.

#### Coordinates
- If have geographical coordinates, can add new features like distance to the nearest shop, hospitals, schools, etc.
- Divide a map into squares with a grid, and in each square find the most expensive flat. Then a new feature is the distance to this flat for each object in the square.
- Organize the data into clusters. Use cluster centers as important points, and generate new features as distances to the clusters.
- Find special areas, like an area with very old buildings, and add distance to this area as a feature.
- Calculate aggregated statistics for the area surrounding an object: number of flats (interpreted as the area's popularity), mean real estate price (indicates how expensive the area is).
- Trick for decision trees: can add slightly rotated coordinates as new features. This helps a model make more precise selections on a map. E.g., a street divides an area into high-cost and low-cost district. If the street is diagonal, this will require many splits; with rotated coordinates, this separation can be accomplished in one split. It can be hard to know which rotations to make, so we may want to add all rotations to 45 or 22.5 degrees.

### Handling missing values
- Missing values can look like NaN, empty strings, or outliers like -999.
- Sometimes, missing values can contain useful information.

#### Finding replaced missing values
- Missing values are sometimes already replaced in the data. One way to find such values is to plot a histogram. E.g., a historgram might show an smooth distribution for most numbers, but a large spike for a specific number. The number might be -1 or the mean value of the feature, which means missing values were replaced. 

#### Missing value imputation
- Three ways of dealing with Missing value imputation:
  - Replace by some value outside the normal range (e.g., -999, -1).
      - Pro: allows to take the missing value into a separate category.
      - Con: linear and NN model performance can suffer.
  - Replace with mean or median.
    - Pro: can help NN and linear models.
    - Con: for tree models, can be harder to select object which had missing value in the first place.
  - Reconstruct value
      - In time series, the rows of a dataset are not independent, and interpolation interpolation works well.
      - For data that is not time series, there will usually be no proper logic to reconstruct MVs.
- Feature generation
    - Add a new "isnull" indicator feature, indicating whether a specific feature is null or NaN. This solves the problem with trees and NN while computing the mean or median. However, this method doubles the number of columns in the dataset.
    - Be careful with replacing missing values before feature generation.
        - When encoding a categorical feature together with a numeric feature, ignore missing values when calculating mean values for the numeric features.
        - In general, avoid filling NaNs before feature generation since this can decrease the usefulness of the features.
- XGBoost can handle missing values that are NaN, and this can sometimes change the score drastically.
- Outliers can be treated as missing values when it makes sense.
- Categories which are present in test data but not in train data should be treated as missing values. Encode such a feature with frequencies of occurrence in the concatenated train and test datasets.


### Additional material and links
- Feature preprocessing
    - [Preprocessing in Sklearn](http://scikit-learn.org/stable/modules/preprocessing.html)
    - [Andrew NG about gradient descent and feature scaling](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling)
    - [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)

- Feature generation
    - [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
    - [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)

## Feature extraction from text and images

### Bag of words

### Word2vec, CNN
